@inproceedings{bartkowskiSyncExploringSynchronization2023a,
  title = {In {{Sync}}: {{Exploring Synchronization}} to {{Increase Trust Between Humans}} and {{Non-humanoid Robots}}},
  shorttitle = {In {{Sync}}},
  booktitle = {Proceedings of the 2023 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Bartkowski, Wieslaw and Nowak, Andrzej and Czajkowski, Filip Ignacy and Schmidt, Albrecht and M{\"u}ller, Florian},
  year = {2023},
  month = apr,
  series = {{{CHI}} '23},
  eprint = {2303.15917},
  pages = {1--14},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/3544548.3581193},
  urldate = {2024-01-09},
  abstract = {When we go for a walk with friends, we can observe an interesting effect: From step lengths to arm movements - our movements unconsciously align; they synchronize. Prior research found that this synchronization is a crucial aspect of human relations that strengthens social cohesion and trust. Generalizing from these findings in synchronization theory, we propose a dynamical approach that can be applied in the design of non-humanoid robots to increase trust. We contribute the results of a controlled experiment with 51 participants exploring our concept in a between-subjects design. For this, we built a prototype of a simple non-humanoid robot that can bend to follow human movements and vary the movement synchronization patterns. We found that synchronized movements lead to significantly higher ratings in an established questionnaire on trust between people and automation but did not influence the willingness to spend money in a trust game.},
  archiveprefix = {arxiv},
  isbn = {978-1-4503-9421-5},
  talk = {https://www.youtube.com/watch?v=dEYNV4KaBR8},
  video = {https://www.youtube.com/watch?v=kdEB9L4OCgk},
  keywords = {design strategy,dynamical approach,non-humanoid robot,synchronization,trust},
  file = {C:\Users\ra46sin\OneDrive\Uni\paper\zotero\bartkowskiSyncExploringSynchronization2023a.pdf}
}

@inproceedings{desoldaDigitalModelingEveryone2023a,
  title = {Digital {{Modeling}} for~{{Everyone}}: {{Exploring How Novices Approach Voice-Based 3D Modeling}}},
  shorttitle = {Digital {{Modeling}} for~{{Everyone}}},
  booktitle = {Human-{{Computer Interaction}} {\textendash} {{INTERACT}} 2023},
  author = {Desolda, Giuseppe and Esposito, Andrea and M{\"u}ller, Florian and Feger, Sebastian},
  editor = {Abdelnour Nocera, Jos{\'e} and Krist{\'i}n L{\'a}rusd{\'o}ttir, Marta and Petrie, Helen and Piccinno, Antonio and Winckler, Marco},
  year = {2023},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {133--155},
  publisher = {{Springer Nature Switzerland}},
  address = {{Cham}},
  doi = {10.1007/978-3-031-42293-5_11},
  abstract = {Manufacturing tools like 3D printers have become accessible to the wider society, making the promise of digital fabrication for everyone seemingly reachable. While the actual manufacturing process is largely automated today, users still require knowledge of complex design applications to produce ready-designed objects and adapt them to their needs or design new objects from scratch. To lower the barrier to the design and customization of personalized 3D models, we explored novice mental models in voice-based 3D modeling by conducting a high-fidelity Wizard of Oz study with 22 participants. We performed a thematic analysis of the collected data to understand how the mental model of novices translates into voice-based 3D modeling. We conclude with design implications for voice assistants. For example, they have to: deal with vague, incomplete and wrong commands; provide a set of straightforward commands to shape simple and composite objects; and offer different strategies to select 3D objects.},
  isbn = {978-3-031-42293-5},
  langid = {english},
  keywords = {3D Design,Digital Fabrication,Voice Interaction,Wizard of Oz Study},
  file = {C:\Users\ra46sin\OneDrive\Uni\paper\zotero\desoldaDigitalModelingEveryone2023a.pdf}
}

@inproceedings{Dezfuli2012,
  title = {{{PalmRC}}: {{Imaginary}} Palm-Based Remote Control for Eyes-Free Television Interaction},
  booktitle = {{{EuroiTV}}'12 - {{Proceedings}} of the 10th {{European Conference}} on {{Interactive TV}} and {{Video}}},
  author = {Dezfuli, Niloofar and Khalilbeigi, Mohammadreza and Huber, Jochen and M{\"u}ller, Florian and M{\"u}hlh{\"a}user, Max},
  year = {2012},
  month = jul,
  pages = {27--34},
  publisher = {{ACM}},
  address = {{New York, New York, USA}},
  doi = {10.1145/2325616.2325623},
  urldate = {2014-07-30},
  abstract = {User input on television (TV) typically requires a mediator device, such as a handheld remote control. While being a well-established interaction paradigm, a handheld device has serious drawbacks: it can be easily misplaced due to its mobility and in case of a touch screen interface, it also requires additional visual attention. Emerging interaction paradigms like 3D mid-air gestures using novel depth sensors, such as Microsoft's Kinect, aim at overcoming these limitations, but are known to be e.g. tiring. In this paper, we propose to leverage the palm as an interactive surface for TV remote control. Our contribution is three-fold: (1) we explore the conceptual design space in an exploratory study. (2) Based upon these results, we investigate the effectiveness and accuracy of such an interface in a controlled experiment. And (3), we contribute PalmRC: an eyes-free, palm-surface-based TV remote control, which in turn is evaluated in an early user feedback session. Our results show that the palm has the potential to be leveraged for device-less and eyes-free TV remote interaction without any third-party mediator device. {\textcopyright} 2012 ACM.},
  isbn = {978-1-4503-1107-6},
  keywords = {alternative remote control,device-less,direct touch,eyes-free,input,memory,non-visual,omnipresent,TV},
  file = {C:\Users\ra46sin\OneDrive\Uni\paper\zotero\Dezfuli2012.pdf}
}

@inproceedings{Dezfuli2012a,
  title = {Leveraging the Palm Surface as an Eyes-Free Tv Remote Control},
  booktitle = {{{CHI}} '12 {{Extended Abstracts}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Dezfuli, Niloofar and Khalilbeigi, Mohammadreza and Huber, Jochen and M{\"u}ller, Florian and M{\"u}hlh{\"a}user, Max},
  year = {2012},
  month = may,
  pages = {2483--2488},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/2212776.2223823},
  urldate = {2014-07-30},
  abstract = {User input on television typically requires a mediator device such as a handheld remote control. While being a well-established interaction paradigm, a handheld device has serious drawbacks: it can be easily misplaced due to its mobility and in case of a touch screen interface, it also requires additional visual attention. Emerging interaction paradigms like 3D mid-air gestures using novel depth sensors such as Microsoft's Kinect aim at overcoming these limitations, but are known for instance to be tiring. In this paper, we propose to leverage the palm as an interactive surface for TV remote control. Our contribution is two-fold: (1) we have explored the conceptual design space in an exploratory study. (2) Based upon these results, we investigated the accuracy and effectiveness of such an interface in a controlled experiment. Our results show that the palm has the potential to be leveraged for device-less and eyes-free TV interactions without any third-party mediator device.},
  isbn = {978-1-4503-1016-1},
  keywords = {device-less,eyes-free,input,omnipresent,tv},
  file = {C:\Users\ra46sin\OneDrive\Uni\paper\zotero\Dezfuli2012a.pdf}
}

@inproceedings{Dezfuli2012Couch,
  title = {{{CouchTV}}: {{Leveraging}} the {{Spatial Information}} of {{Viewers}} for {{Social Interactive Television Systems}} - {{Poster Presentation}}},
  booktitle = {10th {{European Interactive TV Conference}} 2012},
  author = {Dezfuli, Niloofar and Pavlakis, Manolis and M{\"u}ller, Florian and Khalilbeigi, Mohammadreza and M{\"u}hlh{\"a}user, Max},
  year = {2012},
  month = jul
}

@inproceedings{Distante2019,
  title = {Trends on Engineering Interactive Systems: An Overview of Works Presented in Workshops at {{EICS}} 2019},
  booktitle = {Proceedings of the {{ACM SIGCHI Symposium}} on {{Engineering Interactive Computing Systems}} - {{EICS}} '19},
  author = {Distante, Damiano and Voit, Alexandra and Winckler, Marco and Bernhaupt, Regina and Bowen, Judy and Campos, Jos{\'e} Creissac and M{\"u}ller, Florian and Palanque, Philippe and {Van den Bergh}, Jan and Weyers, Benjamin},
  year = {2019},
  pages = {1--6},
  publisher = {{ACM Press}},
  address = {{New York, New York, USA}},
  doi = {10.1145/3319499.3335655},
  urldate = {2019-07-11},
  isbn = {978-1-4503-6745-5},
  keywords = {enabling tecnhologies,Enabling tecnhologies,engineering interactive systems,Engineering interactive systems,HCI,multimodal interaction,Multimodal interaction,multiple stakeholders,Multiple stakeholders,smart environments,Smart environments,smart objects,Smart objects},
  file = {C:\Users\ra46sin\OneDrive\Uni\paper\zotero\Distante2019.pdf}
}

@inproceedings{Elsayed2020,
  title = {{{VRSketchPen}}: {{Unconstrained Haptic Assistance}} for {{Sketching}} in {{Virtual 3D Environments}}},
  booktitle = {26th {{ACM Symposium}} on {{Virtual Reality Software}} and {{Technology}}},
  author = {Elsayed, Hesham and Barrera Machuca, Mayra Donaji and Schaarschmidt, Christian and Marky, Karola and M{\"u}ller, Florian and Riemann, Jan and Matviienko, Andrii and Schmitz, Martin and Weigel, Martin and M{\"u}hlh{\"a}user, Max},
  year = {2020},
  month = nov,
  pages = {1--11},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/3385956.3418953},
  abstract = {Accurate sketching in virtual 3D environments is challenging due to aspects like limited depth perception or the absence of physical support. To address this issue, we propose VRSketchPen - a pen that uses two haptic modalities to support virtual sketching without constraining user actions: (1) pneumatic force feedback to simulate the contact pressure of the pen against virtual surfaces and (2) vibrotactile feedback to mimic textures while moving the pen over virtual surfaces. To evaluate VRSketchPen, we conducted a lab experiment with 20 participants to compare (1) pneumatic, (2) vibrotactile and (3) a combination of both with (4) snapping and no assistance for flat and curved surfaces in a 3D virtual environment. Our findings show that usage of pneumatic, vibrotactile and their combination significantly improves 2D shape accuracy and leads to diminished depth errors for flat and curved surfaces. Qualitative results indicate that users find the addition of unconstraining haptic feedback to significantly improve convenience, confidence and user experience.},
  isbn = {978-1-4503-7619-8},
  keywords = {3D User Interfaces,Haptics,Pneumatic Actuation,Sketching,Vibrotactile Actuation,Virtual Reality},
  file = {C:\Users\ra46sin\OneDrive\Uni\paper\zotero\Elsayed2020.pdf}
}

@inproceedings{Elsayed2021a,
  title = {{{CameraReady}}: {{Assessing}} the {{Influence}} of {{Display Types}} and {{Visualizations}} on {{Posture Guidance}}},
  booktitle = {Designing {{Interactive Systems Conference}} 2021},
  author = {Elsayed, Hesham and Hoffmann, Philipp and G{\"u}nther, Sebastian and Schmitz, Martin and Weigel, Martin and M{\"u}hlh{\"a}user, Max and M{\"u}ller, Florian},
  year = {2021},
  month = jun,
  pages = {1046--1055},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/3461778.3462026},
  isbn = {978-1-4503-8476-6},
  talk = {https://www.youtube.com/watch?v=KAjwxyqAegY},
  video = {https://www.youtube.com/watch?v=9OWH4eBrHtk},
  keywords = {augmented reality,display,posture guidance,visualization},
  file = {C:\Users\ra46sin\OneDrive\Uni\paper\zotero\Elsayed2021a.pdf}
}

@misc{elsayedUnderstandingStationaryMoving2023,
  title = {Understanding {{Stationary}} and {{Moving Direct Skin Vibrotactile Stimulation}} on the {{Palm}}},
  author = {Elsayed, Hesham and Weigel, Martin and M{\"u}ller, Florian and Ibrahim, George and Gugenheimer, Jan and Schmitz, Martin and G{\"u}nther, Sebastian and M{\"u}hlh{\"a}user, Max},
  year = {2023},
  month = feb,
  number = {arXiv:2302.08820},
  eprint = {2302.08820},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2302.08820},
  urldate = {2024-01-09},
  abstract = {Palm-based tactile displays have the potential to evolve from single motor interfaces (e.g., smartphones) to high-resolution tactile displays (e.g., back-of-device haptic interfaces) enabling richer multi-modal experiences with more information. However, we lack a systematic understanding of vibrotactile perception on the palm and the influence of various factors on the core design decisions of tactile displays (number of actuators, resolution, and intensity). In a first experiment (N=16), we investigated the effect of these factors on the users' ability to localize stationary sensations. In a second experiment (N=20), we explored the influence of resolution on recognition rate for moving tactile sensations.Findings show that for stationary sensations a 9 actuator display offers a good trade-off and a \$3{\textbackslash}times3\$ resolution can be accurately localized. For moving sensations, a \$2{\textbackslash}times4\$ resolution led to the highest recognition accuracy, while \$5{\textbackslash}times10\$ enables higher resolution output with a reasonable accuracy.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Human-Computer Interaction},
  file = {C\:\\Users\\ra46sin\\OneDrive\\Uni\\paper\\zotero\\elsayedUnderstandingStationaryMoving2023.pdf;C\:\\Users\\ra46sin\\Zotero\\storage\\NI7G2YV4\\2302.html}
}

@article{elsayedVibroMapUnderstandingSpacing2020,
  title = {{{VibroMap}}: {{Understanding}} the {{Spacing}} of {{Vibrotactile Actuators}} across the {{Body}}},
  shorttitle = {{{VibroMap}}},
  author = {Elsayed, Hesham and Weigel, Martin and M{\"u}ller, Florian and Schmitz, Martin and Marky, Karola and G{\"u}nther, Sebastian and Riemann, Jan and M{\"u}hlh{\"a}user, Max},
  year = {2020},
  month = dec,
  journal = {Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies},
  volume = {4},
  number = {4},
  pages = {125:1--125:16},
  publisher = {{ACM}},
  doi = {10.1145/3432189},
  urldate = {2024-01-10},
  abstract = {In spite of the great potential of on-body vibrotactile displays for a variety of applications, research lacks an understanding of the spacing between vibrotactile actuators. Through two experiments, we systematically investigate vibrotactile perception on the wrist, forearm, upper arm, back, torso, thigh, and leg, each in transverse and longitudinal body orientation. In the first experiment, we address the maximum distance between vibration motors that still preserves the ability to generate phantom sensations. In the second experiment, we investigate the perceptual accuracy of localizing vibrations in order to establish the minimum distance between vibration motors. Based on the results, we derive VibroMap, a spatial map of the functional range of inter-motor distances across the body. VibroMap supports hardware and interaction designers with design guidelines for constructing body-worn vibrotactile displays.},
  talk = {https://www.youtube.com/watch?v=5vRKEkogg5A},
  keywords = {actuator spacing,design implications,ERM vibration motors,haptic output,phantom sensation,vibrotactile interfaces,wearable computing},
  file = {C\:\\Users\\ra46sin\\OneDrive\\Uni\\paper\\zotero\\elsayedVibroMapUnderstandingSpacing2020.pdf;C\:\\Users\\ra46sin\\Zotero\\storage\\ZUAYVS9J\\Elsayed et al. - 2020 - VibroMap Understanding the Spacing of Vibrotactil.pdf}
}

@misc{eskaProperPostureDesigning2022,
  title = {Proper {{Posture}}: {{Designing Posture Feedback Across Musical Instruments}}},
  shorttitle = {Proper {{Posture}}},
  author = {Eska, Bettina and Niess, Jasmin and M{\"u}ller, Florian},
  year = {2022},
  month = may,
  eprint = {2205.15110},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2205.15110},
  urldate = {2024-01-09},
  abstract = {There is a recommended body posture and hand position for playing every musical instrument, allowing efficient and quick movements without blockage. Due to humans' limited cognitive capabilities, they struggle to concentrate on several things simultaneously and thus sometimes lose the correct position while playing their instrument. Incorrect positions when playing an instrument can lead to injuries and movement disorders in the long run. Previous work in HCI mainly focused on developing systems to assist in learning an instrument. However, the design space for posture correction when playing a musical instrument has not yet been explored. In this position paper, we present our vision of providing subtle vibrotactile or thermal feedback to guide the focus of attention back to the correct posture when playing a musical instrument. We discuss our concept with a focus on motion recognition and feedback modalities. Finally, we outline the next steps for future research.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Human-Computer Interaction},
  file = {C\:\\Users\\ra46sin\\OneDrive\\Uni\\paper\\zotero\\eskaProperPostureDesigning2022.pdf;C\:\\Users\\ra46sin\\Zotero\\storage\\DYZ3SBGL\\2205.html}
}

@inproceedings{eskaThermoFeetAssessingOnFoot2023,
  title = {{{ThermoFeet}}: {{Assessing On-Foot Thermal Stimuli}} for {{Directional Cues}}},
  shorttitle = {{{ThermoFeet}}},
  booktitle = {Proceedings of the 22nd {{International Conference}} on {{Mobile}} and {{Ubiquitous Multimedia}}},
  author = {Eska, Bettina and Iyalekhue, Jeff-Owens and G{\"u}nther, Sebastian and Niess, Jasmin and M{\"u}ller, Florian},
  year = {2023},
  month = dec,
  series = {{{MUM}} '23},
  pages = {166--171},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/3626705.3627974},
  urldate = {2024-01-10},
  abstract = {Thermal feedback has been studied for navigation purposes with directional cues and a variety of other use cases. Yet, to date, systems providing thermal feedback were primarily designed for the upper body, targeting hands and arms in particular. As these parts are often occupied with other tasks, there is a need to extend the design space of thermal feedback to other body parts. To close this gap, we assess thermal feedback on the user's feet. This research explores if creating stimuli representing any direction on a circle with only four actuators is possible. To evaluate this concept, we conducted a user study asking the participants to indicate the perceived direction after getting a hot or cold stimulus by direct actuation using one actuator or phantom actuation using two actuators. The results indicate that the detection accuracy was higher for cold signals. In addition, the results showed higher recognition for stimuli linked to actuator distribution than phantom sensation due to spatial summation.},
  isbn = {9798400709210},
  keywords = {direction,foot,foot-based interaction,thermal feedback},
  file = {C:\Users\ra46sin\OneDrive\Uni\paper\zotero\eskaThermoFeetAssessingOnFoot2023.pdf}
}

@inproceedings{funkAssessingAccuracyPoint2019,
  title = {Assessing the {{Accuracy}} of {{Point}} \& {{Teleport Locomotion}} with {{Orientation Indication}} for {{Virtual Reality}} Using {{Curved Trajectories}}},
  booktitle = {Proceedings of the 2019 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Funk, Markus and M{\"u}ller, Florian and Fendrich, Marco and Shene, Megan and Kolvenbach, Moritz and Dobbertin, Niclas and G{\"u}nther, Sebastian and M{\"u}hlh{\"a}user, Max},
  year = {2019},
  month = may,
  series = {{{CHI}} '19},
  pages = {1--12},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/3290605.3300377},
  urldate = {2024-01-09},
  abstract = {Room-scale Virtual Reality (VR) systems have arrived in users' homes where tracked environments are set up in limited physical spaces. As most Virtual Environments (VEs) are larger than the tracked physical space, locomotion techniques are used to navigate in VEs. Currently, in recent VR games, point \& teleport is the most popular locomotion technique. However, it only allows users to select the position of the teleportation and not the orientation that the user is facing after the teleport. This results in users having to manually correct their orientation after teleporting and possibly getting entangled by the cable of the headset. In this paper, we introduce and evaluate three different point \& teleport techniques that enable users to specify the target orientation while teleporting. The results show that, although the three teleportation techniques with orientation indication increase the average teleportation time, they lead to a decreased need for correcting the orientation after teleportation.},
  isbn = {978-1-4503-5970-2},
  video = {https://www.youtube.com/watch?v=uXctClcQu\_g},
  keywords = {locomotion,Locomotion,orientation indication,Orientation indication,point \& teleport,Point and teleport,teleportation,Teleportation,virtual environments,Virtual environments,virtual reality,Virtual reality},
  file = {C\:\\Users\\ra46sin\\OneDrive\\Uni\\paper\\zotero\\funkAssessingAccuracyPoint2019.pdf;C\:\\Users\\ra46sin\\OneDrive\\Uni\\paper\\zotero\\funkAssessingAccuracyPoint22.pdf;C\:\\Users\\ra46sin\\OneDrive\\Uni\\paper\\zotero\\funkAssessingAccuracyPoint3.pdf}
}

@inproceedings{gunther2017byo,
  title = {{{BYO}}*: {{Utilizing 3D Printed Tangible Tools}} for {{Interaction}} on {{Interactive Surfaces}}},
  booktitle = {Proceedings of the 2017 {{ACM Workshop}} on {{Interacting}} with {{Smart Objects}}},
  author = {G{\"u}nther, Sebastian and Schmitz, Martin and M{\"u}ller, Florian and Riemann, Jan and M{\"u}hlh{\"a}user, Max},
  year = {2017},
  month = mar,
  pages = {21--26},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/3038450.3038456},
  abstract = {Sharing and manipulating information are essential for collaborative work in meeting scenarios. Nowadays, people tend to bring their own devices as a result of increasing mobility possibilities. However, transferring data from one device to another can be cumbersome and tedious if restrictions like different platforms, form factors or environmental limitations apply. In this paper, we present two concepts to enrich interaction on and between devices through 3D printed customized tangibles: 1) Bring your own information, and 2) bring your own tools. For this, we enable interactivity for low-cost and passive tangible 3D printed objects by adding conductive material and make use of touch-enabled surfaces. Our system allows users to easily share digital contents across various devices and to manipulate them with individually designed tools without additional hardware required. Copyright {\textcopyright} 2017 ACM.},
  isbn = {978-1-4503-4902-4},
  keywords = {3D printing,capacitive sensing,Capacitive sensing,data manipulation,Data manipulation,data sharing,Data sharing,data visualization,Data visualization,digital fabrication,Digital fabrication,input sensing,Input sensing,rapid prototyping,Rapid prototyping,softwarecampus},
  file = {C:\Users\ra46sin\OneDrive\Uni\paper\zotero\gunther2017byo.pdf}
}

@inproceedings{gunther2018checkmate,
  title = {{{CheckMate}}: {{Exploring}} a {{Tangible Augmented Reality Interface}} for {{Remote Interaction}}},
  booktitle = {Extended {{Abstracts}} of the 2018 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {G{\"u}nther, Sebastian and M{\"u}ller, Florian and Schmitz, Martin and Riemann, Jan and Dezfuli, Niloofar and Funk, Markus and Sch{\"o}n, Dominik and M{\"u}hlh{\"a}user, Max},
  year = {2018},
  month = apr,
  pages = {1--6},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/3170427.3188647},
  isbn = {978-1-4503-5621-3},
  video = {https://www.youtube.com/watch?v=ZjG8n9P5sD8},
  file = {C:\Users\ra46sin\OneDrive\Uni\paper\zotero\gunther2018checkmate.pdf}
}

@inproceedings{gunther2018tactileglove,
  title = {{{TactileGlove}}: {{Assistive Spatial Guidance}} in {{3D Space}} through {{Vibrotactile Navigation}}},
  booktitle = {Proceedings of the 11th {{PErvasive Technologies Related}} to {{Assistive Environments Conference}}},
  author = {G{\"u}nther, Sebastian and M{\"u}ller, Florian and Funk, Markus and Kirchner, Jan and Dezfuli, Niloofar and M{\"u}hlh{\"a}user, Max},
  year = {2018},
  month = jun,
  pages = {273--280},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/3197768.3197785},
  isbn = {978-1-4503-6390-7},
  file = {C:\Users\ra46sin\OneDrive\Uni\paper\zotero\gunther2018tactileglove.pdf}
}

@inproceedings{Gunther2019,
  title = {Slappyfications: {{Towards Ubiquitous Physical}} and {{Embodied Notifications}}},
  booktitle = {Extended {{Abstracts}} of the 2019 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}} - {{CHI}} '19},
  author = {G{\"u}nther, Sebastian and M{\"u}ller, Florian and Funk, Markus and M{\"u}hlh{\"a}user, Max},
  year = {2019},
  publisher = {{ACM}},
  address = {{Glasgow, Scotland, UK}},
  doi = {10.1145/3290607.3311780},
  urldate = {2019-03-28},
  abstract = {With emerging trends of notifying persons through ubiquitous technologies [2], such as ambient light, vibrotactile, or auditory cues, none of these technologies are truly ubiquitous and have proven to be easily missed or ignored. In this work, we propose Slappyfications, a novel way of sending unmissable embodied and ubiquitous notifications through a palm-based interface [1]. Our prototype enables the users to send three types of Slappyfications: poke, slap, and the STEAM-HAMMER. Through a Wizard-of-Oz study, we show the applicability of our system in real-world scenarios. The results reveal a promising trend, as none of the participants missed a single Slappyfication.},
  isbn = {978-1-4503-5971-9},
  video = {https://www.youtube.com/watch?v=qDmrSgyV20s},
  file = {C:\Users\ra46sin\OneDrive\Uni\paper\zotero\Gunther2019.pdf}
}

@inproceedings{Gunther2019pneumact,
  title = {{{PneumAct}}: {{Pneumatic Kinesthetic Actuation}} of {{Body Joints}} in {{Virtual Reality Environments}}},
  booktitle = {Proceedings of the 2019 on {{Designing Interactive Systems Conference}}},
  author = {G{\"u}nther, Sebastian and Makhija, Mohit and M{\"u}ller, Florian and Sch{\"o}n, Dominik and M{\"u}hlh{\"a}user, Max and Funk, Markus},
  year = {2019},
  month = jun,
  pages = {227--240},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/3322276.3322302},
  urldate = {2019-07-11},
  isbn = {978-1-4503-5850-7},
  video = {https://www.youtube.com/watch?v=4lRWxzs4Rgs},
  keywords = {compressed air,force feedback,haptics,kinesthetic,pneumatic,virtual reality},
  file = {C:\Users\ra46sin\OneDrive\Uni\paper\zotero\Gunther2019pneumact.pdf}
}

@inproceedings{Gunther2020,
  title = {Therminator : {{Understanding}} the {{Interdependency}} of {{Visual}} and {{On-Body Thermal Feedback}} in {{Virtual Reality}}},
  booktitle = {Proceedings of the 2020 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}} - {{CHI}} '20},
  author = {G{\"u}nther, Sebastian and Elmoghazy, Omar and M{\"u}ller, Florian and M{\"u}hlh{\"a}user, Max and Sch{\"o}n, Dominik and Schmitz, Martin},
  year = {2020},
  volume = {20},
  pages = {1--14},
  publisher = {{ACM}},
  doi = {10.1145/3313831.3376195},
  urldate = {2020-05-11},
  abstract = {Figure 1. Therminator concepts and example VR applications showing (a) a user during our experiment with a snow visual stimulus, (b) a cold game environment with a user throwing snowballs, (c) a warm tropical islands, and (d) a firefighting simulation with a user extinguishing flames. ABSTRACT Recent advances have made Virtual Reality (VR) more realistic than ever before. This improved realism is attributed to to-day's ability to increasingly appeal to human sensations, such as visual, auditory or tactile. While research also examines temperature sensation as an important aspect, the interdepen-dency of visual and thermal perception in VR is still underex-plored. In this paper, we propose Therminator, a thermal display concept that provides warm and cold on-body feedback in VR through heat conduction of flowing liquids with different temperatures. Further, we systematically evaluate the interdependency of different visual and thermal stimuli on the temperature perception of arm and abdomen with 25 participants. As part of the results, we found varying temperature perception depending on the stimuli, as well as increasing involvement of users during conditions with matching stimuli.},
  isbn = {978-1-4503-6708-0},
  talk = {https://www.youtube.com/watch?v=p36TkvjTXfc},
  video = {https://www.youtube.com/watch?v=q5lkmqAua78},
  keywords = {Author Keywords Haptics,Haptic devices,haptics,temperature,Temperature,thermal feedback,Thermal Feedback,User studies,virtual reality,Virtual Reality CCS Concepts {\textbullet}Human-centered compu},
  file = {C:\Users\ra46sin\OneDrive\Uni\paper\zotero\Gunther2020.pdf}
}

@inproceedings{Gunther2020a,
  title = {{{PneumoVolley}}: {{Pressure-based Haptic Feedback}} on the {{Head}} through {{Pneumatic Actuation}}},
  booktitle = {Extended {{Abstracts}} of the 2020 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}} - {{CHI}} '20},
  author = {G{\"u}nther, Sebastian and Sch{\"o}n, Dominik and M{\"u}ller, Florian and M{\"u}hlh{\"a}user, Max and Schmitz, Martin},
  year = {2020},
  publisher = {{ACM}},
  doi = {10.1145/3334480.3382916},
  urldate = {2020-05-11},
  abstract = {Haptic Feedback brings immersion and presence in Virtual Reality (VR) to the next level. While research proposes the usage of various tactile sensations, such as vibration or ultrasound approaches, the potential applicability of pressure feedback on the head is still under-explored. In this paper, we contribute concepts and design considerations for pressure-based feedback on the head through pneumatic actuation. As a proof-of-concept implementing our pressure-based haptics, we further present PneumoVolley: a VR experience similar to the classic Volleyball game but played with the head. In an exploratory user study with 9 participants, we evaluated our concepts and identified a significantly increased involvement compared to a no-haptics baseline along with high realism and enjoyment ratings using pressure-based feedback on the head in VR. LBW033, Page 1 CHI 2020 Late-Breaking Work},
  isbn = {978-1-4503-6819-3},
  talk = {https://www.youtube.com/watch?v=QWLsdfNxgeA},
  video = {https://www.youtube.com/watch?v=ZKnV8HrUx9M},
  keywords = {2020,April 25-30,CCS Concepts {\textbullet}Human-centered computing {\textrightarrow} Human com,CHI 2020,Haptic devices,Haptics,HI,Honolulu,Pressure feedback,Pressure Feedback,USA Author Keywords Virtual Reality,User studies,Virtual reality,Volleyball},
  file = {C:\Users\ra46sin\OneDrive\Uni\paper\zotero\Gunther2020a.pdf}
}

@inproceedings{guntherActuBoardOpenRapid2021,
  title = {{{ActuBoard}}: {{An Open Rapid Prototyping Platform}} to Integrate {{Hardware Actuators}} in {{Remote Applications}}},
  shorttitle = {{{ActuBoard}}},
  booktitle = {Companion of the 2021 {{ACM SIGCHI Symposium}} on {{Engineering Interactive Computing Systems}}},
  author = {G{\"u}nther, Sebastian and M{\"u}ller, Florian and H{\"u}bner, Felix and M{\"u}hlh{\"a}user, Max and Matviienko, Andrii},
  year = {2021},
  month = jun,
  series = {{{EICS}} '21},
  pages = {70--76},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/3459926.3464757},
  urldate = {2024-01-09},
  abstract = {Prototyping is an essential step in developing tangible experiences and novel devices, ranging from haptic feedback to wearables. However, prototyping of actuated devices nowadays often requires repetitive and time-consuming steps, such as wiring, soldering, and programming basic communication, before HCI researchers and designers can focus on their primary interest: designing interaction. In this paper, we present ActuBoard, a prototyping platform to support 1) quick assembly, 2) less preparation work, and 3) the inclusion of non-tech-savvy users. With ActuBoard, users are not required to create complex circuitry, write a single line of firmware, or implementing communication protocols. Acknowledging existing systems, our platform combines the flexibility of low-level microcontrollers and ease-of-use of abstracted tinker platforms to control actuators from separate applications. As further contribution, we highlight the technical specifications and published the ActuBoard platform as Open Source.},
  code = {https://git.tk.informatik.tu-darmstadt.de/sebastian.guenther/actuboard-public},
  isbn = {978-1-4503-8449-0},
  keywords = {actuators,haptics,hardware,open source,rapid prototyping,tinkering,virtual reality},
  file = {C\:\\Users\\ra46sin\\OneDrive\\Uni\\paper\\zotero\\guntherActuBoardOpenRapid2021.pdf;C\:\\Users\\ra46sin\\OneDrive\\Uni\\paper\\zotero\\guntherActuBoardOpenRapid22.pdf}
}

@inproceedings{guntherSmoothSteelWool2022,
  title = {Smooth as {{Steel Wool}}: {{Effects}} of {{Visual Stimuli}} on the {{Haptic Perception}} of {{Roughness}} in {{Virtual Reality}}},
  shorttitle = {Smooth as {{Steel Wool}}},
  booktitle = {Proceedings of the 2022 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {G{\"u}nther, Sebastian and Rasch, Julian and Sch{\"o}n, Dominik and M{\"u}ller, Florian and Schmitz, Martin and Riemann, Jan and Matviienko, Andrii and M{\"u}hlh{\"a}user, Max},
  year = {2022},
  month = apr,
  series = {{{CHI}} '22},
  pages = {1--17},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/3491102.3517454},
  urldate = {2024-01-09},
  abstract = {Haptic Feedback is essential for lifelike Virtual Reality (VR) experiences. To provide a wide range of matching sensations of being touched or stroked, current approaches typically need large numbers of different physical textures. However, even advanced devices can only accommodate a limited number of textures to remain wearable. Therefore, a better understanding is necessary of how expectations elicited by different visualizations affect haptic perception, to achieve a balance between physical constraints and great variety of matching physical textures. In this work, we conducted an experiment (N=31) assessing how the perception of roughness is affected within VR. We designed a prototype for arm stroking and compared the effects of different visualizations on the perception of physical textures with distinct roughnesses. Additionally, we used the visualizations' real-world materials, no-haptics and vibrotactile feedback as baselines. As one result, we found that two levels of roughness can be sufficient to convey a realistic illusion.},
  isbn = {978-1-4503-9157-3},
  talk = {https://www.youtube.com/watch?v=h0FQZQ26uoU},
  video = {https://www.youtube.com/watch?v=IdbIvF004UA},
  keywords = {caress,haptic feedback,perception,roughness,stroke,touch,virtual reality},
  file = {C:\Users\ra46sin\OneDrive\Uni\paper\zotero\guntherSmoothSteelWool2022.pdf}
}

@article{hirschMyHeartWill2023a,
  title = {My {{Heart Will Go On}}: {{Implicitly Increasing Social Connectedness}} by {{Visualizing Asynchronous Players}}' {{Heartbeats}} in {{VR Games}}},
  shorttitle = {My {{Heart Will Go On}}},
  author = {Hirsch, Linda and M{\"u}ller, Florian and Chiossi, Francesco and Benga, Theodor and Butz, Andreas Martin},
  year = {2023},
  month = oct,
  journal = {Proceedings of the ACM on Human-Computer Interaction},
  volume = {7},
  number = {CHI PLAY},
  pages = {411:976--411:1001},
  publisher = {{ACM}},
  doi = {10.1145/3611057},
  urldate = {2024-01-09},
  abstract = {Social games benefit from social connectedness between players because it improves the gaming experience and increases enjoyment. In virtual reality (VR), various approaches, such as avatars, are developed for multi-player games to increase social connectedness. However, these approaches are lacking in single-player games. To increase social connectedness in such games, our work explores the visualization of physiological data from asynchronous players, i.e., electrocardiogram (ECG). We identified two visualization dimensions, the number of players, and the visualization style, after a design workshop with experts (N=4) and explored them in a single-user virtual escape room game. We spatially and temporally integrated the visualizations and compared two times two visualizations against a baseline condition without visualization in a within-subject lab study (N=34). All but one visualization significantly increased participants' feelings of social connectedness. Heart icons triggered the strongest feeling of connectedness, understanding, and perceived support in playing the game.},
  talk = {https://www.youtube.com/watch?v=PuHfic0rmAs},
  keywords = {asynchronous,escape room,heartbeat,physiological data,shared biodata,single player,social VR,visualization,VR game},
  file = {C:\Users\ra46sin\OneDrive\Uni\paper\zotero\hirschMyHeartWill2023a.pdf}
}

@inproceedings{hirzleWhenXRAI2023a,
  title = {When {{XR}} and {{AI Meet}} - {{A Scoping Review}} on {{Extended Reality}} and {{Artificial Intelligence}}},
  booktitle = {Proceedings of the 2023 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Hirzle, Teresa and M{\"u}ller, Florian and Draxler, Fiona and Schmitz, Martin and Knierim, Pascal and Hornb{\ae}k, Kasper},
  year = {2023},
  month = apr,
  series = {{{CHI}} '23},
  pages = {1--45},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/3544548.3581072},
  urldate = {2024-01-09},
  abstract = {Research on Extended Reality (XR) and Artificial Intelligence (AI) is booming, which has led to an emerging body of literature in their intersection. However, the main topics in this intersection are unclear, as are the benefits of combining XR and AI. This paper presents a scoping review that highlights how XR is applied in AI research and vice versa. We screened 2619 publications from 203 international venues published between 2017 and 2021, followed by an in-depth review of 311 papers. Based on our review, we identify five main topics at the intersection of XR and AI, showing how research at the intersection can benefit each other. Furthermore, we present a list of commonly used datasets, software, libraries, and models to help researchers interested in this intersection. Finally, we present 13 research opportunities and recommendations for future work in XR and AI research.},
  isbn = {978-1-4503-9421-5},
  talk = {https://www.youtube.com/watch?v=VDg-2Pz9lj8},
  keywords = {artificial intelligence,extended reality,scoping review},
  file = {C:\Users\ra46sin\OneDrive\Uni\paper\zotero\hirzleWhenXRAI2023a.pdf}
}

@article{joStudyOnbodyUser2015,
  title = {A {{Study}} of {{On-body User Interface}}: {{PiAM}}({{Palm interAction Module}})},
  shorttitle = {A {{Study}} of {{On-body User Interface}}},
  author = {Jo, Seng-Kyoun and 이현우 and Jinsul, Kim and M{\"u}ller, Florian and Khalilbeigi, Mohammed and M{\"u}hlh{\"a}user, Max},
  year = {2015},
  month = dec,
  journal = {Journal of Knowledge Information Technology and Systems},
  volume = {10},
  number = {6},
  pages = {687--697},
  publisher = {{Korea Knowledge Information Technology Society}},
  issn = {1975-7700},
  urldate = {2024-01-09},
  abstract = {User interface providing easy and efficient control environments with user is emerging as a core keyword in ICT market and smart phone industry as the influence and importance of UI has increased recently. To succeed in ICT market, however, UI has to satisfy industrial requirements including simple and easy control, intuitional access and high recognition. Thus study for providing user-friendly interface has been actively researched. In this paper, we investigate user interface based on human's},
  langid = {english},
  keywords = {⛔ No DOI found},
  file = {C:\Users\ra46sin\OneDrive\Uni\paper\zotero\joStudyOnbodyUser2015.pdf}
}

@patent{Kim2017,
  title = {Manufacturing Method of Sensor Using 3d Printing and 3d Printer Thereof},
  author = {Jung, Woo Sug and Kim, Hwa Suk and Jeon, Jun Ki and Jo, Seong Kyoun and Lee, Hyun Woo and Schmitz, Martin and M{\"u}ller, Florian and Leister, Andreas and Riemann, Jan and Dezfuli, Niloofar and M{\"u}hlh{\"a}user, Max and Khalilbeigi, Mohammadreza},
  year = {2017},
  number = {43},
  publisher = {{TU Darmstadt}},
  urldate = {2019-03-28},
  abstract = {Disclosed is a manufacturing method of a sensor by using 3D printing and 3D printer therefor. According to an embodiment of the present disclosure , a manufacturing method of a sensor by using 3D printing includes : forming a first shape having an inner space by using a non-conduc tive material , and simultaneously or sequentially , forming an electrode at a preset location in the inner space by using a conductive material ; injecting conductive liquid into the inner space ; and forming a second shape on the first shape by using the non-conductive material to seal the inner space of the first shape .},
  website = {https://patents.google.com/patent/US20180312398A1/en?oq=20180312398}
}

@article{koschNotiBikeAssessingTarget2022,
  title = {{{NotiBike}}: {{Assessing Target Selection Techniques}} for {{Cyclist Notifications}} in {{Augmented Reality}}},
  shorttitle = {{{NotiBike}}},
  author = {Kosch, Thomas and Matviienko, Andrii and M{\"u}ller, Florian and Bersch, Jessica and Katins, Christopher and Sch{\"o}n, Dominik and M{\"u}hlh{\"a}user, Max},
  year = {2022},
  month = sep,
  journal = {Proceedings of the ACM on Human-Computer Interaction},
  volume = {6},
  number = {MHCI},
  pages = {197:1--197:24},
  publisher = {{ACM}},
  doi = {10.1145/3546732},
  urldate = {2024-01-09},
  abstract = {Cyclists' attention is often compromised when interacting with notifications in traffic, hence increasing the likelihood of road accidents. To address this issue, we evaluate three notification interaction modalities and investigate their impact on the interaction performance while cycling: gaze-based Dwell Time, Gestures, and Manual And Gaze Input Cascaded (MAGIC) Pointing. In a user study (N=18), participants confirmed notifications in Augmented Reality (AR) using the three interaction modalities in a simulated biking scenario. We assessed the efficiency regarding reaction times, error rates, and perceived task load. Our results show significantly faster response times for MAGIC Pointing compared to Dwell Time and Gestures, while Dwell Time led to a significantly lower error rate compared to Gestures. Participants favored the MAGIC Pointing approach, supporting cyclists in AR selection tasks. Our research sets the boundaries for more comfortable and easier interaction with notifications and discusses implications for target selections in AR while cycling.},
  video = {https://www.youtube.com/watch?v=hTYBTULau7U},
  keywords = {augmented reality,cycling,notifications,selection},
  file = {C:\Users\ra46sin\OneDrive\Uni\paper\zotero\koschNotiBikeAssessingTarget2022.pdf}
}

@inproceedings{liLocationAwareVirtualReality2023a,
  title = {Location-{{Aware Virtual Reality}} for {{Situational Awareness On}} the {{Road}}},
  booktitle = {Proceedings of the 2023 {{ACM Symposium}} on {{Spatial User Interaction}}},
  author = {Li, Jingyi and Mayer, Alexandra and M{\"u}ller, Florian and Matviienko, Andrii and Butz, Andreas},
  year = {2023},
  month = oct,
  series = {{{SUI}} '23},
  pages = {1--12},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/3607822.3614530},
  urldate = {2024-01-09},
  abstract = {When future passengers are immersed in Virtual Reality (VR), the resulting disconnection from the physical world may degrade their situational awareness on the road. We propose incorporating real-world cues into virtual experiences when passing specific locations to address this. We designed two visualizations using points of interest (POIs), street names alone or combined with live street views. We compared them to two baselines, persistently displaying live cues (Always Live) or no cues (Always VR). In a field study (N=17), participants estimated their locations while exposed to VR entertainment during car rides. The results show that adding environmental cues inevitably degrades VR presence compared to Always VR. However, POI-triggered Text\&Live preserves VR presence better than Always Live and attracts user attention to the road more than POI-triggered Text. We discuss situational awareness challenges for using mobile VR on the road and potential incorporation strategies across transport contexts.},
  isbn = {9798400702815},
  keywords = {in-vehicle virtual reality,location-aware system,POI,situational awareness},
  file = {C:\Users\ra46sin\OneDrive\Uni\paper\zotero\liLocationAwareVirtualReality2023a.pdf}
}

@inproceedings{markyLetFretsAssisting2021,
  title = {Let's {{Frets}}! {{Assisting Guitar Students During Practice}} via {{Capacitive Sensing}}},
  booktitle = {Proceedings of the 2021 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Marky, Karola and Wei{\ss}, Andreas and Matviienko, Andrii and Brandherm, Florian and Wolf, Sebastian and Schmitz, Martin and Krell, Florian and M{\"u}ller, Florian and M{\"u}hlh{\"a}user, Max and Kosch, Thomas},
  year = {2021},
  month = may,
  series = {{{CHI}} '21},
  pages = {1--12},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/3411764.3445595},
  urldate = {2024-01-09},
  abstract = {Learning a musical instrument requires regular exercise. However, students are often on their own during their practice sessions due to the limited time with their teachers, which increases the likelihood of mislearning playing techniques. To address this issue, we present Let's Frets - a modular guitar learning system that provides visual indicators and capturing of finger positions on a 3D-printed capacitive guitar fretboard. We based the design of Let's Frets on requirements collected through in-depth interviews with professional guitarists and teachers. In a user study (N=24), we evaluated the feedback modules of Let's Frets against fretboard charts. Our results show that visual indicators require the least time to realize new finger positions while a combination of visual indicators and position capturing yielded the highest playing accuracy. We conclude how Let's Frets enables independent practice sessions that can be translated to other musical instruments.},
  isbn = {978-1-4503-8096-6},
  talk = {https://www.youtube.com/watch?v=YhLuCpgnaBg},
  video = {https://www.youtube.com/watch?v=vFx8c5aF6vA},
  keywords = {capacitive sensing,Capacitive sensing,musical instruments,Musical instruments,support setup,Support setup},
  file = {C\:\\Users\\ra46sin\\OneDrive\\Uni\\paper\\zotero\\markyLetFretsAssisting2021.pdf;C\:\\Users\\ra46sin\\OneDrive\\Uni\\paper\\zotero\\markyLetFretsAssisting22.pdf}
}

@inproceedings{markyLetFretsMastering2021,
  title = {Let's {{Frets}}! {{Mastering Guitar Playing}} with {{Capacitive Sensing}} and {{Visual Guidance}}},
  booktitle = {Extended {{Abstracts}} of the 2021 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Marky, Karola and Wei{\ss}, Andreas and M{\"u}ller, Florian and Schmitz, Martin and M{\"u}hlh{\"a}user, Max and Kosch, Thomas},
  year = {2021},
  month = may,
  series = {{{CHI EA}} '21},
  pages = {1--4},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/3411763.3451536},
  urldate = {2024-01-09},
  abstract = {Mastering the guitar requires regular exercise to develop new skills and maintain existing abilities. We present Let's Frets - a modular guitar support system that provides visual guidance through LEDs that are integrated into a capacitive fretboard to support the practice of chords, scales, melodies, and exercises. Additional feedback is provided through a 3D-printed fretboard that senses the finger positions through capacitive sensing. We envision Let's Frets as an integrated guitar support system that raises the awareness of guitarists about their playing styles, their training progress, the composition of new pieces, and facilitating remote collaborations between teachers as well as guitar students. This interactivity demonstrates Let's Frets with an augmented fretboard and supporting software that runs on a mobile device.},
  isbn = {978-1-4503-8095-9},
  video = {https://www.youtube.com/watch?v=a0C4TkbiRfg},
  keywords = {capacitive sensing,musical instruments,support setup},
  file = {C\:\\Users\\ra46sin\\OneDrive\\Uni\\paper\\zotero\\markyLetFretsMastering2021.pdf;C\:\\Users\\ra46sin\\OneDrive\\Uni\\paper\\zotero\\markyLetFretsMastering22.pdf}
}

@inproceedings{Matviienko2022,
  title = {{{SkyPort}}: {{Investigating 3D Teleportation Methods}} in {{Virtual Environments}}},
  booktitle = {{{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Matviienko, Andrii and M{\"u}ller, Florian and Schmitz, Martin and Fendrich, Marco and M{\"u}hlh{\"a}user, Max},
  year = {2022},
  month = apr,
  pages = {1--11},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/3491102.3501983},
  urldate = {2022-08-12},
  abstract = {Figure 1: Two possible scenarios for teleportation in 3D space: a user is teleporting horizontally to a target using a parabolic aiming method (left) and a user is teleporting vertically to a target using a linear aiming method (right). ABSTRACT Teleportation has become the de facto standard of locomotion in Virtual Reality (VR) environments. However, teleportation with parabolic and linear target aiming methods is restricted to horizontal 2D planes and it is unknown how they transfer to the 3D space. In this paper, we propose six 3D teleportation methods in virtual environments based on the combination of two existing aiming methods (linear and parabolic) and three types of transitioning to a target (instant, interpolated and continuous). To investigate the performance of the proposed teleportation methods, we conducted a controlled lab experiment (N = 24) with a mid-air coin collection task to assess accuracy, efciency and VR sickness. We discovered that the linear aiming method leads to faster and more accurate target selection. Moreover, a combination of linear aiming and instant transitioning leads to the highest efciency and accuracy without increasing VR sickness. {\textbullet} Human-centered computing {\textrightarrow} Virtual reality; User studies ; Empirical studies in HCI.},
  isbn = {978-1-4503-9157-3},
  talk = {https://www.youtube.com/watch?v=sOkzPZnlAeE},
  keywords = {locomotion,teleportation,virtual environments,virtual reality},
  file = {C:\Users\ra46sin\OneDrive\Uni\paper\zotero\Matviienko2022.pdf}
}

@article{matviienkoBabyYouCan2022,
  title = {Baby, {{You}} Can {{Ride}} My {{Bike}}: {{Exploring Maneuver Indications}} of {{Self-Driving Bicycles}} Using a {{Tandem Simulator}}},
  shorttitle = {"{{Baby}}, {{You}} Can {{Ride}} My {{Bike}}"},
  author = {Matviienko, Andrii and Mehmedovic, Damir and M{\"u}ller, Florian and M{\"u}hlh{\"a}user, Max},
  year = {2022},
  month = sep,
  journal = {Proceedings of the ACM on Human-Computer Interaction},
  volume = {6},
  number = {MHCI},
  pages = {188:1--188:21},
  publisher = {{ACM}},
  doi = {10.1145/3546723},
  urldate = {2023-01-16},
  abstract = {We envision a future where self-driving bicycles can take us to our destinations. This allows cyclists to use their time on the bike efficiently for work or relaxation without having to focus their attention on traffic. In the related field of self-driving cars, research has shown that communicating the planned route to passengers plays an important role in building trust in automation and situational awareness. For self-driving bicycles, this information transfer will be even more important, as riders will need to actively compensate for the movement of a self-driving bicycle to maintain balance. In this paper, we investigate maneuver indications for self-driving bicycles: (1) ambient light in a helmet, (2) head-up display indications, (3) speech feedback, (4) vibration on the handlebar, and (5) no assistance. To evaluate these indications, we conducted an outdoor experiment (N = 25) in a proposed tandem simulator consisting of a tandem bicycle with a steering and braking control on the back seat and a rider in full control of it. Our results indicate that riders respond faster to visual cues and focus comparably on the reading task while riding with and without maneuver indications. Additionally, we found that the tandem simulator is realistic, safe, and creates an awareness of a human cyclist controlling the tandem.},
  video = {https://www.youtube.com/watch?v=czOciHFRDk4},
  keywords = {maneuver indications,self-driving bicycles,tandem},
  file = {C:\Users\ra46sin\OneDrive\Uni\paper\zotero\matviienkoBabyYouCan2022.pdf}
}

@inproceedings{matviienkoBikeARUnderstandingCyclists2022,
  title = {{{BikeAR}}: {{Understanding Cyclists}}' {{Crossing Decision-Making}} at {{Uncontrolled Intersections}} Using {{Augmented Reality}}},
  shorttitle = {{{BikeAR}}},
  booktitle = {Proceedings of the 2022 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Matviienko, Andrii and M{\"u}ller, Florian and Sch{\"o}n, Dominik and Seesemann, Paul and G{\"u}nther, Sebastian and M{\"u}hlh{\"a}user, Max},
  year = {2022},
  month = apr,
  series = {{{CHI}} '22},
  pages = {1--15},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/3491102.3517560},
  urldate = {2023-08-15},
  abstract = {Cycling has become increasingly popular as a means of transportation. However, cyclists remain a highly vulnerable group of road users. According to accident reports, one of the most dangerous situations for cyclists are uncontrolled intersections, where cars approach from both directions. To address this issue and assist cyclists in crossing decision-making at uncontrolled intersections, we designed two visualizations that: (1) highlight occluded cars through an X-ray vision and (2) depict the remaining time the intersection is safe to cross via a Countdown. To investigate the efficiency of these visualizations, we proposed an Augmented Reality simulation as a novel evaluation method, in which the above visualizations are represented as AR, and conducted a controlled experiment with 24 participants indoors. We found that the X-ray ensures a fast selection of shorter gaps between cars, while the Countdown facilitates a feeling of safety and provides a better intersection overview.},
  isbn = {978-1-4503-9157-3},
  talk = {https://www.youtube.com/watch?v=eA7D239WOd0},
  video = {https://www.youtube.com/watch?v=YKsDlPmSd68},
  keywords = {augmented reality,crossing decision-making,cyclist safety},
  file = {C:\Users\ra46sin\OneDrive\Uni\paper\zotero\matviienkoBikeARUnderstandingCyclists2022.pdf}
}

@inproceedings{matviienkoEScootARExploringUnimodal2022,
  title = {E-{{ScootAR}}: {{Exploring Unimodal Warnings}} for {{E-Scooter Riders}} in {{Augmented Reality}}},
  shorttitle = {E-{{ScootAR}}},
  booktitle = {Extended {{Abstracts}} of the 2022 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Matviienko, Andrii and M{\"u}ller, Florian and Sch{\"o}n, Dominik and Fayard, R{\'e}gis and Abaspur, Salar and Li, Yi and M{\"u}hlh{\"a}user, Max},
  year = {2022},
  month = apr,
  series = {{{CHI EA}} '22},
  pages = {1--7},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/3491101.3519831},
  urldate = {2024-01-09},
  abstract = {Micro-mobility is becoming a more popular means of transportation. However, this increased popularity brings its challenges. In particular, the accident rates for E-Scooter riders increase, which endangers the riders and other road users. In this paper, we explore the idea of augmenting E-Scooters with unimodal warnings to prevent collisions with other road users, which include Augmented Reality (AR) notifications, vibrotactile feedback on the handlebar, and auditory signals in the AR glasses. We conducted an outdoor experiment (N = 13) using an Augmented Reality simulation and compared these types of warnings in terms of reaction time, accident rate, and feeling of safety. Our results indicate that AR and auditory warnings lead to shorter reaction times, have a better perception, and create a better feeling of safety than vibrotactile warnings. Moreover, auditory signals have a higher acceptance by the riders compared to the other two types of warnings.},
  isbn = {978-1-4503-9156-6},
  talk = {https://www.youtube.com/watch?v=uYBIB51xgNs},
  keywords = {augmented reality,E-Scooter,micro-mobility,traffic safety},
  file = {C:\Users\ra46sin\OneDrive\Uni\paper\zotero\matviienkoEScootARExploringUnimodal2022.pdf}
}

@inproceedings{matviienkoReducingVirtualReality2022,
  title = {Reducing {{Virtual Reality Sickness}} for {{Cyclists}} in {{VR Bicycle Simulators}}},
  booktitle = {Proceedings of the 2022 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Matviienko, Andrii and M{\"u}ller, Florian and Zickler, Marcel and Gasche, Lisa Alina and Abels, Julia and Steinert, Till and M{\"u}hlh{\"a}user, Max},
  year = {2022},
  month = apr,
  series = {{{CHI}} '22},
  pages = {1--14},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/3491102.3501959},
  urldate = {2024-01-09},
  abstract = {Virtual Reality (VR) bicycle simulations aim to recreate the feeling of riding a bicycle and are commonly used in many application areas. However, current solutions still create mismatches between the visuals and physical movement, which causes VR sickness and diminishes the cycling experience. To reduce VR sickness in bicycle simulators, we conducted two controlled lab experiments addressing two main causes of VR sickness: (1) steering methods and (2) cycling trajectory. In the first experiment (N = 18) we compared handlebar, HMD, and upper-body steering methods. In the second experiment (N = 24) we explored three types of movement in VR (1D, 2D, and 3D trajectories) and three countermeasures (airflow, vibration, and dynamic Field-of-View) to reduce VR sickness. We found that handlebar steering leads to the lowest VR sickness without decreasing cycling performance and airflow suggests to be the most promising method to reduce VR sickness for all three types of trajectories.},
  isbn = {978-1-4503-9157-3},
  talk = {https://www.youtube.com/watch?v=BC5fpXVYnnA},
  keywords = {bicycle simulators,cycling,virtual reality,VR sickness},
  file = {C:\Users\ra46sin\OneDrive\Uni\paper\zotero\matviienkoReducingVirtualReality2022.pdf}
}

@inproceedings{matviienkoVRtangiblesAssistingChildren2021,
  title = {{{VRtangibles}}: {{Assisting Children}} in {{Creating Virtual Scenes}} Using {{Tangible Objects}} and {{Touch Input}}},
  shorttitle = {{{VRtangibles}}},
  booktitle = {Extended {{Abstracts}} of the 2021 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Matviienko, Andrii and Langer, Marcel and M{\"u}ller, Florian and Schmitz, Martin and M{\"u}hlh{\"a}user, Max},
  year = {2021},
  month = may,
  series = {{{CHI EA}} '21},
  pages = {1--7},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/3411763.3451671},
  urldate = {2024-01-09},
  abstract = {Children are increasingly exposed to virtual reality (VR) technology as end-users. However, they miss an opportunity to become active creators due to the barrier of insufficient technical background. Creating scenes in VR requires considerable programming knowledge and excludes non-tech-savvy users, e.g., school children. In this paper, we showcase a system called VRtangibles, which combines tangible objects and touch input to create virtual scenes without programming. With VRtangibles, we aim to engage children in the active creation of virtual scenes via playful hands-on activities. From the lab study with six school children, we discovered that the majority of children were successful in creating virtual scenes using VRtangibles and found it engaging and fun to use.},
  isbn = {978-1-4503-8095-9},
  talk = {https://www.youtube.com/watch?v=wW-d9yokhYI},
  keywords = {children,education,tangibles,touch input,virtual reality},
  file = {C\:\\Users\\ra46sin\\OneDrive\\Uni\\paper\\zotero\\matviienkoVRtangiblesAssistingChildren2021.pdf;C\:\\Users\\ra46sin\\OneDrive\\Uni\\paper\\zotero\\matviienkoVRtangiblesAssistingChildren22.pdf}
}

@inproceedings{Meurisch2018,
  title = {{{UPA}}'18: 3rd {{International Workshop}} on {{Ubiquitous Personal Assistance}}},
  booktitle = {Proceedings of the 2018 {{ACM International Joint Conference}} and 2018 {{International Symposium}} on {{Pervasive}} and {{Ubiquitous Computing}} and {{Wearable Computers}} - {{UbiComp}} '18},
  author = {Meurisch, Christian and M{\"u}hlh{\"a}user, Max and Scholl, Philipp M. and Naeem, Usman and Pejovi{\'c}, Veljko and M{\"u}ller, Florian and Di Lascio, Elena and Kuo, Pei-Yi Patricia and Kauschke, Sebastian and Azam, Muhammad Awais},
  year = {2018},
  pages = {766--769},
  publisher = {{ACM Press}},
  address = {{New York, New York, USA}},
  doi = {10.1145/3267305.3274133},
  urldate = {2019-01-29},
  isbn = {978-1-4503-5966-5},
  keywords = {anticipatory mobile computing,Anticipatory mobile computing,digital personal assistants,Digital personal assistants,personalization,Personalization,proactive support,Proactive support,ubiquitous devices,Ubiquitous devices},
  file = {C:\Users\ra46sin\OneDrive\Uni\paper\zotero\Meurisch2018.pdf}
}

@article{meurischExploringUserExpectations2020,
  title = {Exploring {{User Expectations}} of {{Proactive AI Systems}}},
  author = {Meurisch, Christian and {Mihale-Wilson}, Cristina A. and Hawlitschek, Adrian and Giger, Florian and M{\"u}ller, Florian and Hinz, Oliver and M{\"u}hlh{\"a}user, Max},
  year = {2020},
  month = dec,
  journal = {Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies},
  volume = {4},
  number = {4},
  pages = {146:1--146:22},
  publisher = {{ACM}},
  doi = {10.1145/3432193},
  urldate = {2024-01-09},
  abstract = {Recent advances in artificial intelligence (AI) enabled digital assistants to evolve towards proactive user support. However, expectations as to when and to what extent assistants should take the initiative are still unclear; discrepancies to the actual system behavior might negatively affect user acceptance. In this paper, we present an in-the-wild study for exploring user expectations of such user-supporting AI systems in terms of different proactivity levels and use cases. We collected 3,168 in-situ responses from 272 participants through a mixed method of automated user tracking and context-triggered surveying. Using a data-driven approach, we gain insights into initial expectations and how they depend on different human factors and contexts. Our insights can help to design AI systems with varying degree of proactivity and preset to meet individual expectations.},
  keywords = {artificial intelligence,personal assistants,privacy demands,proactivity},
  file = {C\:\\Users\\ra46sin\\OneDrive\\Uni\\paper\\zotero\\meurischExploringUserExpectations2020.pdf;C\:\\Users\\ra46sin\\OneDrive\\Uni\\paper\\zotero\\meurischExploringUserExpectations22.pdf}
}

@book{Mueller_Diss_published,
  title = {Around-{{Body Interaction}}: {{Leveraging Limb-movements}} for {{Interacting}} in a {{Digitally Augmented Physical World}}},
  author = {M{\"u}ller, Florian},
  year = {2020},
  publisher = {{TUprints}},
  address = {{Darmstadt}},
  doi = {10.25534/tuprints-00011388},
  isbn = {978-3-7502-7761-8},
  selected = {true},
  annotation = {tes.arxiv: 2303.15913},
  file = {C:\Users\ra46sin\OneDrive\Uni\paper\zotero\Mueller_Diss_published.pdf}
}

@book{muhlhauserMuC22Proceedings2022,
  title = {{{MuC}} '22: {{Proceedings}} of {{Mensch}} Und {{Computer}} 2022},
  author = {M{\"u}hlhauser, Max and Reuter, Christian and Pfleging, Bastian and Kosch, Thomas and Matviienko, Andrii and Gerling, Kathrin and Mayer, Sven and Heuten, Wilko and D{\"o}ring, Tanja and M{\"u}ller, Florian and Schmitz, Martin},
  year = {2022},
  month = sep,
  publisher = {{Association for Computing Machinery}},
  doi = {10.1145/3543758},
  isbn = {978-1-4503-9690-5}
}

@inproceedings{Muller2015,
  title = {A {{Study}} on {{Proximity-based Hand Input}} for {{One-handed Mobile Interaction}}},
  booktitle = {Proceedings of the 3rd {{ACM Symposium}} on {{Spatial User Interaction}}},
  author = {M{\"u}ller, Florian and Khalilbeigi, Mohammadreza and Dezfuli, Niloofar and Sahami Shirazi, Alireza and G{\"u}nther, Sebastian and M{\"u}hlh{\"a}user, Max},
  year = {2015},
  month = aug,
  pages = {53--56},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/2788940.2788955},
  abstract = {On-body user interfaces utilize the human's skin for both sensing input and displaying graphical output. In this paper, we present how the degree of freedom offered by the elbow joint, i.e., exion and extension, can be leveraged to extend the input space of projective user interfaces. The user can move his hand towards or away from himself to browse through a multi-layer information space. We conducted a controlled experiment to investigate how accurately and ef-ficiently users can interact in the space. The results revealed that the accuracy and effciency of proximity-based interactions mainly depend on the traveling distance to the target layer while neither the hand side nor the direction of interaction have a signifcant inuence. Based on our findings, we propose guidelines for designing on-body user interfaces.},
  isbn = {978-1-4503-3703-8},
  keywords = {Design,Human Factors,Measurement,Measurement.,notion,tosync},
  file = {C:\Users\ra46sin\OneDrive\Uni\paper\zotero\Muller2015.pdf}
}

@inproceedings{Muller2015b,
  title = {Palm-Based {{Interaction}} with {{Head-mounted Displays}}},
  booktitle = {Proceedings of the 17th {{International Conference}} on {{Human-Computer Interaction}} with {{Mobile Devices}} and {{Services Adjunct}} - {{MobileHCI}} '15},
  author = {M{\"u}ller, Florian and Dezfuli, Niloofar and M{\"u}hlh{\"a}user, Max and Schmitz, Martin and Khalilbeigi, Mohammadreza},
  year = {2015},
  pages = {963--965},
  publisher = {{ACM Press}},
  address = {{New York, New York, USA}},
  doi = {10.1145/2786567.2794314},
  urldate = {2018-08-08},
  isbn = {978-1-4503-3653-6},
  keywords = {Design,Head-mounted Displays,Human Factors,notion},
  file = {C:\Users\ra46sin\OneDrive\Uni\paper\zotero\Muller2015b.pdf}
}

@inproceedings{muller2016proxiwatch,
  title = {{{ProxiWatch}}: {{Enhancing}} Smartwatch Interaction through Proximity-Based Hand Input},
  booktitle = {Proceedings of the 2016 {{CHI Conference Extended Abstracts}} on {{Human Factors}} in {{Computing Systems}} - {{CHI EA}} '16},
  author = {M{\"u}ller, Florian and G{\"u}nther, Sebastian and Dezfuli, Niloofar and Khalilbeigi, Mohammadreza and M{\"u}hlh{\"a}user, Max},
  year = {2016},
  pages = {2617--2624},
  publisher = {{ACM}},
  address = {{New York, New York, USA}},
  doi = {10.1145/2851581.2892450},
  urldate = {2018-08-08},
  abstract = {Smartwatches allow ubiquitous and mobile interaction with digital contents. Because of the small screen sizes, tradi-tional interaction techniques are often not applicable. In this work, we show how the degree of freedom offered by the elbow joint, i.e., flexion and extension, can be leveraged as an additional one-handed input modality for smartwatches. By moving the watch towards or away from the body, the user is able to provide input to the smartwatch without a second hand. We present the results of a controlled ex-periment focusing on the human capabilities for proximity-based interaction. Based on the results, we propose guide-lines for designing proximity-based smartwatch interfaces and present ProxiWatch: a one-handed and proximity-based input modality for smartwatches alongside a proto-typical implementation.},
  isbn = {978-1-4503-4082-3},
  keywords = {design,Design,human factors,Human factors,Human Factors,measurement,Measurement,notion,smartwatch,Smartwatch,Smartwatch.},
  file = {C:\Users\ra46sin\OneDrive\Uni\paper\zotero\muller2016proxiwatch.pdf}
}

@inproceedings{muller2017cloudbits,
  title = {Cloudbits: {{Supporting Conversations}} through {{Augmented Zero-query Search Visualization}}},
  booktitle = {Proceedings of the 5th {{Symposium}} on {{Spatial User Interaction}}},
  author = {M{\"u}ller, Florian and G{\"u}nther, Sebastian and Nejad, Azita Hosseini and Dezfuli, Niloofar and Khalilbeigi, Mohammadreza and M{\"u}hlh{\"a}user, Max},
  year = {2017},
  month = oct,
  volume = {17},
  pages = {30--38},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/3131277.3132173},
  urldate = {2018-07-27},
  abstract = {{\textcopyright} 2017 Copyright held by the owner/author(s). The retrieval of additional information from public (e.g., map data) or private (e.g., e-mail) information sources using personal smart devices is a common habit in today's co-located conversations. This behavior of users imposes challenges in two main areas: 1) cognitive focus switching and 2) information sharing.},
  isbn = {978-1-4503-5486-8},
  talk = {https://www.youtube.com/watch?v=sUe5C8RfON0},
  keywords = {-  Human-centered computing  -{$>$}  Mixed / augmented,Collaborative interaction,Design,HMD,Human Factors,notion,softwarecampus},
  file = {C:\Users\ra46sin\OneDrive\Uni\paper\zotero\muller2017cloudbits.pdf}
}

@inproceedings{muller2018camea,
  title = {{{CaMea}}: {{Camera-Supported Workpiece Measurement}} for {{CNC Milling Machines}}},
  booktitle = {Proceedings of the 11th {{PErvasive Technologies Related}} to {{Assistive Environments Conference}} on - {{PETRA}} '18},
  author = {M{\"u}ller, Florian and Barnikol, Maximilian and Funk, Markus and Schmitz, Martin and M{\"u}hlh{\"a}user, Max},
  year = {2018},
  pages = {345--350},
  publisher = {{ACM}},
  address = {{New York, New York, USA}},
  doi = {10.1145/3197768.3201569},
  isbn = {978-1-4503-6390-7},
  video = {https://www.youtube.com/watch?v=jzBM\_lchKtg},
  keywords = {CNC machine,Digital fabrication,Graphical user interface,Human computer interaction,notion,Worpiece measurement},
  file = {C:\Users\ra46sin\OneDrive\Uni\paper\zotero\muller2018camea.pdf}
}

@inproceedings{muller2018personalized,
  title = {Personalized {{User-Carried Single Button Interfaces}} as {{Shortcuts}} for {{Interacting}} with {{Smart Devices}}},
  booktitle = {Extended {{Abstracts}} of the 2018 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}} - {{CHI}} '18},
  author = {M{\"u}ller, Florian and Schmitz, Martin and Funk, Markus and G{\"u}nther, Sebastian and Dezfuli, Niloofar and M{\"u}hlh{\"a}user, Max},
  year = {2018},
  pages = {1--6},
  publisher = {{ACM}},
  address = {{New York, New York, USA}},
  doi = {10.1145/3170427.3188661},
  isbn = {978-1-4503-5621-3},
  video = {https://www.youtube.com/watch?v=3wNXOfMofXw},
  keywords = {human factors,Human Factors,interaction,Interaction,notion,smart devices,Smart Devices},
  file = {C:\Users\ra46sin\OneDrive\Uni\paper\zotero\muller2018personalized.pdf}
}

@inproceedings{muller2018smartobjects,
  title = {{{SmartObjects}}: {{Sixth Workshop}} on {{Interacting}} with {{Smart Objects}}},
  booktitle = {Extended {{Abstracts}} of the 2018 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}} - {{CHI}} '18},
  author = {M{\"u}ller, Florian and {Schnelle-Walka}, Dirk and {Grosse-Puppendahl}, Tobias and G{\"u}nther, Sebastian and Funk, Markus and Luyten, Kris and Brdiczka, Oliver and Dezfuli, Niloofar and M{\"u}hlh{\"a}user, Max},
  year = {2018},
  pages = {1--6},
  publisher = {{ACM}},
  address = {{New York, New York, USA}},
  doi = {10.1145/3170427.3170606},
  abstract = {Smart objects are everyday objects that have computing capabilities and give rise to new ways of interaction with our environment. The increasing number of smart objects in our life shapes how we interact beyond the desktop. In this workshop we explore various aspects of the design, development and deployment of smart objects including how one can interact with smart objects.},
  isbn = {978-1-4503-5621-3},
  keywords = {Context-awareness,Embodied interaction,Enabling techologies,HCI,Multimodal and adapter interaction,notion,Novel interaction,Smart objects,softwarecampus,Tangible interaction},
  file = {C:\Users\ra46sin\OneDrive\Uni\paper\zotero\muller2018smartobjects.pdf}
}

@inproceedings{Muller2019,
  title = {Mind the {{Tap}}: {{Assessing Foot-Taps}} for {{Interacting}} with {{Head-Mounted Displays}}},
  booktitle = {Proceedings of the 2019 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}} - {{CHI}} '19},
  author = {M{\"u}ller, Florian and McManus, Joshua and G{\"u}nther, Sebastian and Schmitz, Martin and M{\"u}hlh{\"a}user, Max and Funk, Markus},
  year = {2019},
  pages = {1--13},
  publisher = {{ACM Press}},
  address = {{New York, New York, USA}},
  doi = {10.1145/3290605.3300707},
  award = {honorablemention},
  isbn = {978-1-4503-5970-2},
  video = {https://www.youtube.com/watch?v=D5hTVIEb7iA},
  keywords = {all or part of,foot interaction,Foot interaction,hmd,HMD,human factors,Human factors,is granted without fee,notion,or hard copies of,permission to make digital,personal or classroom use,provided that copies,this work for,user study,User study},
  file = {C:\Users\ra46sin\OneDrive\Uni\paper\zotero\Muller2019.pdf},
  selected = {true}
}

@book{Muller2019soproceedings,
  title = {Proceedings of the 7th {{Workshop}} on {{Interacting}} with {{Smart Objects}}},
  editor = {M{\"u}ller, Florian and {Schnelle-Walka}, Dirk and G{\"u}nther, Sebastian and Marky, Karola and Funk, Markus and M{\"u}hlh{\"a}user, Max},
  year = {2019},
  publisher = {{TUprints}},
  urldate = {2019-10-30},
  file = {C:\Users\ra46sin\OneDrive\Uni\paper\zotero\Muller2019soproceedings.pdf}
}

@article{Muller2020a,
  title = {Around-Body {{Interaction}}: {{Interacting While}} on the {{Go}}},
  author = {M{\"u}ller, Florian and G{\"u}nther, Sebastian and M{\"u}hlhauser, Max},
  year = {2020},
  month = apr,
  journal = {IEEE Pervasive Computing},
  volume = {19},
  number = {2},
  pages = {74--78},
  publisher = {{IEEE}},
  issn = {1536-1268},
  doi = {10.1109/MPRV.2020.2977850},
  file = {C:\Users\ra46sin\OneDrive\Uni\paper\zotero\Muller2020a.pdf}
}

@incollection{Muller2020c,
  title = {Around-{{Body Interaction}}: {{{\"U}ber}} Die {{Nutzung}} Der {{Bewegungen}} von {{Gliedma{\ss}en}} Zur {{Interaktion}} in Einer Digital Erweiterten Physischen {{Welt}}},
  booktitle = {Ausgezeichnete {{Informatikdissertationen}} 2019},
  author = {M{\"u}ller, Florian},
  year = {2020},
  month = may,
  pages = {179--188},
  publisher = {{GI}},
  address = {{Schoss Dagstuhl, Deutschland}},
  isbn = {978-3-88579-775-3},
  file = {C:\Users\ra46sin\OneDrive\Uni\paper\zotero\Muller2020c.pdf}
}

@inproceedings{Muller2020d,
  title = {Walk {{The Line}}: {{Leveraging Lateral Shifts}} of the {{Walking Path}} as an {{Input Modality}} for {{Head-Mounted Displays}}},
  booktitle = {Proceedings of the 2020 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {M{\"u}ller, Florian and Schmitz, Martin and Schmitt, Daniel and G{\"u}nther, Sebastian and Funk, Markus and M{\"u}hlh{\"a}user, Max},
  year = {2020},
  month = apr,
  pages = {1--15},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/3313831.3376852},
  abstract = {Recent technological advances have made head-mounted displays (HMDs) smaller and untethered, fostering the vision of ubiquitous interaction in a digitally augmented physical world. Consequently, a major part of the interaction with such devices will happen on the go, calling for interaction techniques that allow users to interact while walking. In this paper, we explore lateral shifts of the walking path as a hands-free input modality. The available input options are visualized as lanes on the ground parallel to the user's walking path. Users can select options by shifting the walking path sideways to the respective lane. We contribute the results of a controlled experiment with 18 participants, confirming the viability of our approach for fast, accurate, and joyful interactions. Further, based on the findings of the controlled experiment, we present three example applications.},
  isbn = {978-1-4503-6708-0},
  selected = {true},
  talk = {https://www.youtube.com/watch?v=uQ5w3Wvrb3w},
  video = {https://www.youtube.com/watch?v=ylAlzFqWx7g},
  file = {C:\Users\ra46sin\OneDrive\Uni\paper\zotero\Muller2020d.pdf}
}

@inproceedings{mullerTicTacToesAssessingToe2023a,
  title = {{{TicTacToes}}: {{Assessing Toe Movements}} as an {{Input Modality}}},
  shorttitle = {{{TicTacToes}}},
  booktitle = {Proceedings of the 2023 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {M{\"u}ller, Florian and Schmitt, Daniel and Matviienko, Andrii and Sch{\"o}n, Dominik and G{\"u}nther, Sebastian and Kosch, Thomas and Schmitz, Martin},
  year = {2023},
  month = apr,
  series = {{{CHI}} '23},
  eprint = {2303.15811},
  pages = {1--17},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/3544548.3580954},
  urldate = {2024-01-09},
  abstract = {From carrying grocery bags to holding onto handles on the bus, there are a variety of situations where one or both hands are busy, hindering the vision of ubiquitous interaction with technology. Voice commands, as a popular hands-free alternative, struggle with ambient noise and privacy issues. As an alternative approach, research explored movements of various body parts (e.g., head, arms) as input modalities, with foot-based techniques proving particularly suitable for hands-free interaction. Whereas previous research only considered the movement of the foot as a whole, in this work, we argue that our toes offer further degrees of freedom that can be leveraged for interaction. To explore the viability of toe-based interaction, we contribute the results of a controlled experiment with 18 participants assessing the impact of five factors on the accuracy, efficiency and user experience of such interfaces. Based on the findings, we provide design recommendations for future toe-based interfaces.},
  archiveprefix = {arxiv},
  isbn = {978-1-4503-9421-5},
  selected = {true},
  talk = {https://www.youtube.com/watch?v=2enVDAGiE8E},
  video = {https://www.youtube.com/watch?v=FzA-6F5SJ44},
  keywords = {Body-Centric Interaction,Foot,Foot-Based Interaction,Input,Toes},
  file = {C:\Users\ra46sin\OneDrive\Uni\paper\zotero\mullerTicTacToesAssessingToe2023a.pdf}
}

@inproceedings{mullerUndoPortExploringInfluence2023a,
  title = {{{UndoPort}}: {{Exploring}} the {{Influence}} of {{Undo-Actions}} for {{Locomotion}} in {{Virtual Reality}} on the {{Efficiency}}, {{Spatial Understanding}} and {{User Experience}}},
  shorttitle = {{{UndoPort}}},
  booktitle = {Proceedings of the 2023 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {M{\"u}ller, Florian and Ye, Arantxa and Sch{\"o}n, Dominik and Rasch, Julian},
  year = {2023},
  month = apr,
  series = {{{CHI}} '23},
  eprint = {2303.15800},
  pages = {1--15},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/3544548.3581557},
  urldate = {2024-01-09},
  abstract = {When we get lost in Virtual Reality (VR) or want to return to a previous location, we use the same methods of locomotion for the way back as for the way forward. This is time-consuming and requires additional physical orientation changes, increasing the risk of getting tangled in the headsets' cables. In this paper, we propose the use of undo actions to revert locomotion steps in VR. We explore eight different variations of undo actions as extensions of point\&teleport, based on the possibility to undo position and orientation changes together with two different visualizations of the undo step (discrete and continuous). We contribute the results of a controlled experiment with 24 participants investigating the efficiency and orientation of the undo techniques in a radial maze task. We found that the combination of position and orientation undo together with a discrete visualization resulted in the highest efficiency without increasing orientation errors.},
  archiveprefix = {arxiv},
  isbn = {978-1-4503-9421-5},
  talk = {https://www.youtube.com/watch?v=INzk1\_a2Z3k},
  video = {https://www.youtube.com/watch?v=BwRc4f8VSEk},
  keywords = {Locomotion,Teleport,Undo,Virtual Reality},
  file = {C:\Users\ra46sin\OneDrive\Uni\paper\zotero\mullerUndoPortExploringInfluence2023a.pdf},
  selected = {true}
}

@inproceedings{murauer2018analysis,
  title = {An {{Analysis}} of {{Language Impact}} on {{Augmented Reality Order Picking Training}}},
  booktitle = {Proceedings of the 11th {{PErvasive Technologies Related}} to {{Assistive Environments Conference}}},
  author = {Murauer, Nela and M{\"u}ller, Florian and G{\"u}nther, Sebastian and Sch{\"o}n, Dominik and Pflanz, Nerina and Funk, Markus},
  year = {2018},
  month = jun,
  pages = {351--357},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/3197768.3201570},
  isbn = {978-1-4503-6390-7},
  file = {C:\Users\ra46sin\OneDrive\Uni\paper\zotero\murauer2018analysis.pdf}
}

@inproceedings{raschGoingGoingGone2023a,
  title = {Going, {{Going}}, {{Gone}}: {{Exploring Intention Communication}} for {{Multi-User Locomotion}} in {{Virtual Reality}}},
  shorttitle = {Going, {{Going}}, {{Gone}}},
  booktitle = {Proceedings of the 2023 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Rasch, Julian and Rusakov, Vladislav Dmitrievic and Schmitz, Martin and M{\"u}ller, Florian},
  year = {2023},
  month = apr,
  series = {{{CHI}} '23},
  pages = {1--13},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/3544548.3581259},
  urldate = {2024-01-09},
  abstract = {Exploring virtual worlds together with others adds a social component to the Virtual Reality (VR) experience that increases connectedness. In the physical world, joint locomotion comes naturally through implicit intention communication and subsequent adjustments of the movement patterns. In VR, however, discrete locomotion techniques such as point\&teleport come without prior intention communication, hampering the collective experience. Related work proposes fixed groups, with a single person controlling the group movement, resulting in the loss of individual movement capabilities. To close the gap and mediate between these two extremes, we introduce three intention communication methods and explore them with two baseline methods. We contribute the results of a controlled experiment (n=20) investigating these methods from the perspective of a leader and a follower in a dyadic locomotion task. Our results suggest shared visualizations support the understanding of movement intentions, increasing the group feeling while maintaining individual freedom of movement.},
  award = {bestpaper},
  isbn = {978-1-4503-9421-5},
  talk = {https://www.youtube.com/watch?v=k9ZvRsWYHDU},
  video = {https://www.youtube.com/watch?v=IC9XBi4Tr34},
  keywords = {Connectedness,Locomotion,Multi-User,SocialVR,Teleportation,Virtual Reality},
  file = {C:\Users\ra46sin\OneDrive\Uni\paper\zotero\raschGoingGoingGone2023a.pdf}
}

@inproceedings{raschHandsOn3DPrinted2023a,
  title = {Hands-{{On 3D Printed Electronics}}},
  booktitle = {Proceedings of the {{Seventeenth International Conference}} on {{Tangible}}, {{Embedded}}, and {{Embodied Interaction}}},
  author = {Rasch, Julian and M{\"u}ller, Florian and Kosch, Thomas and Schmitz, Martin and Feger, Sebastian S.},
  year = {2023},
  month = feb,
  series = {{{TEI}} '23},
  pages = {1--2},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/3569009.3571846},
  urldate = {2024-01-09},
  abstract = {The parallel improvements in multi-material 3D printers and the quality of conductive filament open new possibilities for the fabrication of tangible and functional objects. In this studio, we discuss best practices for 3D printed electronics, talk about encountered problems, and derive design recommendations. We will guide the participants through a fabrication process by practically designing and printing objects. Consequently, we contemplate individual functional fabricated components, including small printed circuits and multi-material prints. We aim to spark a discussion about individually experienced challenges participants encountered during their design and fabrication process. This discussion includes problem-solving strategies, whose insights benefit other participants. Finally, we show the potential of printed electronics and discuss encouraging new opportunities in this field.},
  isbn = {978-1-4503-9977-7},
  keywords = {3D Printing,Additive Manufacturing,Circuit Engineering,Printed Electronics},
  file = {C:\Users\ra46sin\OneDrive\Uni\paper\zotero\raschHandsOn3DPrinted2023a.pdf}
}

@inproceedings{riemann2016freetop,
  title = {{{FreeTop}}: {{Finding}} Free Spots for Projective Augmentation},
  booktitle = {Proceedings of the 2016 {{CHI Conference Extended Abstracts}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Riemann, Jan and Khalilbeigi, Mohammadreza and Schmitz, Martin and Doeweling, Sebastian and M{\"u}ller, Florian and M{\"u}hlh{\"a}user, Max},
  year = {2016},
  month = may,
  volume = {07-12-May-},
  pages = {1598--1606},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/2851581.2892321},
  abstract = {Augmenting the physical world using projection technologies or head-worn displays becomes increasingly popular in research and commercial applications. However, a common problem is interference between the physical surface's texture and the projection. In this paper, we present FreeTop, a combined approach to finding areas suitable for projection, which considers multiple aspects influencing projection quality, like visual texture and physical surface structure. FreeTop can be used in stationary and mobile settings for locating free areas in arbitrary physical settings suitable for projective augmentation and touch interaction.},
  isbn = {978-1-4503-4082-3},
  keywords = {Hybrid physical-digital interaction,Interactive displays,Multitouch,Peripheral displays,Projection},
  file = {C:\Users\ra46sin\OneDrive\Uni\paper\zotero\riemann2016freetop.pdf}
}

@inproceedings{riemann2017evaluation,
  title = {An {{Evaluation}} of {{Hybrid Stacking}} on {{Interactive Tabletops}}},
  booktitle = {Proceedings of the 2017 {{ACM Workshop}} on {{Interacting}} with {{Smart Objects}}},
  author = {Riemann, Jan and M{\"u}ller, Florian and G{\"u}nther, Sebastian and M{\"u}hlh{\"a}user, Max},
  year = {2017},
  month = mar,
  pages = {13--20},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/3038450.3038451},
  abstract = {Stacking is a common practice of organizing documents in the physical world. With the recent advent of interactive tabletops, physical documents can now coexist with digital documents on the same surface. As a result, systems were developed and studied which allow piling of both types of documents with the physical documents being placed on top of the digital ones. In this paper, we study the concept of true hybrid stacking, allowing users to stack both types of documents in an arbitrary order using a hybrid tabletop system called StackTop. We discuss the results and derive implications for future hybrid tabletop systems with stacking support.},
  isbn = {978-1-4503-4902-4},
  keywords = {Hybrid physical-digital interaction,Interactive tabletop displays,Multitouch,Peripheral displays,Piling,softwarecampus,Stacking},
  file = {C:\Users\ra46sin\OneDrive\Uni\paper\zotero\riemann2017evaluation.pdf}
}

@inproceedings{schmitz2016liquido,
  title = {Liquido: {{Embedding Liquids}} into {{3D Printed Objects}} to {{Sense Tilting}} and {{Motion}}},
  booktitle = {Proceedings of the 2016 {{CHI Conference Extended Abstracts}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Schmitz, Martin and Leister, Andreas and Dezfuli, Niloofar and Riemann, Jan and M{\"u}ller, Florian and M{\"u}hlh{\"a}user, Max},
  year = {2016},
  month = may,
  pages = {2688--2696},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/2851581.2892275},
  abstract = {Tilting and motion are widely used as interaction modalities in smart objects such as wearables and smart phones (e.g., to detect posture or shaking). They are often sensed with accelerometers. In this paper, we propose to embed liquids into 3D printed objects while printing to sense various tilting and motion interactions via capacitive sensing. This method reduces the assembly effort after printing and is a low-cost and easy-to-apply way of extending the input capabilities of 3D printed objects. We contribute two liquid sensing patterns and a practical printing process using a standard dual-extrusion 3D printer and commercially available materials. We validate the method by a series of evaluations and provide a set of interactive example applications. {\textcopyright} 2016 Authors.},
  isbn = {978-1-4503-4082-3},
  keywords = {3D printing,capacitive sensing,digital fabrication,input sensing,interaction devices,motion,printed electronics,rapid prototyping,tilting},
  file = {C:\Users\ra46sin\OneDrive\Uni\paper\zotero\schmitz2016liquido.pdf}
}

@inproceedings{schmitz2022squeezy,
  title = {Squeezy-{{Feely}}: {{Investigating Lateral Thumb-Index Pinching}} as an {{Input Modality}}},
  booktitle = {{{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Schmitz, Martin and G{\"u}nther, Sebastian and Sch{\"o}n, Dominik and M{\"u}ller, Florian},
  year = {2022},
  month = apr,
  pages = {1--15},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/3491102.3501981},
  abstract = {From zooming on smartphones and mid-air gestures to deformable user interfaces, thumb-index pinching grips are used in many interaction techniques. However, there is still a lack of systematic understanding of how the accuracy and efficiency of such grips are affected by various factors such as counterforce, grip span, and grip direction. Therefore, in this paper, we contribute an evaluation (N = 18) of thumb-index pinching performance in a visual targeting task using scales up to 75 items. As part of our findings, we conclude that the pinching interaction between the thumb and index finger is a promising modality also for one-dimensional input on higher scales. Furthermore, we discuss and outline implications for future user interfaces that benefit from pinching as an additional and complementary interaction modality.},
  isbn = {978-1-4503-9157-3},
  talk = {https://www.youtube.com/watch?v=MD3WY1FIUaA},
  video = {https://www.youtube.com/watch?v=hoHcyrAqTeM},
  keywords = {Deformation,Input,Mixed Reality,Pinching,Thumb-to-finger,User Studies},
  file = {C:\Users\ra46sin\OneDrive\Uni\paper\zotero\schmitz2022squeezy.pdf}
}

@inproceedings{schmitzItsyBitsFabricationRecognition2021,
  title = {Itsy-{{Bits}}: {{Fabrication}} and {{Recognition}} of {{3D-Printed Tangibles}} with {{Small Footprints}} on {{Capacitive Touchscreens}}},
  shorttitle = {Itsy-{{Bits}}},
  booktitle = {Proceedings of the 2021 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Schmitz, Martin and M{\"u}ller, Florian and M{\"u}hlh{\"a}user, Max and Riemann, Jan and Le, Huy Viet Viet},
  year = {2021},
  month = may,
  series = {{{CHI}} '21},
  pages = {1--12},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/3411764.3445502},
  urldate = {2024-01-09},
  abstract = {Tangibles on capacitive touchscreens are a promising approach to overcome the limited expressiveness of touch input. While research has suggested many approaches to detect tangibles, the corresponding tangibles are either costly or have a considerable minimal size. This makes them bulky and unattractive for many applications. At the same time, they obscure valuable display space for interaction. To address these shortcomings, we contribute Itsy-Bits: a fabrication pipeline for 3D printing and recognition of tangibles on capacitive touchscreens with a footprint as small as a fingertip. Each Itsy-Bit consists of an enclosing 3D object and a unique conductive 2D shape on its bottom. Using only raw data of commodity capacitive touchscreens, Itsy-Bits reliably identifies and locates a variety of shapes in different sizes and estimates their orientation. Through example applications and a technical evaluation, we demonstrate the feasibility and applicability of Itsy-Bits for tangibles with small footprints.},
  isbn = {978-1-4503-8096-6},
  talk = {https://www.youtube.com/watch?v=fI3zZz4fnMY},
  video = {https://www.youtube.com/watch?v=WrdRQlt2fsA},
  keywords = {3d printing,3D Printing,Machine learning,Machine Learning,Tangibles,Touchscreen},
  file = {C\:\\Users\\ra46sin\\OneDrive\\Uni\\paper\\zotero\\schmitzItsyBitsFabricationRecognition2021.pdf;C\:\\Users\\ra46sin\\OneDrive\\Uni\\paper\\zotero\\schmitzItsyBitsFabricationRecognition22.pdf}
}

@inproceedings{SchmitzMartinStitzFlorianMuller2019,
  title = {./Trilaterate: {{A Fabrication Pipeline}} to {{Design}} and {{3D Print Hover-}}, {{Touch-}}, and {{Force-Sensitive Objects}}},
  booktitle = {Proceedings of the 2019 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}} - {{CHI}} '19},
  author = {Schmitz, Martin and Stitz, Martin and M{\"u}ller, Florian and Funk, Markus and M{\"u}hlh{\"a}user, Max},
  year = {2019},
  pages = {1--13},
  publisher = {{ACM Press}},
  address = {{New York, New York, USA}},
  doi = {10.1145/3290605.3300684},
  urldate = {2019-03-28},
  abstract = {Hover, touch, and force are promising input modalities that get increasingly integrated into screens and everyday objects. However, these interactions are often limited to fat surfaces and the integration of suitable sensors is time-consuming and costly. To alleviate these limitations, we contribute Tri-laterate: A fabrication pipeline to 3D print custom objects that detect the 3D position of a fnger hovering, touching, or forcing them by combining multiple capacitance measurements via capacitive trilateration. Trilaterate places and routes actively-shielded sensors inside the object and operates on consumer-level 3D printers. We present technical evaluations and example applications that validate and demonstrate the wide applicability of Trilaterate. CCS CONCEPTS {\textbullet} Human-centered computing {\textrightarrow} Interaction devices; {\textbullet} Hardware {\textrightarrow} Tactile and hand-based interfaces;},
  isbn = {978-1-4503-5970-2},
  video = {https://www.youtube.com/watch?v=4BT6Nw2kWPs},
  keywords = {3D printing,capacitive sensing,Capacitive sensing,force,Force,hover,Hover,touch,Touch},
  file = {C:\Users\ra46sin\OneDrive\Uni\paper\zotero\SchmitzMartinStitzFlorianMuller2019.pdf}
}

@inproceedings{schmitzOhSnapFabrication2021,
  title = {Oh, {{Snap}}! {{A Fabrication Pipeline}} to {{Magnetically Connect Conventional}} and {{3D-Printed Electronics}}},
  booktitle = {Proceedings of the 2021 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Schmitz, Martin and Riemann, Jan and M{\"u}ller, Florian and Kreis, Steffen and M{\"u}hlh{\"a}user, Max},
  year = {2021},
  month = may,
  series = {{{CHI}} '21},
  pages = {1--11},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/3411764.3445641},
  urldate = {2024-01-09},
  abstract = {3D printing has revolutionized rapid prototyping by speeding up the creation of custom-shaped objects. With the rise of multi-material 3D printers, these custom-shaped objects can now be made interactive in a single pass through passive conductive structures. However, connecting conventional electronics to these conductive structures often still requires time-consuming manual assembly involving many wires, soldering or gluing. To alleviate these shortcomings, we propose : a fabrication pipeline and interfacing concept to magnetically connect a 3D-printed object equipped with passive sensing structures to conventional sensing electronics. To this end, utilizes ferromagnetic and conductive 3D-printed structures, printable in a single pass on standard printers. We further present a proof-of-concept capacitive sensing board that enables easy and robust magnetic assembly to quickly create interactive 3D-printed objects. We evaluate by assessing the robustness and quality of the connection and demonstrate its broad applicability by a series of example applications.},
  code = {https://github.com/Telecooperation/oh-snap},
  isbn = {978-1-4503-8096-6},
  talk = {https://www.youtube.com/watch?v=0AUrrtwaPVQ},
  video = {https://www.youtube.com/watch?v=JX3ZwKnnJVs},
  keywords = {3d printing,3D printing,capacitive sensing,Capacitive sensing,prototyping,Prototyping,proximity,Proximity,touch,Touch},
  file = {C\:\\Users\\ra46sin\\OneDrive\\Uni\\paper\\zotero\\schmitzOhSnapFabrication2021.pdf;C\:\\Users\\ra46sin\\OneDrive\\Uni\\paper\\zotero\\schmitzOhSnapFabrication22.pdf}
}

@inproceedings{schmitzRethinkingSmartObjects2022,
  title = {Rethinking {{Smart Objects}}: {{The International Workshop}} on {{Interacting}} with {{Smart Objects}} in {{Interactive Spaces}}},
  shorttitle = {Rethinking {{Smart Objects}}},
  booktitle = {Companion {{Proceedings}} of the 2022 {{Conference}} on {{Interactive Surfaces}} and {{Spaces}}},
  author = {Schmitz, Martin and G{\"u}nther, Sebastian and Marky, Karola and M{\"u}ller, Florian and Matviienko, Andrii and Voit, Alexandra and Marky, Roberts and M{\"u}hlh{\"a}user, Max and Kosch, Thomas},
  year = {2022},
  month = nov,
  series = {{{ISS}} '22},
  pages = {64--67},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/3532104.3571470},
  urldate = {2024-01-09},
  abstract = {The increasing proliferation of smart objects in everyday life has changed how we interact with computers. Instead of concentrating computational capabilities and interaction into one device, everyday objects have naturally integrated parts of interactive features. Although this has led to many practical applications, the possibilities for explicit or implicit interaction with such objects are still limited in interaction spaces. We still often rely on smartphones as interactive hubs for controlling smart objects, hence not fulfilling the vision of truly smart objects. The workshop Rethinking Smart Objects invites practitioners and researchers from both academia and industry to discuss novel interaction paradigms and the integration and societal implications of using smart objects in interactive space. This workshop will include an action plan with leading questions, aiming to move the research field forward.},
  isbn = {978-1-4503-9356-0},
  website = {https://smart-objects.org/},
  keywords = {Smart Objects,Tangible Interfaces,Ubiquitous Computing},
  file = {C:\Users\ra46sin\OneDrive\Uni\paper\zotero\schmitzRethinkingSmartObjects2022.pdf}
}

@inproceedings{schnelle2016scwt,
  title = {{{SCWT}}: {{A Joint Workshop}} on {{Smart Connected}} and {{Wearable Things}}},
  booktitle = {Companion {{Publication}} of the 21st {{International Conference}} on {{Intelligent User Interfaces}}},
  author = {{Schnelle-Walka}, Dirk and Limonad, Lior and {Grosse-Puppendahl}, Tobias and Lanir, Joel and M{\"u}ller, Florian and Mecella, Massimo and Luyten, Kris and Kuflik, Tsvi and Brdiczka, Oliver and M{\"u}hlh{\"a}user, Max},
  year = {2016},
  month = mar,
  pages = {3--5},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/2876456.2882849},
  abstract = {The increasing number of smart objects in our everyday life shapes how we interact beyond the desktop. In this workshop we discuss how advanced interactions with smart objects in the context of the Internet-of-Thingsshould be designed from various perspectives, such as HCI and AI as well as industry and academia.},
  isbn = {978-1-4503-4140-0},
  file = {C:\Users\ra46sin\OneDrive\Uni\paper\zotero\schnelle2016scwt.pdf}
}

@inproceedings{schnelle2017smartobjects,
  title = {{{SmartObjects}}: {{Fifth Workshop}} on {{Interacting}} with {{Smart Objects}}},
  booktitle = {Proceedings of the 22nd {{International Conference}} on {{Intelligent User Interfaces Companion}}},
  author = {{Schnelle-Walka}, Dirk and M{\"u}ller, Florian and {Grosse-Puppendahl}, Tobias and Luyten, Kris and M{\"u}hlh{\"a}user, Max and Brdiczka, Oliver},
  year = {2017},
  month = mar,
  pages = {21--23},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/3030024.3040249},
  abstract = {Smart objects are everyday objects that have computing capabilities and give rise to new ways of interaction with our environment. The increasing number of smart objects in our life shapes how we interact beyond the desktop. In this workshop we explore various aspects of the design, development and deployment of smart objects including how one can interact with smart objects.},
  isbn = {978-1-4503-4893-5},
  keywords = {dez2012},
  file = {C:\Users\ra46sin\OneDrive\Uni\paper\zotero\schnelle2017smartobjects.pdf}
}

@inproceedings{schonTailorTwistAssessing2023a,
  title = {Tailor {{Twist}}: {{Assessing Rotational Mid-Air Interactions}} for {{Augmented Reality}}},
  shorttitle = {Tailor {{Twist}}},
  booktitle = {Proceedings of the 2023 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Sch{\"o}n, Dominik and Kosch, Thomas and M{\"u}ller, Florian and Schmitz, Martin and G{\"u}nther, Sebastian and Bommhardt, Lukas and M{\"u}hlh{\"a}user, Max},
  year = {2023},
  month = apr,
  series = {{{CHI}} '23},
  pages = {1--14},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/3544548.3581461},
  urldate = {2024-01-09},
  abstract = {Mid-air gestures, widely used in today's Augmented Reality (AR) applications, are prone to the ``gorilla arm'' effect, leading to discomfort with prolonged interactions. While prior work has proposed metrics to quantify this effect and means to improve comfort and ergonomics, these works usually only consider simplistic, one-dimensional AR interactions, like reaching for a point or pushing a button. However, interacting with AR environments also involves far more complex tasks, such as rotational knobs, potentially impacting ergonomics. This paper advances the understanding of the ergonomics of rotational mid-air interactions in AR. For this, we contribute the results of a controlled experiment exposing the participants to a rotational task in the interaction space defined by their arms' reach. Based on the results, we discuss how novel future mid-air gesture modalities benefit from our findings concerning ergonomic-aware rotational interaction.},
  isbn = {978-1-4503-9421-5},
  talk = {https://www.youtube.com/watch?v=auieeDWFq},
  video = {https://www.youtube.com/watch?v=25Nj5-MSnhA},
  keywords = {Augmented Reality,Mid-Air Gesture,Rotational Interaction},
  file = {C:\Users\ra46sin\OneDrive\Uni\paper\zotero\schonTailorTwistAssessing2023a.pdf}
}

@inproceedings{schonTrackItPipeFabricationPipeline2022,
  title = {{{TrackItPipe}}: {{A Fabrication Pipeline To Incorporate Location}} and {{Rotation Tracking Into 3D Printed Objects}}},
  shorttitle = {{{TrackItPipe}}},
  booktitle = {Adjunct {{Proceedings}} of the 35th {{Annual ACM Symposium}} on {{User Interface Software}} and {{Technology}}},
  author = {Sch{\"o}n, Dominik and Kosch, Thomas and Schmitz, Martin and M{\"u}ller, Florian and G{\"u}nther, Sebastian and Kreutz, Johannes and M{\"u}hlh{\"a}user, Max},
  year = {2022},
  month = oct,
  series = {{{UIST}} '22 {{Adjunct}}},
  pages = {1--5},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/3526114.3558719},
  urldate = {2024-01-09},
  abstract = {The increasing convergence of the digital and physical world creates a growing urgency to integrate 3D printed physical tangibles with virtual environments. A precise position and rotation tracking are essential to integrate such physical objects with a virtual environment. However, available 3D models commonly do not provide tracking support on their composition, which requires modifications by CAD experts. This poses a challenge for users with no prior CAD experience. This work presents TrackItPipe, a fabrication pipeline supporting users by semi-automatically adding tracking capabilities for 3D printable tangibles tailored to environmental requirements. TrackItPipe integrates modifications to the 3D model, produces the respective tangibles for 3D printing, and provides integration scripts for Mixed Reality. Using TrackItPipe, users can rapidly equip objects with tracking capabilities.},
  code = {https://github.com/Dominik-Schoen/TrackItPipe},
  isbn = {978-1-4503-9321-8},
  keywords = {3D Printing,Fabrication,Mixed Reality,Tracking},
  file = {C:\Users\ra46sin\OneDrive\Uni\paper\zotero\schonTrackItPipeFabricationPipeline2022.pdf}
}

@book{SmartObjects2018proceedings,
  title = {Proceedings of the 6th {{Workshop}} on {{Interacting}} with {{Smart Objects}} ({{SmartObjects}})},
  editor = {M{\"u}ller, Florian and {Schnelle-Walka}, Dirk and G{\"u}nther, Sebastian and Funk, Markus},
  year = {2018},
  journal = {6th Workshop on Interacting with Smart Objects (SmartObjects)},
  number = {2082},
  publisher = {{CEUR-WS}},
  address = {{Aachen}},
  issn = {1613-0073},
  website = {https://ceur-ws.org/Vol-2082/},
  file = {C:\Users\ra46sin\OneDrive\Uni\paper\zotero\SmartObjects2018proceedings.pdf}
}

@inproceedings{VonWillich2019a,
  title = {{{VRChairRacer}}: {{Using}} an {{Office Chair Backrest}} as a {{Locomotion Technique}} for {{VR Racing Games}}},
  booktitle = {Extended {{Abstracts}} of the 2019 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}} - {{CHI}} '19},
  author = {{von Willich}, Julius and Sch{\"o}n, Dominik and G{\"u}nther, Sebastian and M{\"u}ller, Florian and M{\"u}hlh{\"a}user, Max and Funk, Markus},
  year = {2019},
  publisher = {{ACM}},
  address = {{Glasgow, Scotland, UK}},
  doi = {10.1145/3290607.3313254},
  urldate = {2019-03-28},
  abstract = {Locomotion in Virtual Reality (VR) is an important topic as there is a mismatch between the size of a Virtual Environment and the physically available tracking space. Although many locomotion techniques have been proposed, research on VR locomotion has not concluded yet. In this demonstration, we contribute to the area of VR locomotion by introducing VRChairRacer. VRChairRacer introduces a novel mapping the velocity of a racing cart on the backrest of an office chair. Further, it maps a users' rotation onto the steering of a virtual racing cart. VRChairRacer demonstrates this locomotion technique to the community through an immersive multiplayer racing demo.},
  video = {https://www.youtube.com/watch?v=v906aGntoKY},
  keywords = {Locomotion,Racing Game,Virtual Reality},
  file = {C:\Users\ra46sin\OneDrive\Uni\paper\zotero\VonWillich2019a.pdf}
}

@inproceedings{vonwillichDensingQueenExplorationMethods2023a,
  title = {{{DensingQueen}}: {{Exploration Methods}} for {{Spatial Dense Dynamic Data}}},
  shorttitle = {{{DensingQueen}}},
  booktitle = {Proceedings of the 2023 {{ACM Symposium}} on {{Spatial User Interaction}}},
  author = {{von Willich}, Julius and G{\"u}nther, Sebastian and Matviienko, Andrii and Schmitz, Martin and M{\"u}ller, Florian and M{\"u}hlh{\"a}user, Max},
  year = {2023},
  month = oct,
  series = {{{SUI}} '23},
  pages = {1--12},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/3607822.3614535},
  urldate = {2024-01-09},
  abstract = {Research has proposed various interaction techniques to manage the occlusion of 3D data in Virtual Reality (VR), e.g., via gradual refinement. However, tracking dynamically moving data in a dense 3D environment poses the challenge of ever-changing occlusion, especially if motion carries relevant information, which is lost in still images. In this paper, we evaluated two interaction modalities for Spatial Dense Dynamic Data (SDDD), adapted from existing interaction methods for static and spatial data. We evaluated these modalities for exploring SDDD in VR, in an experiment with 18 participants. Furthermore, we investigated the influence of our interaction modalities on different levels of data density on the users' performance in a no-knowledge task and a prior-knowledge task. Our results indicated significantly degraded performance for higher levels of density. Further, we found that our flashlight-inspired modality successfully improved tracking in SDDD, while a cutting plane-inspired approach was more suitable for highlighting static volumes of interest, particularly in such high-density environments.},
  code = {https://github.com/LOEWE-emergenCITY/DensingQueen},
  isbn = {9798400702815},
  keywords = {Data exploration,Data Interaction,Dense Data,Dynamic Data,Spatial Data,Virtual Reality},
  file = {C:\Users\ra46sin\OneDrive\Uni\paper\zotero\vonwillichDensingQueenExplorationMethods2023a.pdf}
}

@inproceedings{vonwillichYouInvadedMy2019,
  title = {You {{Invaded}} My {{Tracking Space}}! {{Using Augmented Virtuality}} for {{Spotting Passersby}} in {{Room-Scale Virtual Reality}}},
  booktitle = {Proceedings of the 2019 on {{Designing Interactive Systems Conference}} - {{DIS}} '19},
  author = {{von Willich}, Julius and Funk, Markus and M{\"u}ller, Florian and Marky, Karola and Riemann, Jan and M{\"u}hlh{\"a}user, Max},
  year = {2019},
  pages = {487--496},
  publisher = {{ACM Press}},
  address = {{New York, New York, USA}},
  doi = {10.1145/3322276.3322334},
  urldate = {2019-07-11},
  isbn = {978-1-4503-5850-7},
  video = {https://www.youtube.com/watch?v=SGOFeRX0tmk},
  keywords = {ar,passersby visualization,vr},
  file = {C:\Users\ra46sin\OneDrive\Uni\paper\zotero\VonWillich2019a2.pdf}
}

@inproceedings{weissUsingPseudoStiffnessEnrich2023a,
  title = {Using {{Pseudo-Stiffness}} to {{Enrich}} the {{Haptic Experience}} in {{Virtual Reality}}},
  booktitle = {Proceedings of the 2023 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Weiss, Yannick and Villa, Steeven and Schmidt, Albrecht and Mayer, Sven and M{\"u}ller, Florian},
  year = {2023},
  month = apr,
  series = {{{CHI}} '23},
  pages = {1--15},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/3544548.3581223},
  urldate = {2024-01-09},
  abstract = {Providing users with a haptic sensation of the hardness and softness of objects in virtual reality is an open challenge. While physical props and haptic devices help, their haptic properties do not allow for dynamic adjustments. To overcome this limitation, we present a novel technique for changing the perceived stiffness of objects based on a visuo-haptic illusion. We achieved this by manipulating the hands' Control-to-Display (C/D) ratio in virtual reality while pressing down on an object with fixed stiffness. In the first study (N=12), we determine the detection thresholds of the illusion. Our results show that we can exploit a C/D ratio from 0.7 to 3.5 without user detection. In the second study (N=12), we analyze the illusion's impact on the perceived stiffness. Our results show that participants perceive the objects to be up to 28.1\% softer and 8.9\% stiffer, allowing for various haptic applications in virtual reality.},
  isbn = {978-1-4503-9421-5},
  talk = {https://www.youtube.com/watch?v=Oex8NlPvcVU},
  keywords = {haptic illusions,pseudo-haptics,virtual reality},
  file = {C:\Users\ra46sin\OneDrive\Uni\paper\zotero\weissUsingPseudoStiffnessEnrich2023a.pdf}
}

@inproceedings{Willich2020,
  title = {Podoportation: {{Foot-Based Locomotion}} in {{Virtual Reality}}},
  booktitle = {Proceedings of the 2020 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}} - {{CHI}} '20},
  author = {Willich, Julius Von and Schmitz, Martin and M{\"u}ller, Florian and Schmitt, Daniel and M{\"u}hlh{\"a}user, Max},
  year = {2020},
  volume = {20},
  publisher = {{ACM}},
  doi = {10.1145/3313831.3376626},
  urldate = {2020-05-11},
  abstract = {Virtual Reality (VR) allows for infinitely large environments. However, the physical traversable space is always limited by real-world boundaries. This discrepancy between physical and virtual dimensions renders traditional locomotion methods used in real world unfeasible. To alleviate these limitations, research proposed various artificial locomotion concepts such as teleportation, treadmills, and redirected walking. However, these concepts occupy the user's hands, require complex hardware or large physical spaces. In this paper, we contribute nine VR locomotion concepts for foot-based locomotion, relying on the 3D position of the user's feet and the pressure applied to the sole as input modalities. We evaluate our concepts and compare them to state-of-the-art point \& teleport technique in a controlled experiment with 20 participants. The results confirm the viability of our approaches for foot-based and engaging locomotion. Further, based on the findings, we contribute a wireless hardware prototype implementation.},
  isbn = {978-1-4503-6708-0},
  talk = {https://www.youtube.com/watch?v=6c8JujTvVkY},
  video = {https://www.youtube.com/watch?v=xvqZTbJdXYE},
  keywords = {foot-based input,Foot-based input CCS Concepts {\textbullet}Human-centered comp,Interaction devices,locomotion,Locomotion,User stud-ies,virtual reality,Virtual Reality},
  file = {C:\Users\ra46sin\OneDrive\Uni\paper\zotero\Willich2020.pdf}
}
