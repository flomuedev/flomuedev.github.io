<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Florian Müller</title> <meta name="author" content="Florian Müller"> <meta name="description" content="Florian Müller is a a postdoctoral Human-Computer Interaction researcher at the Human-Centered Ubiquitous Media Lab at the LMU Munich. "> <meta name="keywords" content="hci, research, urban interaction"> <meta property="og:site_name" content="Florian Müller"> <meta property="og:type" content="website"> <meta property="og:title" content="Florian Müller | about"> <meta property="og:url" content="https://www.flomue.com/"> <meta property="og:description" content="Florian Müller is a a postdoctoral Human-Computer Interaction researcher at the Human-Centered Ubiquitous Media Lab at the LMU Munich. "> <meta property="og:image" content="https://www.flomue.com/assets/img/florian_profile_square.jpg"> <meta property="og:locale" content="en"> <meta name="twitter:card" content="summary"> <meta name="twitter:title" content="about"> <meta name="twitter:description" content="Florian Müller is a a postdoctoral Human-Computer Interaction researcher at the Human-Centered Ubiquitous Media Lab at the LMU Munich. "> <meta name="twitter:image" content="https://www.flomue.com/assets/img/florian_profile_square.jpg"> <script type="application/ld+json">
    {
        "author":
        {
            "@type": "Person",
            "name": "Florian  Müller",
            "url": "https://www.flomue.com"
        },
        "url": "https://www.flomue.com/",
        "@type": "WebSite",
        "description": "Florian Müller is a a postdoctoral Human-Computer Interaction researcher at the Human-Centered Ubiquitous Media Lab at the LMU Munich.
",
        "headline": "about",
        "image":"https://www.flomue.com/assets/img/florian_profile_square.jpg",
        "@context": "https://schema.org"
    }
    </script><script type="application/ld+json">
    {
        "@context": "https://schema.org",
        "@type": "ProfilePage",
        "mainEntity": {
            "@type": "Person",
            "name": "Florian Müller",
            "url": "https://www.flomue.com/",
            "image": "https://www.flomue.com/assets/img/florian_profile_square.jpg",
            "sameAs": [
                "https://www.linkedin.com/in/florian-mueller-hci",
                "https://github.com/flomuedev",
                "https://orcid.org/0000-0002-9621-6214",
                "https://scholar.google.com/citations?user=slfzfQIAAAAJ",
                "https://www.en.um.informatik.uni-muenchen.de/people/employees/mueller_florian/index.html",
                "https://dblp.uni-trier.de/pid/m/FlorianMueller-3.html",
                "https://www.researchgate.net/profile/Florian-Mueller-52",
                "https://www.semanticscholar.org/author/Florian-M%C3%BCller/144769044"

            ],
            "jobTitle": "Postdoctoral Researcher",
            "worksFor": {
                "@type": "Organization",
                "name": "LMU Munich"
            }  
        }
    }
</script> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link rel="stylesheet" href="/assets/css/fonts.css?caa062f90620afb533f3e4ca33f9d981"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?19f3075a2d19613090fe9e16b564e1fe" media="" id="highlight_theme_light"> <link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png"> <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png"> <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png"> <link rel="manifest" href="/site.webmanifest"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://www.flomue.com/"> <script src="https://cdn.telemetrydeck.com/websdk/telemetrydeck.min.js" data-app-id="A19A11BC-58CD-4EB7-83FA-40FFCFEB64F1"> </script> <link rel="stylesheet" href="/assets/css/yt-consent.css?a6d140f226e1b86136356683f6470ac1"> <script src="/assets/js/yt-consent.js?0f324e6659291dde56fbbbb9aeb6cf60"></script> </head> <body class="fixed-top-nav sticky-bottom-footer"> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item active"> <a class="nav-link" href="/">about<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications</a> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title"> <span class="font-weight-bold">Florian</span> Müller </h1> <p class="desc">urban interaction · hci4ai · ar/vr enthusiast · statistics nerd</p> </header> <article> <div class="profile float-right"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/florian_profile_square-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/florian_profile_square-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/florian_profile_square-1400.webp"></source> <img src="/assets/img/florian_profile_square.jpg?bb233f4cf48e7ea2e8dacec87b46b3c0" class="img-fluid z-depth-1 rounded-circle" width="auto" height="auto" alt="florian_profile_square.jpg" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="clearfix"> <p>I am a postdoctoral <code class="language-plaintext highlighter-rouge">Human-Computer Interaction</code> researcher at the Human-Centered Ubiquitous Media Lab at the LMU Munich. My research follows the vision of seamless <code class="language-plaintext highlighter-rouge">mobile interaction in and with the physical world</code>, where digital information, AI agents, and physical environments merge into a unified <code class="language-plaintext highlighter-rouge">eXtended Reality</code>. Towards this vision, I explore novel <code class="language-plaintext highlighter-rouge">interaction techniques</code> for mobile XR and <code class="language-plaintext highlighter-rouge">collaboration with AI</code> systems. My approach is informed by a deep understanding of <code class="language-plaintext highlighter-rouge">body-centric</code>, <code class="language-plaintext highlighter-rouge">tangible</code> and <code class="language-plaintext highlighter-rouge">haptic interfaces</code>, as well as strong technical expertise, aiming to make the interaction more efficient and accurate but also more enjoyable and fun.</p> </div> <h2><a href="/publications/" style="color: inherit;">selected publications</a></h2> <div class="publications"> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/Mueller_Diss_published-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/Mueller_Diss_published-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/Mueller_Diss_published-1400.webp"></source> <img src="/assets/img/publication_preview/Mueller_Diss_published.jpg?a54ae344509c6900fd55061bc31edb35" class="preview z-depth-1 rounded" width="auto" height="auto" alt="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Mueller_Diss_published" class="col-sm-8"> <div class="title"><a href="/publications/Mueller_Diss_published">Around-Body Interaction: Leveraging Limb-movements for Interacting in a Digitally Augmented Physical World</a></div> <div class="author"> <em>Florian Müller</em> </div> <div class="periodical"> 2020 </div> <div class="periodical"> </div> <div class="links"> <a href="/assets/pdf/Mueller_Diss_published.pdf" class="btn btn-sm z-depth-0" role="button"><i class="fa-solid fa-file-pdf"></i> PDF</a> <a href="https://dx.doi.org/10.25534/tuprints-00011388" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="ai ai-doi"> TUprints</i></a> </div> <div class="abstract hidden"> <p>Recent technological advances have made head-mounted displays (HMDs) smaller and untethered, fostering the vision of ubiquitous interaction with information in a digitally augmented physical world. For interacting with such devices, three main types of input - besides not very intuitive finger gestures - have emerged so far: 1) Touch input on the frame of the devices or 2) on accessories (controller) as well as 3) voice input. While these techniques have both advantages and disadvantages depending on the current situation of the user, they largely ignore the skills and dexterity that we show when interacting with the real world: Throughout our lives, we have trained extensively to use our limbs to interact with and manipulate the physical world around us. This thesis explores how the skills and dexterity of our upper and lower limbs, acquired and trained in interacting with the real world, can be transferred to the interaction with HMDs. Thereby, this thesis develops the vision of around-body interaction, in which we use the space around our body, defined by the reach of our limbs, for fast, accurate, and enjoyable interaction with information.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/Muller2019-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/Muller2019-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/Muller2019-1400.webp"></source> <img src="/assets/img/publication_preview/Muller2019.jpg?20c959ea24bd7d67fcd430a10bfa54fb" class="preview z-depth-1 rounded" width="auto" height="auto" alt="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Muller2019" class="col-sm-8"> <div class="title"><a href="/publications/Muller2019">Mind the Tap: Assessing Foot-Taps for Interacting with Head-Mounted Displays</a></div> <div class="author"> <em>Florian Müller</em>, Joshua McManus, <a href="https://www.sebastian-guenther.com/" rel="external nofollow noopener" target="_blank">Sebastian Günther</a>, <a href="https://mschmitz.org/" rel="external nofollow noopener" target="_blank">Martin Schmitz</a>, <a href="https://www.informatik.tu-darmstadt.de/telekooperation/staff_tk/tk_staff_details_23168.en.jsp" rel="external nofollow noopener" target="_blank">Max Mühlhäuser</a>, and Markus Funk</div> <div class="periodical"> <em>In Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems - CHI ’19</em>, 2019 </div> <div class="periodical"> </div> <div class="links"> <a href="/assets/pdf/Muller2019.pdf" class="btn btn-sm z-depth-0" role="button"><i class="fa-solid fa-file-pdf"></i> PDF</a> <a href="https://dx.doi.org/10.1145/3290605.3300707" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="ai ai-acmdl"> ACM</i></a> <a href="https://www.youtube.com/watch?v=D5hTVIEb7iA" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-youtube"></i> Video</a> </div> <div class="abstract hidden"> <p>From voice commands and air taps to touch gestures on frames: Various techniques for interacting with head-mounted displays (HMDs) have been proposed. While these techniques have both benefits and drawbacks dependent on the current situation of the user, research on interacting with HMDs has not concluded yet. In this paper, we add to the body of research on interacting with HMDs by exploring foot-tapping as an input modality. Through two controlled experiments with a total of 36 participants, we first explore direct interaction with interfaces that are displayed on the floor and require the user to look down to interact. Secondly, we investigate indirect interaction with interfaces that, although operated by the user’s feet, are always visible as they are floating in front of the user. Based on the results of the two experiments, we provide design recommendations for direct and indirect foot-based user interfaces.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/Muller2020d-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/Muller2020d-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/Muller2020d-1400.webp"></source> <img src="/assets/img/publication_preview/Muller2020d.jpg?a6512e1e7ec1be26759b485bf96e9cf7" class="preview z-depth-1 rounded" width="auto" height="auto" alt="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Muller2020d" class="col-sm-8"> <div class="title"> <i class="fa-solid fa-award" style="color: var(--global-theme-color);"></i> <a href="/publications/Muller2020d">Walk The Line: Leveraging Lateral Shifts of the Walking Path as an Input Modality for Head-Mounted Displays</a> </div> <div class="author"> <em>Florian Müller</em>, <a href="https://mschmitz.org/" rel="external nofollow noopener" target="_blank">Martin Schmitz</a>, Daniel Schmitt, <a href="https://www.sebastian-guenther.com/" rel="external nofollow noopener" target="_blank">Sebastian Günther</a>, Markus Funk, and <a href="https://www.informatik.tu-darmstadt.de/telekooperation/staff_tk/tk_staff_details_23168.en.jsp" rel="external nofollow noopener" target="_blank">Max Mühlhäuser</a> </div> <div class="periodical"> <em>In Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems</em>, Apr 2020 </div> <div class="periodical"> </div> <div class="links"> <a href="/assets/pdf/Muller2020d.pdf" class="btn btn-sm z-depth-0" role="button"><i class="fa-solid fa-file-pdf"></i> PDF</a> <a href="https://dx.doi.org/10.1145/3313831.3376852" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="ai ai-acmdl"> ACM</i></a> <a href="https://www.youtube.com/watch?v=ylAlzFqWx7g" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-youtube"></i> Video</a> <a href="https://www.youtube.com/watch?v=uQ5w3Wvrb3w" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="fa-solid fa-person-chalkboard"></i> Talk</a> </div> <div class="abstract hidden"> <p>Recent technological advances have made head-mounted displays (HMDs) smaller and untethered, fostering the vision of ubiquitous interaction in a digitally augmented physical world. Consequently, a major part of the interaction with such devices will happen on the go, calling for interaction techniques that allow users to interact while walking. In this paper, we explore lateral shifts of the walking path as a hands-free input modality. The available input options are visualized as lanes on the ground parallel to the user’s walking path. Users can select options by shifting the walking path sideways to the respective lane. We contribute the results of a controlled experiment with 18 participants, confirming the viability of our approach for fast, accurate, and joyful interactions. Further, based on the findings of the controlled experiment, we present three example applications.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/mullerTicTacToesAssessingToe2023a-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/mullerTicTacToesAssessingToe2023a-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/mullerTicTacToesAssessingToe2023a-1400.webp"></source> <img src="/assets/img/publication_preview/mullerTicTacToesAssessingToe2023a.jpg?4f5fa35ae4f4ead6fcf7e31a2a1b8812" class="preview z-depth-1 rounded" width="auto" height="auto" alt="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="mullerTicTacToesAssessingToe2023a" class="col-sm-8"> <div class="title"><a href="/publications/mullerTicTacToesAssessingToe2023a">TicTacToes: Assessing Toe Movements as an Input Modality</a></div> <div class="author"> <em>Florian Müller</em>, Daniel Schmitt, <a href="http://andriimatviienko.com/" rel="external nofollow noopener" target="_blank">Andrii Matviienko</a>, Dominik Schön, <a href="https://www.sebastian-guenther.com/" rel="external nofollow noopener" target="_blank">Sebastian Günther</a>, <a href="https://thomaskosch.com/" rel="external nofollow noopener" target="_blank">Thomas Kosch</a>, and <a href="https://mschmitz.org/" rel="external nofollow noopener" target="_blank">Martin Schmitz</a> </div> <div class="periodical"> <em>In Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems</em>, Apr 2023 </div> <div class="periodical"> </div> <div class="links"> <a href="/assets/pdf/mullerTicTacToesAssessingToe2023a.pdf" class="btn btn-sm z-depth-0" role="button"><i class="fa-solid fa-file-pdf"></i> PDF</a> <a href="https://dx.doi.org/10.1145/3544548.3580954" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="ai ai-acmdl"> ACM</i></a> <a href="http://arxiv.org/abs/2303.15811" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="ai ai-arxiv"></i> arxiv</a> <a href="https://www.youtube.com/watch?v=FzA-6F5SJ44" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-youtube"></i> Video</a> <a href="https://www.youtube.com/watch?v=2enVDAGiE8E" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="fa-solid fa-person-chalkboard"></i> Talk</a> </div> <div class="abstract hidden"> <p>From carrying grocery bags to holding onto handles on the bus, there are a variety of situations where one or both hands are busy, hindering the vision of ubiquitous interaction with technology. Voice commands, as a popular hands-free alternative, struggle with ambient noise and privacy issues. As an alternative approach, research explored movements of various body parts (e.g., head, arms) as input modalities, with foot-based techniques proving particularly suitable for hands-free interaction. Whereas previous research only considered the movement of the foot as a whole, in this work, we argue that our toes offer further degrees of freedom that can be leveraged for interaction. To explore the viability of toe-based interaction, we contribute the results of a controlled experiment with 18 participants assessing the impact of five factors on the accuracy, efficiency and user experience of such interfaces. Based on the findings, we provide design recommendations for future toe-based interfaces.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/mullerUndoPortExploringInfluence2023a-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/mullerUndoPortExploringInfluence2023a-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/mullerUndoPortExploringInfluence2023a-1400.webp"></source> <img src="/assets/img/publication_preview/mullerUndoPortExploringInfluence2023a.jpg?1c1155ce77f6a5d7a4a48972a6652d74" class="preview z-depth-1 rounded" width="auto" height="auto" alt="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="mullerUndoPortExploringInfluence2023a" class="col-sm-8"> <div class="title"><a href="/publications/mullerUndoPortExploringInfluence2023a">UndoPort: Exploring the Influence of Undo-Actions for Locomotion in Virtual Reality on the Efficiency, Spatial Understanding and User Experience</a></div> <div class="author"> <em>Florian Müller</em>, Arantxa Ye, Dominik Schön, and <a href="https://julian-rasch.com/" rel="external nofollow noopener" target="_blank">Julian Rasch</a> </div> <div class="periodical"> <em>In Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems</em>, Apr 2023 </div> <div class="periodical"> </div> <div class="links"> <a href="/assets/pdf/mullerUndoPortExploringInfluence2023a.pdf" class="btn btn-sm z-depth-0" role="button"><i class="fa-solid fa-file-pdf"></i> PDF</a> <a href="https://dx.doi.org/10.1145/3544548.3581557" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="ai ai-acmdl"> ACM</i></a> <a href="http://arxiv.org/abs/2303.15800" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="ai ai-arxiv"></i> arxiv</a> <a href="https://www.youtube.com/watch?v=BwRc4f8VSEk" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-youtube"></i> Video</a> <a href="https://www.youtube.com/watch?v=INzk1_a2Z3k" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="fa-solid fa-person-chalkboard"></i> Talk</a> </div> <div class="abstract hidden"> <p>When we get lost in Virtual Reality (VR) or want to return to a previous location, we use the same methods of locomotion for the way back as for the way forward. This is time-consuming and requires additional physical orientation changes, increasing the risk of getting tangled in the headsets’ cables. In this paper, we propose the use of undo actions to revert locomotion steps in VR. We explore eight different variations of undo actions as extensions of point&amp;teleport, based on the possibility to undo position and orientation changes together with two different visualizations of the undo step (discrete and continuous). We contribute the results of a controlled experiment with 24 participants investigating the efficiency and orientation of the undo techniques in a radial maze task. We found that the combination of position and orientation undo together with a discrete visualization resulted in the highest efficiency without increasing orientation errors.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/raschJustUndoIt2024-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/raschJustUndoIt2024-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/raschJustUndoIt2024-1400.webp"></source> <img src="/assets/img/publication_preview/raschJustUndoIt2024.jpg?c01e9d55e56d30fee90a54395b1c0b6c" class="preview z-depth-1 rounded" width="auto" height="auto" alt="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="raschJustUndoIt2024" class="col-sm-8"> <div class="title"><a href="/publications/raschJustUndoIt2024">Just Undo It: Exploring Undo Mechanics in Multi-User Virtual Reality</a></div> <div class="author"> <a href="https://julian-rasch.com/" rel="external nofollow noopener" target="_blank">Julian Rasch</a>, Florian Perzl, <a href="https://yannick-weiss.com/" rel="external nofollow noopener" target="_blank">Yannick Weiss</a>, and <em>Florian Müller</em> </div> <div class="periodical"> <em>In Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems</em>, Mar 2024 </div> <div class="periodical"> </div> <div class="links"> <a href="/assets/pdf/raschJustUndoIt2024.pdf" class="btn btn-sm z-depth-0" role="button"><i class="fa-solid fa-file-pdf"></i> PDF</a> <a href="https://dx.doi.org/10.1145/3613904.3642864" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="ai ai-acmdl"> ACM</i></a> <a href="http://arxiv.org/abs/2403.11756" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="ai ai-arxiv"></i> arxiv</a> <a href="https://www.youtube.com/watch?v=6g5sEdy-UFc" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-youtube"></i> Video</a> <a href="https://www.youtube.com/watch?v=COExWxXxy98" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="fa-solid fa-person-chalkboard"></i> Talk</a> </div> <div class="abstract hidden"> <p>With the proliferation of VR and a metaverse on the horizon, many multi-user activities are migrating to the VR world, calling for effective collaboration support. As one key feature, traditional collaborative systems provide users with undo mechanics to reverse errors and other unwanted changes. While undo has been extensively researched in this domain and is now considered industry standard, it is strikingly absent for VR systems in research and industry. This work addresses this research gap by exploring different undo techniques for basic object manipulation in different collaboration modes in VR. We conducted a study involving 32 participants organized in teams of two. Here, we studied users’ performance and preferences in a tower stacking task, varying the available undo techniques and their mode of collaboration. The results suggest that users desire and use undo in VR and that the choice of the undo technique impacts users’ performance and social connection.</p> </div> </div> </div> </li> </ol> </div> <div class="social"> <div class="contact-icons"> <a href="mailto:%6D%61%69%6C@%66%6C%6F%6D%75%65.%63%6F%6D" title="email"><i class="fa-regular fa-envelope"></i></a> <a href="https://orcid.org/0000-0002-9621-6214" title="ORCID" rel="external nofollow noopener" target="_blank"><i class="ai ai-orcid"></i></a> <a href="https://scholar.google.com/citations?user=slfzfQIAAAAJ" title="Google Scholar" rel="external nofollow noopener" target="_blank"><i class="ai ai-google-scholar-square"></i></a> <a href="https://www.semanticscholar.org/author/144769044" title="Semantic Scholar" rel="external nofollow noopener" target="_blank"><i class="ai ai-semantic-scholar"></i></a> <a href="https://www.researchgate.net/profile/Florian-Mueller-52/" title="ResearchGate" rel="external nofollow noopener" target="_blank"><i class="ai ai-researchgate"></i></a> <a href="https://github.com/flomuedev" title="GitHub" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-github"></i></a> <a href="https://www.linkedin.com/in/florian-mueller-hci" title="LinkedIn" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-linkedin"></i></a> <a href="https://www.en.um.informatik.uni-muenchen.de/people/employees/mueller_florian/index.html" title="Work" rel="external nofollow noopener" target="_blank"><i class="fa-solid fa-briefcase"></i></a> <a href="https://dblp.uni-trier.de/pid/m/FlorianMueller-3.html" title="DBLP" rel="external nofollow noopener" target="_blank"><i class="ai ai-dblp"></i></a> </div> <div class="contact-note"> Get in touch </div> </div> </article> </div> </div> <footer class="sticky-bottom mt-5"> <div class="container"> © Copyright Florian Müller | <a href="https://www.flomue.com/imprint">Impressum</a> </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script type="text/javascript">$(function(){$('[data-toggle="tooltip"]').tooltip()});</script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?7b30caa5023af4af8408a472dc4e1ebb"></script> <script defer src="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js?d633890033921b33e0ceb13d22340a9c"></script> <script defer src="/assets/js/common.js?acdb9690d7641b2f8d40529018c71a01"></script> <script defer src="/assets/js/copy_code.js?07b8786bab9b4abe90d10e61f7d12ff7" type="text/javascript"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>