<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Florian Müller</title> <meta name="author" content="Florian Müller"> <meta name="description" content="Florian Müller is an Assistant Professor for Mobile Human-Computer Interaction at TU Darmstadt. "> <meta name="keywords" content="hci, research, urban interaction"> <meta property="og:site_name" content="Florian Müller"> <meta property="og:type" content="website"> <meta property="og:title" content="Florian Müller | about"> <meta property="og:url" content="https://www.flomue.com/"> <meta property="og:description" content="Florian Müller is an Assistant Professor for Mobile Human-Computer Interaction at TU Darmstadt. "> <meta property="og:image" content="https://www.flomue.com/assets/img/florian_profile_square.jpg"> <meta property="og:locale" content="en"> <meta name="twitter:card" content="summary"> <meta name="twitter:title" content="about"> <meta name="twitter:description" content="Florian Müller is an Assistant Professor for Mobile Human-Computer Interaction at TU Darmstadt. "> <meta name="twitter:image" content="https://www.flomue.com/assets/img/florian_profile_square.jpg"> <script type="application/ld+json">
    {
        "author":
        {
            "@type": "Person",
            "name": "Florian  Müller",
            "url": "https://www.flomue.com"
        },
        "url": "https://www.flomue.com/",
        "@type": "WebSite",
        "description": "Florian Müller is an Assistant Professor for Mobile Human-Computer Interaction at TU Darmstadt.
",
        "headline": "about",
        "image":"https://www.flomue.com/assets/img/florian_profile_square.jpg",
        "@context": "https://schema.org"
    }
    </script><script type="application/ld+json">
    {
        "@context": "https://schema.org",
        "@type": "ProfilePage",
        "mainEntity": {
            "@type": "Person",
            "name": "Florian Müller",
            "url": "https://www.flomue.com/",
            "image": "https://www.flomue.com/assets/img/florian_profile_square.jpg",
            "sameAs": [
                "https://www.linkedin.com/in/florian-mueller-hci",
                "https://github.com/flomuedev",
                "https://orcid.org/0000-0002-9621-6214",
                "https://scholar.google.com/citations?user=slfzfQIAAAAJ",
                "https://www.en.um.informatik.uni-muenchen.de/people/employees/mueller_florian/index.html",
                "https://dblp.uni-trier.de/pid/m/FlorianMueller-3.html",
                "https://www.researchgate.net/profile/Florian-Mueller-52",
                "https://www.semanticscholar.org/author/Florian-M%C3%BCller/144769044",
                "https://www.informatik.tu-darmstadt.de/hci/hci_tuda/team_hci/details_152256.de.jsp"
            ],
            "jobTitle": "Assistant Professor",
            "worksFor": {
                "@type": "Organization",
                "name": "TU Darmstadt"
            }  
        }
    }
</script> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link rel="stylesheet" href="/assets/css/fonts.css?caa062f90620afb533f3e4ca33f9d981"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?19f3075a2d19613090fe9e16b564e1fe" media="" id="highlight_theme_light"> <link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png"> <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png"> <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png"> <link rel="manifest" href="/site.webmanifest"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://www.flomue.com/"> <script src="https://cdn.telemetrydeck.com/websdk/telemetrydeck.min.js" data-app-id="A19A11BC-58CD-4EB7-83FA-40FFCFEB64F1"> </script> <link rel="stylesheet" href="/assets/css/yt-consent.css?a6d140f226e1b86136356683f6470ac1"> <script src="/assets/js/yt-consent.js?0f324e6659291dde56fbbbb9aeb6cf60"></script> </head> <body class="fixed-top-nav sticky-bottom-footer"> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item active"> <a class="nav-link" href="/">about<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications</a> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title"> <span class="font-weight-bold">Florian</span> Müller </h1> <p class="desc">urban interaction · hci4ai · ar/vr enthusiast · statistics nerd</p> </header> <article> <div class="profile float-right"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/florian_profile_square-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/florian_profile_square-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/florian_profile_square-1400.webp"></source> <img src="/assets/img/florian_profile_square.jpg?bb233f4cf48e7ea2e8dacec87b46b3c0" class="img-fluid z-depth-1 rounded-circle" width="auto" height="auto" alt="florian_profile_square.jpg" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="clearfix"> <p>I am an Assistant Professor for <code class="language-plaintext highlighter-rouge">Mobile Human-Computer Interaction</code> at TU Darmstadt where I lead the <a href="https://www.informatik.tu-darmstadt.de/hci/hci_tuda/urbanpage.de.jsp" rel="external nofollow noopener" target="_blank">Urban Interaction Lab</a>. My research follows the vision of seamless <code class="language-plaintext highlighter-rouge">mobile interaction in and with the physical world</code>, where digital information, AI agents, and physical environments merge into a unified <code class="language-plaintext highlighter-rouge">eXtended Reality</code>. Towards this vision, I explore novel <code class="language-plaintext highlighter-rouge">interaction techniques</code> for mobile XR and <code class="language-plaintext highlighter-rouge">collaboration with AI</code> systems. My approach is informed by a deep understanding of <code class="language-plaintext highlighter-rouge">body-centric</code>, <code class="language-plaintext highlighter-rouge">tangible</code> and <code class="language-plaintext highlighter-rouge">haptic interfaces</code>, as well as strong technical expertise, aiming to make the interaction more efficient and accurate but also more enjoyable and fun.</p> </div> <h2><a href="/publications/" style="color: inherit;">latest publications</a></h2> <div class="publications"> <h2 class="bibliography">2025</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/ghavamianBitterTasteConfidence2025-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/ghavamianBitterTasteConfidence2025-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/ghavamianBitterTasteConfidence2025-1400.webp"></source> <img src="/assets/img/publication_preview/ghavamianBitterTasteConfidence2025.jpg?97dab6dd426a326b636e728e90674169" class="preview z-depth-1 rounded" width="auto" height="auto" alt="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="ghavamianBitterTasteConfidence2025" class="col-sm-8"> <div class="title"><a href="/publications/ghavamianBitterTasteConfidence2025">The Bitter Taste of Confidence: Exploring Audio-Visual Taste Modulation in Immersive Reality</a></div> <div class="author"> Pooria Ghavamian, Jan Henri Beyer, Sophie Orth, Mia Johanna Nona Zech, <em>Florian Müller</em>, and <a href="http://andriimatviienko.com/" rel="external nofollow noopener" target="_blank">Andrii Matviienko</a> </div> <div class="periodical"> <em>In Proceedings of the 2025 ACM International Conference on Interactive Media Experiences</em>, May 2025 </div> <div class="periodical"> </div> <div class="links"> <a href="/assets/pdf/ghavamianBitterTasteConfidence2025.pdf" class="btn btn-sm z-depth-0" role="button"><i class="fa-solid fa-file-pdf"></i> PDF</a> <a href="https://dx.doi.org/10.1145/3706370.3731654" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="ai ai-acmdl"> ACM</i></a> </div> <div class="abstract hidden"> <p>Extended Reality (XR) technologies present innovative ways to augment sensory experiences, including taste perception. In this study, we investigated how augmented reality (AR) visual filters and synchronized audio cues affect gustation through a controlled experiment with 18 participants. Our findings revealed unexpected crossmodal interactions: while pink visual filter typically associated with sweetness reduced perceived bitterness in isolation, it paradoxically enhanced bitterness perception when combined with sweet-associated audio cue. Furthermore, we observed an inverse correlation between participant confidence levels and their perception of taste intensities across multiple dimensions, highlighting confidence as an overlooked factor in sensory experience design. These findings inform the design of nuanced multisensory experiences in immersive media, where subtle crossmodal interactions significantly influence user perception.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/hirschSocialMediARverseInvestigating2025-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/hirschSocialMediARverseInvestigating2025-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/hirschSocialMediARverseInvestigating2025-1400.webp"></source> <img src="/assets/img/publication_preview/hirschSocialMediARverseInvestigating2025.jpg?9c7a22faa200802a4f867dd6aedce7f9" class="preview z-depth-1 rounded" width="auto" height="auto" alt="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="hirschSocialMediARverseInvestigating2025" class="col-sm-8"> <div class="title"><a href="/publications/hirschSocialMediARverseInvestigating2025">Social MediARverse: Investigating Users’ Social Media Content Sharing and Consuming Intentions with Location-Based AR</a></div> <div class="author"> <a href="https://www.lindahirsch.de/" rel="external nofollow noopener" target="_blank">Linda Hirsch</a>, Florian Mueller, Mari Kruse, Andreas Butz, and <a href="https://human-ai-interaction.com/" rel="external nofollow noopener" target="_blank">Robin Welsch</a> </div> <div class="periodical"> <em>Virtual Reality</em>, Jul 2025 </div> <div class="periodical"> </div> <div class="links"> <a href="/assets/pdf/hirschSocialMediARverseInvestigating2025.pdf" class="btn btn-sm z-depth-0" role="button"><i class="fa-solid fa-file-pdf"></i> PDF</a> <a href="https://dx.doi.org/10.1007/s10055-025-01188-z" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="ai ai-springer"> Springer</i></a> <a href="http://arxiv.org/abs/2409.00211" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="ai ai-arxiv"></i> arxiv</a> </div> <div class="abstract hidden"> <p>Augmented Reality (AR) is evolving to become the next frontier in social media, merging physical and virtual reality into a living metaverse, a Social MediARverse. With this transition, we must understand how different contexts—public, semi-public, and private—affect user engagement with AR content. We address this gap in current research by conducting an online survey with 110 participants, showcasing 36 AR videos, and polling them about the content’s fit and appropriateness. Specifically, we manipulated these three spaces, two forms of dynamism (dynamic vs. static), and two dimensionalities (2D vs. 3D). Our findings reveal that dynamic AR content is generally more favorably received than static content. Additionally, users find sharing and engaging with AR content in private settings more comfortable than in others. By this, the study offers valuable insights for designing and implementing future Social MediARverses and guides industry and academia on content visualization and contextual considerations.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/limbagoDontTheyReally2025-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/limbagoDontTheyReally2025-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/limbagoDontTheyReally2025-1400.webp"></source> <img src="/assets/img/publication_preview/limbagoDontTheyReally2025.jpg?5d6075e6c025f820a6fd5cf6e4e58538" class="preview z-depth-1 rounded" width="auto" height="auto" alt="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="limbagoDontTheyReally2025" class="col-sm-8"> <div class="title"><a href="/publications/limbagoDontTheyReally2025">Don’t They Really Hear Us? A Design Space for Private Conversations in Social Virtual Reality</a></div> <div class="author"> Josephus Jasper Limbago, <a href="https://human-ai-interaction.com/" rel="external nofollow noopener" target="_blank">Robin Welsch</a>, <em>Florian Müller</em>, and Mario Di Francesco</div> <div class="periodical"> <em>IEEE Transactions on Visualization and Computer Graphics</em>, Jul 2025 </div> <div class="periodical"> </div> <div class="links"> <a href="/assets/pdf/limbagoDontTheyReally2025.pdf" class="btn btn-sm z-depth-0" role="button"><i class="fa-solid fa-file-pdf"></i> PDF</a> <a href="https://dx.doi.org/10.1109/TVCG.2025.3549844" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="ai ai-ieee"> IEEE</i></a> </div> <div class="abstract hidden"> <p>Seamless transition between public dialogue and private talks is essential in everyday conversations. Social Virtual Reality (VR) has revolutionized interpersonal communication by creating a sense of closeness over distance through virtual avatars. However, existing social VR platforms are not successful in providing safety and supporting private conversations, thereby hindering self-disclosure and limiting the potential for meaningful experiences. We approach this problem by exploring the factors affecting private conversations in social VR applications, including the usability of different interaction methods and the awareness with respect to the virtual world. We conduct both expert interviews and a controlled experiment with a social VR prototype we realized. We then leverage the outcomes of the two studies to establish a design space that considers diverse dimensions (including privacy levels, social awareness, and modalities), laying the groundwork for more intuitive and meaningful experiences of private conversation in social VR.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/raschARYouTrack2025-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/raschARYouTrack2025-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/raschARYouTrack2025-1400.webp"></source> <img src="/assets/img/publication_preview/raschARYouTrack2025.jpg?7a02fdb1b97e3d7342a3d7c9e58a6b37" class="preview z-depth-1 rounded" width="auto" height="auto" alt="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="raschARYouTrack2025" class="col-sm-8"> <div class="title"><a href="/publications/raschARYouTrack2025">AR You on Track? Investigating Effects of Augmented Reality Anchoring on Dual-Task Performance While Walking</a></div> <div class="author"> <a href="https://julian-rasch.com/" rel="external nofollow noopener" target="_blank">Julian Rasch</a>, Matthias Wilhalm, <em>Florian Müller</em>, and <a href="https://www.francesco-chiossi-hci.com/" rel="external nofollow noopener" target="_blank">Francesco Chiossi</a> </div> <div class="periodical"> <em>In Proceedings of the 2025 CHI Conference on Human Factors in Computing Systems - CHI ’25</em>, Jul 2025 </div> <div class="periodical"> </div> <div class="links"> <a href="/assets/pdf/raschARYouTrack2025.pdf" class="btn btn-sm z-depth-0" role="button"><i class="fa-solid fa-file-pdf"></i> PDF</a> <a href="https://dx.doi.org/10.1145/3706598.3714258" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="ai ai-acmdl"> ACM</i></a> <a href="http://arxiv.org/abs/2502.20944" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="ai ai-arxiv"></i> arxiv</a> </div> <div class="abstract hidden"> <p>With the increasing spread of AR head-mounted displays suitable for everyday use, interaction with information becomes ubiquitous, even while walking. However, this requires constant shifts of our attention between walking and interacting with virtual information to fulfill both tasks adequately. Accordingly, we as a community need a thorough understanding of the mutual influences of walking and interacting with digital information to design safe yet effective interactions. Thus, we systematically investigate the effects of different AR anchors (hand, head, torso) and task difficulties on user experience and performance. We engage participants (n=26) in a dual-task paradigm involving a visual working memory task while walking. We assess the impact of dual-tasking on both virtual and walking performance, and subjective evaluations of mental and physical load. Our results show that head-anchored AR content least affected walking while allowing for fast and accurate virtual task interaction, while hand-anchored content increased reaction times and workload.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/raschCreepyCoCreatorInvestigatingAI2025-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/raschCreepyCoCreatorInvestigatingAI2025-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/raschCreepyCoCreatorInvestigatingAI2025-1400.webp"></source> <img src="/assets/img/publication_preview/raschCreepyCoCreatorInvestigatingAI2025.jpg?e619f60cb6381277d63b6b200fe2ae31" class="preview z-depth-1 rounded" width="auto" height="auto" alt="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="raschCreepyCoCreatorInvestigatingAI2025" class="col-sm-8"> <div class="title"><a href="/publications/raschCreepyCoCreatorInvestigatingAI2025">CreepyCoCreator? Investigating AI Representation Modes for 3D Object Co-Creation in Virtual Reality</a></div> <div class="author"> <a href="https://julian-rasch.com/" rel="external nofollow noopener" target="_blank">Julian Rasch</a>, Julia Töws, Teresa Hirzle, <em>Florian Müller</em>, and <a href="https://mschmitz.org/" rel="external nofollow noopener" target="_blank">Martin Schmitz</a> </div> <div class="periodical"> <em>In Proceedings of the 2025 CHI Conference on Human Factors in Computing Systems - CHI ’25</em>, Jul 2025 </div> <div class="periodical"> </div> <div class="links"> <a href="/assets/pdf/raschCreepyCoCreatorInvestigatingAI2025.pdf" class="btn btn-sm z-depth-0" role="button"><i class="fa-solid fa-file-pdf"></i> PDF</a> <a href="https://dx.doi.org/10.1145/3706598.3713720" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="ai ai-acmdl"> ACM</i></a> <a href="http://arxiv.org/abs/2502.03069" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="ai ai-arxiv"></i> arxiv</a> </div> <div class="abstract hidden"> <p>Generative AI in Virtual Reality offers the potential for collaborative object-building, yet challenges remain in aligning AI contributions with user expectations. In particular, users often struggle to understand and collaborate with AI when its actions are not transparently represented. This paper thus explores the co-creative object-building process through a Wizard-of-Oz study, focusing on how AI can effectively convey its intent to users during object customization in Virtual Reality. Inspired by human-to-human collaboration, we focus on three representation modes: the presence of an embodied avatar, whether the AI’s contributions are visualized immediately or incrementally, and whether the areas modified are highlighted in advance. The findings provide insights into how these factors affect user perception and interaction with object-generating AI tools in Virtual Reality as well as satisfaction and ownership of the created objects. The results offer design implications for co-creative world-building systems, aiming to foster more effective and satisfying collaborations between humans and AI in Virtual Reality.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/raschVisionAIDrivenAdaptation2025-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/raschVisionAIDrivenAdaptation2025-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/raschVisionAIDrivenAdaptation2025-1400.webp"></source> <img src="/assets/img/publication_preview/raschVisionAIDrivenAdaptation2025.jpg?6079be780ebb2d093b477a7c9927fb59" class="preview z-depth-1 rounded" width="auto" height="auto" alt="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="raschVisionAIDrivenAdaptation2025" class="col-sm-8"> <div class="title"><a href="/publications/raschVisionAIDrivenAdaptation2025">A Vision for AI-Driven Adaptation of Dynamic AR Content to Users and Environments</a></div> <div class="author"> <a href="https://julian-rasch.com/" rel="external nofollow noopener" target="_blank">Julian Rasch</a>, <em>Florian Müller</em>, and <a href="https://www.francesco-chiossi-hci.com/" rel="external nofollow noopener" target="_blank">Francesco Chiossi</a> </div> <div class="periodical"> Apr 2025 </div> <div class="periodical"> </div> <div class="links"> <a href="/assets/pdf/raschVisionAIDrivenAdaptation2025.pdf" class="btn btn-sm z-depth-0" role="button"><i class="fa-solid fa-file-pdf"></i> PDF</a> <a href="http://arxiv.org/abs/2504.16562" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="ai ai-arxiv"></i> arxiv</a> </div> <div class="abstract hidden"> <p>Augmented Reality (AR) is transforming the way we interact with virtual information in the physical world. By overlaying digital content in real-world environments, AR enables new forms of immersive and engaging experiences. However, existing AR systems often struggle to effectively manage the many interactive possibilities that AR presents. This vision paper speculates on AI-driven approaches for adaptive AR content placement, dynamically adjusting to user movement and environmental changes. By leveraging machine learning methods, such a system would intelligently manage content distribution between AR projections integrated into the external environment and fixed static content, enabling seamless UI layout and potentially reducing users’ cognitive load. By exploring the possibilities of AI-driven dynamic AR content placement, we aim to envision new opportunities for innovation and improvement in various industries, from urban navigation and workplace productivity to immersive learning and beyond. This paper outlines a vision for the development of more intuitive, engaging, and effective AI-powered AR experiences.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/sayffaerthExpertsEyesExploring2025-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/sayffaerthExpertsEyesExploring2025-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/sayffaerthExpertsEyesExploring2025-1400.webp"></source> <img src="/assets/img/publication_preview/sayffaerthExpertsEyesExploring2025.jpg?d61e0af2381f4f3aa6c51c97f3bd6ebe" class="preview z-depth-1 rounded" width="auto" height="auto" alt="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="sayffaerthExpertsEyesExploring2025" class="col-sm-8"> <div class="title"><a href="/publications/sayffaerthExpertsEyesExploring2025">Through the Expert’s Eyes: Exploring Asynchronous Expert Perspectives and Gaze Visualizations in XR</a></div> <div class="author"> Clara Sayffaerth, Annika Köhler, <a href="https://julian-rasch.com/" rel="external nofollow noopener" target="_blank">Julian Rasch</a>, Albrecht Schmidt, and <em>Florian Müller</em> </div> <div class="periodical"> Aug 2025 </div> <div class="periodical"> </div> <div class="links"> <a href="/assets/pdf/sayffaerthExpertsEyesExploring2025.pdf" class="btn btn-sm z-depth-0" role="button"><i class="fa-solid fa-file-pdf"></i> PDF</a> <a href="http://arxiv.org/abs/2509.00944" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="ai ai-arxiv"></i> arxiv</a> </div> <div class="abstract hidden"> <p>Transferring knowledge across generations is fundamental to human civilization, yet the challenge of passing on complex practical skills persists. Methods without a physically present instructor, such as videos, often fail to explain complex manual tasks, where spatial and social factors are critical. Technologies such as eXtended Reality and Artificial Intelligence hold the potential to retain expert knowledge and facilitate the creation of tailored, contextualized, and asynchronous explanations regardless of time and place. In contrast to videos, the learner’s perspective can be different from the recorded perspective in XR. This paper investigates the impact of asynchronous first- and third-person perspectives and gaze visualizations on efficiency, feeling of embodiment, and connectedness during manual tasks. The empirical results of our study (N=36) show that the first-person perspective is better in quantitative measures and preferred by users. We identify best practices for presenting preserved knowledge and provide guidelines for designing future systems.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/schonPegsPixelsComparative2025-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/schonPegsPixelsComparative2025-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/schonPegsPixelsComparative2025-1400.webp"></source> <img src="/assets/img/publication_preview/schonPegsPixelsComparative2025.jpg?b5376fd0c2d1dd08eb2e0ef74d240c10" class="preview z-depth-1 rounded" width="auto" height="auto" alt="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="schonPegsPixelsComparative2025" class="col-sm-8"> <div class="title"><a href="/publications/schonPegsPixelsComparative2025">From Pegs to Pixels: A Comparative Analysis of the Nine Hole Peg Test and a Digital Copy Drawing Test for Fine Motor Control Assessment</a></div> <div class="author"> Dominik Schön, <a href="https://thomaskosch.com/" rel="external nofollow noopener" target="_blank">Thomas Kosch</a>, <a href="https://mschmitz.org/" rel="external nofollow noopener" target="_blank">Martin Schmitz</a>, <a href="https://www.sebastian-guenther.com/" rel="external nofollow noopener" target="_blank">Sebastian Günther</a>, <a href="https://www.informatik.tu-darmstadt.de/telekooperation/staff_tk/tk_staff_details_23168.en.jsp" rel="external nofollow noopener" target="_blank">Max Mühlhäuser</a>, and <em>Florian Müller</em> </div> <div class="periodical"> <em>Proc. ACM Hum.-Comput. Interact.</em>, Sep 2025 </div> <div class="periodical"> </div> <div class="links"> <a href="/assets/pdf/schonPegsPixelsComparative2025.pdf" class="btn btn-sm z-depth-0" role="button"><i class="fa-solid fa-file-pdf"></i> PDF</a> <a href="https://dx.doi.org/10.1145/3743714" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="ai ai-acmdl"> ACM</i></a> <a href="entry.supps" class="btn btn-sm z-depth-0" role="button"><i class="ai ai-open-data"></i> data</a> </div> <div class="abstract hidden"> <p>User interaction with digital systems requires Fine Motor Control (FMC), especially if the interfaces are complex or require high fidelity and fine-grained interactions. Despite its importance, Fine Motor Control is often overlooked in interactive system design, partly because of its complex assessment. Measuring changes in fine motor abilities due to prolonged use or fatigue currently requires repeated manual testing. This paper analyzes the concept of using the digital mobile devices’ input behavior to assess the user’s Fine Motor Control. For this, we show that Fine Motor Control can be assessed for touch and stylus-based interaction with a digital mobile system. We conducted a user study, where participants performed a Nine Hole Peg Test and a predefined Copy Drawing Test before and after exercises that affect fine motor skills. Based on this data, we investigated how metrics such as pressure, velocity, and entropy for touch and stylus input can be used to predict Fine Motor Control.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/weingartnerAssessingVisualizationInteraction2025-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/weingartnerAssessingVisualizationInteraction2025-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/weingartnerAssessingVisualizationInteraction2025-1400.webp"></source> <img src="/assets/img/publication_preview/weingartnerAssessingVisualizationInteraction2025.jpg?fcafbacfcfa8f4429be4afbdfc6a4ae5" class="preview z-depth-1 rounded" width="auto" height="auto" alt="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="weingartnerAssessingVisualizationInteraction2025" class="col-sm-8"> <div class="title"><a href="/publications/weingartnerAssessingVisualizationInteraction2025">Assessing Visualization and Interaction Techniques to Support Comparison Tasks in Virtual Reality</a></div> <div class="author"> Henrike Weingärtner, <a href="https://julian-rasch.com/" rel="external nofollow noopener" target="_blank">Julian Rasch</a>, Nils Rothamel, and <em>Florian Müller</em> </div> <div class="periodical"> <em>In Proceedings of the Extended Abstracts of the CHI Conference on Human Factors in Computing Systems</em>, Apr 2025 </div> <div class="periodical"> </div> <div class="links"> <a href="/assets/pdf/weingartnerAssessingVisualizationInteraction2025.pdf" class="btn btn-sm z-depth-0" role="button"><i class="fa-solid fa-file-pdf"></i> PDF</a> <a href="https://dx.doi.org/10.1145/3706599.3719737" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="ai ai-acmdl"> ACM</i></a> </div> <div class="abstract hidden"> <p>Desktop screens are effective for supporting comparison tasks, but as the scale increases to room-sized or larger structures, context is lost. Users are forced to focus on isolated details through panning, zooming, and scrolling, making it difficult to maintain an overview while exploring finer details. Virtual Reality (VR) potentially offers a solution to this problem by immersing users in 3D spaces and enabling more intuitive comparisons. While related work has proposed many solutions for visualizing and interacting for comparison tasks in desktop environments, knowledge regarding the efficacy of supporting such tasks in VR environments is still lacking. We investigated varying visualization and interaction techniques in a controlled experiment with 24 participants. Our findings provide valuable insights for designing VR systems that improve usability, reduce workload, and enhance performance in comparison tasks.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/wuOneDoesNot2025-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/wuOneDoesNot2025-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/wuOneDoesNot2025-1400.webp"></source> <img src="/assets/img/publication_preview/wuOneDoesNot2025.jpg?095b75b4928365c4ba1100a39d471cc8" class="preview z-depth-1 rounded" width="auto" height="auto" alt="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="wuOneDoesNot2025" class="col-sm-8"> <div class="title"> <i class="fa-solid fa-award" style="color: var(--global-theme-color);"></i> <a href="/publications/wuOneDoesNot2025">One Does Not Simply Meme Alone: Evaluating Co-Creativity Between LLMs and Humans in the Generation of Humor</a> </div> <div class="author"> Zhikun Wu, Thomas Weber, and <em>Florian Müller</em> </div> <div class="periodical"> <em>In Proceedings of the 30th International Conference on Intelligent User Interfaces</em>, Mar 2025 </div> <div class="periodical"> </div> <div class="links"> <a href="/assets/pdf/wuOneDoesNot2025.pdf" class="btn btn-sm z-depth-0" role="button"><i class="fa-solid fa-file-pdf"></i> PDF</a> <a href="https://dx.doi.org/10.1145/3708359.3712094" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="ai ai-acmdl"> ACM</i></a> <a href="http://arxiv.org/abs/2501.11433" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="ai ai-arxiv"></i> arxiv</a> </div> <div class="abstract hidden"> <p>Collaboration has been shown to enhance creativity, leading to more innovative and effective outcomes. While previous research has explored the abilities of Large Language Models (LLMs) to serve as co-creative partners in tasks like writing poetry or creating narratives, the collaborative potential of LLMs in humor-rich and culturally nuanced domains remains an open question. To address this gap, we conducted a user study to explore the potential of LLMs in co-creating memes—a humor-driven and culturally specific form of creative expression. We conducted a user study with three groups of 50 participants each: a human-only group creating memes without AI assistance, a human-AI collaboration group interacting with a state-of-the-art LLM model, and an AI-only group where the LLM autonomously generated memes. We assessed the quality of the generated memes through crowdsourcing, with each meme rated on creativity, humor, and shareability. Our results showed that LLM assistance increased the number of ideas generated and reduced the effort participants felt. However, it did not improve the quality of the memes when humans were collaborated with LLM. Interestingly, memes created entirely by AI performed better than both human-only and human-AI collaborative memes in all areas on average. However, when looking at the top-performing memes, human-created ones were better in humor, while human-AI collaborations stood out in creativity and shareability. These findings highlight the complexities of human-AI collaboration in creative tasks. While AI can boost productivity and create content that appeals to a broad audience, human creativity remains crucial for content that connects on a deeper level.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/zyskaPullRequestsClassroom2025-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/zyskaPullRequestsClassroom2025-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/zyskaPullRequestsClassroom2025-1400.webp"></source> <img src="/assets/img/publication_preview/zyskaPullRequestsClassroom2025.jpg?4ae7af8d1793afca648eca0d566e82fe" class="preview z-depth-1 rounded" width="auto" height="auto" alt="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="zyskaPullRequestsClassroom2025" class="col-sm-8"> <div class="title"><a href="/publications/zyskaPullRequestsClassroom2025">Pull Requests From The Classroom: Co-Developing Curriculum And Code</a></div> <div class="author"> <a href="http://zyska.org/" rel="external nofollow noopener" target="_blank">Dennis Zyska</a>, Ilia Kuznetsov, <em>Florian Müller</em>, and Iryna Gurevych</div> <div class="periodical"> <em>In Proceedings of Mensch Und Computer 2025 - MuC ’25</em>, Aug 2025 </div> <div class="periodical"> </div> <div class="links"> <a href="/assets/pdf/zyskaPullRequestsClassroom2025.pdf" class="btn btn-sm z-depth-0" role="button"><i class="fa-solid fa-file-pdf"></i> PDF</a> <a href="https://dx.doi.org/10.1145/3743049.3748581" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="ai ai-acmdl"> ACM</i></a> <a href="http://arxiv.org/abs/2508.00646" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="ai ai-arxiv"></i> arxiv</a> </div> <div class="abstract hidden"> <p>Educational technologies often misalign with instructors’ pedagogical goals, forcing adaptations that compromise teaching efficacy. In this paper, we present a case study on the co-development of curriculum and technology in the context of a university course on scientific writing. Specifically, we examine how a custom-built peer feedback system was iteratively developed alongside the course to support annotation, feedback exchange, and revision. Results show that while co-development fostered stronger alignment between software features and course goals, it also exposed usability limitations and infrastructure-related frustrations, emphasizing the need for closer coordination between teaching and technical teams.</p> </div> </div> </div> </li> </ol> </div> <p><a href="/publications/">Show all publications</a></p> <div class="social"> <div class="contact-icons"> <a href="mailto:%6D%61%69%6C@%66%6C%6F%6D%75%65.%63%6F%6D" title="email"><i class="fa-regular fa-envelope"></i></a> <a href="https://orcid.org/0000-0002-9621-6214" title="ORCID" rel="external nofollow noopener" target="_blank"><i class="ai ai-orcid"></i></a> <a href="https://scholar.google.com/citations?user=slfzfQIAAAAJ" title="Google Scholar" rel="external nofollow noopener" target="_blank"><i class="ai ai-google-scholar-square"></i></a> <a href="https://www.semanticscholar.org/author/144769044" title="Semantic Scholar" rel="external nofollow noopener" target="_blank"><i class="ai ai-semantic-scholar"></i></a> <a href="https://www.researchgate.net/profile/Florian-Mueller-52/" title="ResearchGate" rel="external nofollow noopener" target="_blank"><i class="ai ai-researchgate"></i></a> <a href="https://github.com/flomuedev" title="GitHub" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-github"></i></a> <a href="https://www.linkedin.com/in/florian-mueller-hci" title="LinkedIn" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-linkedin"></i></a> <a href="https://www.informatik.tu-darmstadt.de/hci/hci_tuda/team_hci/details_152256.de.jsp" title="Work" rel="external nofollow noopener" target="_blank"><i class="fa-solid fa-briefcase"></i></a> <a href="https://dblp.uni-trier.de/pid/m/FlorianMueller-3.html" title="DBLP" rel="external nofollow noopener" target="_blank"><i class="ai ai-dblp"></i></a> </div> <div class="contact-note"> Get in touch </div> </div> </article> </div> </div> <footer class="sticky-bottom mt-5"> <div class="container"> © Copyright Florian Müller | <a href="https://www.flomue.com/imprint">Impressum</a> </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script type="text/javascript">$(function(){$('[data-toggle="tooltip"]').tooltip()});</script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?7b30caa5023af4af8408a472dc4e1ebb"></script> <script defer src="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js?d633890033921b33e0ceb13d22340a9c"></script> <script defer src="/assets/js/common.js?acdb9690d7641b2f8d40529018c71a01"></script> <script defer src="/assets/js/copy_code.js?07b8786bab9b4abe90d10e61f7d12ff7" type="text/javascript"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>