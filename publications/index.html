<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>publications | Florian Müller</title> <meta name="author" content="Florian Müller"> <meta name="description" content="Florian Müller authored and co-authored 90+ papers at international peer-reviewed conferences and journals."> <meta name="keywords" content="hci, research, urban interaction"> <meta property="og:site_name" content="Florian Müller"> <meta property="og:type" content="website"> <meta property="og:title" content="Florian Müller | publications"> <meta property="og:url" content="https://www.flomue.com/publications/"> <meta property="og:description" content="Florian Müller authored and co-authored 90+ papers at international peer-reviewed conferences and journals."> <meta property="og:image" content="https://www.flomue.com/assets/img/florian_profile_square.jpg"> <meta property="og:locale" content="en"> <meta name="twitter:card" content="summary"> <meta name="twitter:title" content="publications"> <meta name="twitter:description" content="Florian Müller authored and co-authored 90+ papers at international peer-reviewed conferences and journals."> <meta name="twitter:image" content="https://www.flomue.com/assets/img/florian_profile_square.jpg"> <script type="application/ld+json">
    {
        "author":
        {
            "@type": "Person",
            "name": "Florian  Müller",
            "url": "https://www.flomue.com"
        },
        "url": "https://www.flomue.com/publications/",
        "@type": "WebSite",
        "description": "Florian Müller authored and co-authored 90+ papers at international peer-reviewed conferences and journals.",
        "headline": "publications",
        "image":"https://www.flomue.com/assets/img/florian_profile_square.jpg",
        "@context": "https://schema.org"
    }
    </script> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link rel="stylesheet" href="/assets/css/fonts.css?caa062f90620afb533f3e4ca33f9d981"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?19f3075a2d19613090fe9e16b564e1fe" media="" id="highlight_theme_light"> <link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png"> <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png"> <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png"> <link rel="manifest" href="/site.webmanifest"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://www.flomue.com/publications/"> <script src="https://cdn.telemetrydeck.com/websdk/telemetrydeck.min.js" data-app-id="A19A11BC-58CD-4EB7-83FA-40FFCFEB64F1"> </script> <link rel="stylesheet" href="/assets/css/yt-consent.css?a6d140f226e1b86136356683f6470ac1"> <script src="/assets/js/yt-consent.js?0f324e6659291dde56fbbbb9aeb6cf60"></script> </head> <body class="fixed-top-nav sticky-bottom-footer"> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Florian </span>Müller</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about</a> </li> <li class="nav-item active"> <a class="nav-link" href="/publications/">publications<span class="sr-only">(current)</span></a> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">publications</h1> <p class="post-description">Distinguished papers are marked with <i class="fa-solid fa-award" style="color: var(--global-theme-color);"></i> for Honourable Mention Awards and <i class="fa-solid fa-trophy" style="color: var(--global-theme-color);"></i> for Best Paper Awards. For an up‑to‑date list of 90+ publications, please see <a href="https://scholar.google.com/citations?user=slfzfQIAAAAJ" rel="external nofollow noopener" target="_blank"><i class="fa-solid fa-trophy"></i> Google Scholar</a>.</p> </header> <article> <div class="publications"> <h2 class="bibliography">2025</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/ghavamianBitterTasteConfidence2025-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/ghavamianBitterTasteConfidence2025-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/ghavamianBitterTasteConfidence2025-1400.webp"></source> <img src="/assets/img/publication_preview/ghavamianBitterTasteConfidence2025.jpg?97dab6dd426a326b636e728e90674169" class="preview z-depth-1 rounded" width="auto" height="auto" alt="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="ghavamianBitterTasteConfidence2025" class="col-sm-8"> <div class="title"><a href="/publications/ghavamianBitterTasteConfidence2025">The Bitter Taste of Confidence: Exploring Audio-Visual Taste Modulation in Immersive Reality</a></div> <div class="author"> Pooria Ghavamian, Jan Henri Beyer, Sophie Orth, Mia Johanna Nona Zech, <em>Florian Müller</em>, and <a href="http://andriimatviienko.com/" rel="external nofollow noopener" target="_blank">Andrii Matviienko</a> </div> <div class="periodical"> <em>In Proceedings of the 2025 ACM International Conference on Interactive Media Experiences</em>, May 2025 </div> <div class="periodical"> </div> <div class="links"> <a href="/assets/pdf/ghavamianBitterTasteConfidence2025.pdf" class="btn btn-sm z-depth-0" role="button"><i class="fa-solid fa-file-pdf"></i> PDF</a> <a href="https://dx.doi.org/10.1145/3706370.3731654" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="ai ai-acmdl"> ACM</i></a> </div> <div class="abstract hidden"> <p>Extended Reality (XR) technologies present innovative ways to augment sensory experiences, including taste perception. In this study, we investigated how augmented reality (AR) visual filters and synchronized audio cues affect gustation through a controlled experiment with 18 participants. Our findings revealed unexpected crossmodal interactions: while pink visual filter typically associated with sweetness reduced perceived bitterness in isolation, it paradoxically enhanced bitterness perception when combined with sweet-associated audio cue. Furthermore, we observed an inverse correlation between participant confidence levels and their perception of taste intensities across multiple dimensions, highlighting confidence as an overlooked factor in sensory experience design. These findings inform the design of nuanced multisensory experiences in immersive media, where subtle crossmodal interactions significantly influence user perception.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/hirschSocialMediARverseInvestigating2025-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/hirschSocialMediARverseInvestigating2025-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/hirschSocialMediARverseInvestigating2025-1400.webp"></source> <img src="/assets/img/publication_preview/hirschSocialMediARverseInvestigating2025.jpg?9c7a22faa200802a4f867dd6aedce7f9" class="preview z-depth-1 rounded" width="auto" height="auto" alt="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="hirschSocialMediARverseInvestigating2025" class="col-sm-8"> <div class="title"><a href="/publications/hirschSocialMediARverseInvestigating2025">Social MediARverse: Investigating Users’ Social Media Content Sharing and Consuming Intentions with Location-Based AR</a></div> <div class="author"> <a href="https://www.lindahirsch.de/" rel="external nofollow noopener" target="_blank">Linda Hirsch</a>, Florian Mueller, Mari Kruse, Andreas Butz, and <a href="https://human-ai-interaction.com/" rel="external nofollow noopener" target="_blank">Robin Welsch</a> </div> <div class="periodical"> <em>Virtual Reality</em>, Jul 2025 </div> <div class="periodical"> </div> <div class="links"> <a href="/assets/pdf/hirschSocialMediARverseInvestigating2025.pdf" class="btn btn-sm z-depth-0" role="button"><i class="fa-solid fa-file-pdf"></i> PDF</a> <a href="https://dx.doi.org/10.1007/s10055-025-01188-z" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="ai ai-springer"> Springer</i></a> <a href="http://arxiv.org/abs/2409.00211" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="ai ai-arxiv"></i> arxiv</a> </div> <div class="abstract hidden"> <p>Augmented Reality (AR) is evolving to become the next frontier in social media, merging physical and virtual reality into a living metaverse, a Social MediARverse. With this transition, we must understand how different contexts—public, semi-public, and private—affect user engagement with AR content. We address this gap in current research by conducting an online survey with 110 participants, showcasing 36 AR videos, and polling them about the content’s fit and appropriateness. Specifically, we manipulated these three spaces, two forms of dynamism (dynamic vs. static), and two dimensionalities (2D vs. 3D). Our findings reveal that dynamic AR content is generally more favorably received than static content. Additionally, users find sharing and engaging with AR content in private settings more comfortable than in others. By this, the study offers valuable insights for designing and implementing future Social MediARverses and guides industry and academia on content visualization and contextual considerations.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/limbagoDontTheyReally2025-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/limbagoDontTheyReally2025-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/limbagoDontTheyReally2025-1400.webp"></source> <img src="/assets/img/publication_preview/limbagoDontTheyReally2025.jpg?5d6075e6c025f820a6fd5cf6e4e58538" class="preview z-depth-1 rounded" width="auto" height="auto" alt="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="limbagoDontTheyReally2025" class="col-sm-8"> <div class="title"><a href="/publications/limbagoDontTheyReally2025">Don’t They Really Hear Us? A Design Space for Private Conversations in Social Virtual Reality</a></div> <div class="author"> Josephus Jasper Limbago, <a href="https://human-ai-interaction.com/" rel="external nofollow noopener" target="_blank">Robin Welsch</a>, <em>Florian Müller</em>, and Mario Di Francesco</div> <div class="periodical"> <em>IEEE Transactions on Visualization and Computer Graphics</em>, Jul 2025 </div> <div class="periodical"> </div> <div class="links"> <a href="/assets/pdf/limbagoDontTheyReally2025.pdf" class="btn btn-sm z-depth-0" role="button"><i class="fa-solid fa-file-pdf"></i> PDF</a> <a href="https://dx.doi.org/10.1109/TVCG.2025.3549844" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="ai ai-ieee"> IEEE</i></a> </div> <div class="abstract hidden"> <p>Seamless transition between public dialogue and private talks is essential in everyday conversations. Social Virtual Reality (VR) has revolutionized interpersonal communication by creating a sense of closeness over distance through virtual avatars. However, existing social VR platforms are not successful in providing safety and supporting private conversations, thereby hindering self-disclosure and limiting the potential for meaningful experiences. We approach this problem by exploring the factors affecting private conversations in social VR applications, including the usability of different interaction methods and the awareness with respect to the virtual world. We conduct both expert interviews and a controlled experiment with a social VR prototype we realized. We then leverage the outcomes of the two studies to establish a design space that considers diverse dimensions (including privacy levels, social awareness, and modalities), laying the groundwork for more intuitive and meaningful experiences of private conversation in social VR.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/raschARYouTrack2025-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/raschARYouTrack2025-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/raschARYouTrack2025-1400.webp"></source> <img src="/assets/img/publication_preview/raschARYouTrack2025.jpg?7a02fdb1b97e3d7342a3d7c9e58a6b37" class="preview z-depth-1 rounded" width="auto" height="auto" alt="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="raschARYouTrack2025" class="col-sm-8"> <div class="title"><a href="/publications/raschARYouTrack2025">AR You on Track? Investigating Effects of Augmented Reality Anchoring on Dual-Task Performance While Walking</a></div> <div class="author"> <a href="https://julian-rasch.com/" rel="external nofollow noopener" target="_blank">Julian Rasch</a>, Matthias Wilhalm, <em>Florian Müller</em>, and <a href="https://www.francesco-chiossi-hci.com/" rel="external nofollow noopener" target="_blank">Francesco Chiossi</a> </div> <div class="periodical"> <em>In Proceedings of the 2025 CHI Conference on Human Factors in Computing Systems - CHI ’25</em>, Jul 2025 </div> <div class="periodical"> </div> <div class="links"> <a href="/assets/pdf/raschARYouTrack2025.pdf" class="btn btn-sm z-depth-0" role="button"><i class="fa-solid fa-file-pdf"></i> PDF</a> <a href="https://dx.doi.org/10.1145/3706598.3714258" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="ai ai-acmdl"> ACM</i></a> <a href="http://arxiv.org/abs/2502.20944" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="ai ai-arxiv"></i> arxiv</a> </div> <div class="abstract hidden"> <p>With the increasing spread of AR head-mounted displays suitable for everyday use, interaction with information becomes ubiquitous, even while walking. However, this requires constant shifts of our attention between walking and interacting with virtual information to fulfill both tasks adequately. Accordingly, we as a community need a thorough understanding of the mutual influences of walking and interacting with digital information to design safe yet effective interactions. Thus, we systematically investigate the effects of different AR anchors (hand, head, torso) and task difficulties on user experience and performance. We engage participants (n=26) in a dual-task paradigm involving a visual working memory task while walking. We assess the impact of dual-tasking on both virtual and walking performance, and subjective evaluations of mental and physical load. Our results show that head-anchored AR content least affected walking while allowing for fast and accurate virtual task interaction, while hand-anchored content increased reaction times and workload.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/raschCreepyCoCreatorInvestigatingAI2025-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/raschCreepyCoCreatorInvestigatingAI2025-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/raschCreepyCoCreatorInvestigatingAI2025-1400.webp"></source> <img src="/assets/img/publication_preview/raschCreepyCoCreatorInvestigatingAI2025.jpg?e619f60cb6381277d63b6b200fe2ae31" class="preview z-depth-1 rounded" width="auto" height="auto" alt="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="raschCreepyCoCreatorInvestigatingAI2025" class="col-sm-8"> <div class="title"><a href="/publications/raschCreepyCoCreatorInvestigatingAI2025">CreepyCoCreator? Investigating AI Representation Modes for 3D Object Co-Creation in Virtual Reality</a></div> <div class="author"> <a href="https://julian-rasch.com/" rel="external nofollow noopener" target="_blank">Julian Rasch</a>, Julia Töws, Teresa Hirzle, <em>Florian Müller</em>, and <a href="https://mschmitz.org/" rel="external nofollow noopener" target="_blank">Martin Schmitz</a> </div> <div class="periodical"> <em>In Proceedings of the 2025 CHI Conference on Human Factors in Computing Systems - CHI ’25</em>, Jul 2025 </div> <div class="periodical"> </div> <div class="links"> <a href="/assets/pdf/raschCreepyCoCreatorInvestigatingAI2025.pdf" class="btn btn-sm z-depth-0" role="button"><i class="fa-solid fa-file-pdf"></i> PDF</a> <a href="https://dx.doi.org/10.1145/3706598.3713720" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="ai ai-acmdl"> ACM</i></a> <a href="http://arxiv.org/abs/2502.03069" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="ai ai-arxiv"></i> arxiv</a> </div> <div class="abstract hidden"> <p>Generative AI in Virtual Reality offers the potential for collaborative object-building, yet challenges remain in aligning AI contributions with user expectations. In particular, users often struggle to understand and collaborate with AI when its actions are not transparently represented. This paper thus explores the co-creative object-building process through a Wizard-of-Oz study, focusing on how AI can effectively convey its intent to users during object customization in Virtual Reality. Inspired by human-to-human collaboration, we focus on three representation modes: the presence of an embodied avatar, whether the AI’s contributions are visualized immediately or incrementally, and whether the areas modified are highlighted in advance. The findings provide insights into how these factors affect user perception and interaction with object-generating AI tools in Virtual Reality as well as satisfaction and ownership of the created objects. The results offer design implications for co-creative world-building systems, aiming to foster more effective and satisfying collaborations between humans and AI in Virtual Reality.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/raschVisionAIDrivenAdaptation2025-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/raschVisionAIDrivenAdaptation2025-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/raschVisionAIDrivenAdaptation2025-1400.webp"></source> <img src="/assets/img/publication_preview/raschVisionAIDrivenAdaptation2025.jpg?6079be780ebb2d093b477a7c9927fb59" class="preview z-depth-1 rounded" width="auto" height="auto" alt="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="raschVisionAIDrivenAdaptation2025" class="col-sm-8"> <div class="title"><a href="/publications/raschVisionAIDrivenAdaptation2025">A Vision for AI-Driven Adaptation of Dynamic AR Content to Users and Environments</a></div> <div class="author"> <a href="https://julian-rasch.com/" rel="external nofollow noopener" target="_blank">Julian Rasch</a>, <em>Florian Müller</em>, and <a href="https://www.francesco-chiossi-hci.com/" rel="external nofollow noopener" target="_blank">Francesco Chiossi</a> </div> <div class="periodical"> Apr 2025 </div> <div class="periodical"> </div> <div class="links"> <a href="/assets/pdf/raschVisionAIDrivenAdaptation2025.pdf" class="btn btn-sm z-depth-0" role="button"><i class="fa-solid fa-file-pdf"></i> PDF</a> <a href="http://arxiv.org/abs/2504.16562" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="ai ai-arxiv"></i> arxiv</a> </div> <div class="abstract hidden"> <p>Augmented Reality (AR) is transforming the way we interact with virtual information in the physical world. By overlaying digital content in real-world environments, AR enables new forms of immersive and engaging experiences. However, existing AR systems often struggle to effectively manage the many interactive possibilities that AR presents. This vision paper speculates on AI-driven approaches for adaptive AR content placement, dynamically adjusting to user movement and environmental changes. By leveraging machine learning methods, such a system would intelligently manage content distribution between AR projections integrated into the external environment and fixed static content, enabling seamless UI layout and potentially reducing users’ cognitive load. By exploring the possibilities of AI-driven dynamic AR content placement, we aim to envision new opportunities for innovation and improvement in various industries, from urban navigation and workplace productivity to immersive learning and beyond. This paper outlines a vision for the development of more intuitive, engaging, and effective AI-powered AR experiences.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/sayffaerthExpertsEyesExploring2025-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/sayffaerthExpertsEyesExploring2025-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/sayffaerthExpertsEyesExploring2025-1400.webp"></source> <img src="/assets/img/publication_preview/sayffaerthExpertsEyesExploring2025.jpg?d61e0af2381f4f3aa6c51c97f3bd6ebe" class="preview z-depth-1 rounded" width="auto" height="auto" alt="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="sayffaerthExpertsEyesExploring2025" class="col-sm-8"> <div class="title"><a href="/publications/sayffaerthExpertsEyesExploring2025">Through the Expert’s Eyes: Exploring Asynchronous Expert Perspectives and Gaze Visualizations in XR</a></div> <div class="author"> Clara Sayffaerth, Annika Köhler, <a href="https://julian-rasch.com/" rel="external nofollow noopener" target="_blank">Julian Rasch</a>, Albrecht Schmidt, and <em>Florian Müller</em> </div> <div class="periodical"> Aug 2025 </div> <div class="periodical"> </div> <div class="links"> <a href="/assets/pdf/sayffaerthExpertsEyesExploring2025.pdf" class="btn btn-sm z-depth-0" role="button"><i class="fa-solid fa-file-pdf"></i> PDF</a> <a href="http://arxiv.org/abs/2509.00944" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="ai ai-arxiv"></i> arxiv</a> </div> <div class="abstract hidden"> <p>Transferring knowledge across generations is fundamental to human civilization, yet the challenge of passing on complex practical skills persists. Methods without a physically present instructor, such as videos, often fail to explain complex manual tasks, where spatial and social factors are critical. Technologies such as eXtended Reality and Artificial Intelligence hold the potential to retain expert knowledge and facilitate the creation of tailored, contextualized, and asynchronous explanations regardless of time and place. In contrast to videos, the learner’s perspective can be different from the recorded perspective in XR. This paper investigates the impact of asynchronous first- and third-person perspectives and gaze visualizations on efficiency, feeling of embodiment, and connectedness during manual tasks. The empirical results of our study (N=36) show that the first-person perspective is better in quantitative measures and preferred by users. We identify best practices for presenting preserved knowledge and provide guidelines for designing future systems.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/schonPegsPixelsComparative2025-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/schonPegsPixelsComparative2025-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/schonPegsPixelsComparative2025-1400.webp"></source> <img src="/assets/img/publication_preview/schonPegsPixelsComparative2025.jpg?b5376fd0c2d1dd08eb2e0ef74d240c10" class="preview z-depth-1 rounded" width="auto" height="auto" alt="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="schonPegsPixelsComparative2025" class="col-sm-8"> <div class="title"><a href="/publications/schonPegsPixelsComparative2025">From Pegs to Pixels: A Comparative Analysis of the Nine Hole Peg Test and a Digital Copy Drawing Test for Fine Motor Control Assessment</a></div> <div class="author"> Dominik Schön, <a href="https://thomaskosch.com/" rel="external nofollow noopener" target="_blank">Thomas Kosch</a>, <a href="https://mschmitz.org/" rel="external nofollow noopener" target="_blank">Martin Schmitz</a>, <a href="https://www.sebastian-guenther.com/" rel="external nofollow noopener" target="_blank">Sebastian Günther</a>, <a href="https://www.informatik.tu-darmstadt.de/telekooperation/staff_tk/tk_staff_details_23168.en.jsp" rel="external nofollow noopener" target="_blank">Max Mühlhäuser</a>, and <em>Florian Müller</em> </div> <div class="periodical"> <em>Proc. ACM Hum.-Comput. Interact.</em>, Sep 2025 </div> <div class="periodical"> </div> <div class="links"> <a href="/assets/pdf/schonPegsPixelsComparative2025.pdf" class="btn btn-sm z-depth-0" role="button"><i class="fa-solid fa-file-pdf"></i> PDF</a> <a href="https://dx.doi.org/10.1145/3743714" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="ai ai-acmdl"> ACM</i></a> <a href="entry.supps" class="btn btn-sm z-depth-0" role="button"><i class="ai ai-open-data"></i> data</a> </div> <div class="abstract hidden"> <p>User interaction with digital systems requires Fine Motor Control (FMC), especially if the interfaces are complex or require high fidelity and fine-grained interactions. Despite its importance, Fine Motor Control is often overlooked in interactive system design, partly because of its complex assessment. Measuring changes in fine motor abilities due to prolonged use or fatigue currently requires repeated manual testing. This paper analyzes the concept of using the digital mobile devices’ input behavior to assess the user’s Fine Motor Control. For this, we show that Fine Motor Control can be assessed for touch and stylus-based interaction with a digital mobile system. We conducted a user study, where participants performed a Nine Hole Peg Test and a predefined Copy Drawing Test before and after exercises that affect fine motor skills. Based on this data, we investigated how metrics such as pressure, velocity, and entropy for touch and stylus input can be used to predict Fine Motor Control.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/weingartnerAssessingVisualizationInteraction2025-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/weingartnerAssessingVisualizationInteraction2025-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/weingartnerAssessingVisualizationInteraction2025-1400.webp"></source> <img src="/assets/img/publication_preview/weingartnerAssessingVisualizationInteraction2025.jpg?fcafbacfcfa8f4429be4afbdfc6a4ae5" class="preview z-depth-1 rounded" width="auto" height="auto" alt="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="weingartnerAssessingVisualizationInteraction2025" class="col-sm-8"> <div class="title"><a href="/publications/weingartnerAssessingVisualizationInteraction2025">Assessing Visualization and Interaction Techniques to Support Comparison Tasks in Virtual Reality</a></div> <div class="author"> Henrike Weingärtner, <a href="https://julian-rasch.com/" rel="external nofollow noopener" target="_blank">Julian Rasch</a>, Nils Rothamel, and <em>Florian Müller</em> </div> <div class="periodical"> <em>In Proceedings of the Extended Abstracts of the CHI Conference on Human Factors in Computing Systems</em>, Apr 2025 </div> <div class="periodical"> </div> <div class="links"> <a href="/assets/pdf/weingartnerAssessingVisualizationInteraction2025.pdf" class="btn btn-sm z-depth-0" role="button"><i class="fa-solid fa-file-pdf"></i> PDF</a> <a href="https://dx.doi.org/10.1145/3706599.3719737" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="ai ai-acmdl"> ACM</i></a> </div> <div class="abstract hidden"> <p>Desktop screens are effective for supporting comparison tasks, but as the scale increases to room-sized or larger structures, context is lost. Users are forced to focus on isolated details through panning, zooming, and scrolling, making it difficult to maintain an overview while exploring finer details. Virtual Reality (VR) potentially offers a solution to this problem by immersing users in 3D spaces and enabling more intuitive comparisons. While related work has proposed many solutions for visualizing and interacting for comparison tasks in desktop environments, knowledge regarding the efficacy of supporting such tasks in VR environments is still lacking. We investigated varying visualization and interaction techniques in a controlled experiment with 24 participants. Our findings provide valuable insights for designing VR systems that improve usability, reduce workload, and enhance performance in comparison tasks.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/wuOneDoesNot2025-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/wuOneDoesNot2025-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/wuOneDoesNot2025-1400.webp"></source> <img src="/assets/img/publication_preview/wuOneDoesNot2025.jpg?095b75b4928365c4ba1100a39d471cc8" class="preview z-depth-1 rounded" width="auto" height="auto" alt="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="wuOneDoesNot2025" class="col-sm-8"> <div class="title"> <i class="fa-solid fa-award" style="color: var(--global-theme-color);"></i> <a href="/publications/wuOneDoesNot2025">One Does Not Simply Meme Alone: Evaluating Co-Creativity Between LLMs and Humans in the Generation of Humor</a> </div> <div class="author"> Zhikun Wu, Thomas Weber, and <em>Florian Müller</em> </div> <div class="periodical"> <em>In Proceedings of the 30th International Conference on Intelligent User Interfaces</em>, Mar 2025 </div> <div class="periodical"> </div> <div class="links"> <a href="/assets/pdf/wuOneDoesNot2025.pdf" class="btn btn-sm z-depth-0" role="button"><i class="fa-solid fa-file-pdf"></i> PDF</a> <a href="https://dx.doi.org/10.1145/3708359.3712094" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="ai ai-acmdl"> ACM</i></a> <a href="http://arxiv.org/abs/2501.11433" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="ai ai-arxiv"></i> arxiv</a> </div> <div class="abstract hidden"> <p>Collaboration has been shown to enhance creativity, leading to more innovative and effective outcomes. While previous research has explored the abilities of Large Language Models (LLMs) to serve as co-creative partners in tasks like writing poetry or creating narratives, the collaborative potential of LLMs in humor-rich and culturally nuanced domains remains an open question. To address this gap, we conducted a user study to explore the potential of LLMs in co-creating memes—a humor-driven and culturally specific form of creative expression. We conducted a user study with three groups of 50 participants each: a human-only group creating memes without AI assistance, a human-AI collaboration group interacting with a state-of-the-art LLM model, and an AI-only group where the LLM autonomously generated memes. We assessed the quality of the generated memes through crowdsourcing, with each meme rated on creativity, humor, and shareability. Our results showed that LLM assistance increased the number of ideas generated and reduced the effort participants felt. However, it did not improve the quality of the memes when humans were collaborated with LLM. Interestingly, memes created entirely by AI performed better than both human-only and human-AI collaborative memes in all areas on average. However, when looking at the top-performing memes, human-created ones were better in humor, while human-AI collaborations stood out in creativity and shareability. These findings highlight the complexities of human-AI collaboration in creative tasks. While AI can boost productivity and create content that appeals to a broad audience, human creativity remains crucial for content that connects on a deeper level.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/zyskaPullRequestsClassroom2025-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/zyskaPullRequestsClassroom2025-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/zyskaPullRequestsClassroom2025-1400.webp"></source> <img src="/assets/img/publication_preview/zyskaPullRequestsClassroom2025.jpg?4ae7af8d1793afca648eca0d566e82fe" class="preview z-depth-1 rounded" width="auto" height="auto" alt="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="zyskaPullRequestsClassroom2025" class="col-sm-8"> <div class="title"><a href="/publications/zyskaPullRequestsClassroom2025">Pull Requests From The Classroom: Co-Developing Curriculum And Code</a></div> <div class="author"> <a href="http://zyska.org/" rel="external nofollow noopener" target="_blank">Dennis Zyska</a>, Ilia Kuznetsov, <em>Florian Müller</em>, and Iryna Gurevych</div> <div class="periodical"> <em>In Proceedings of Mensch Und Computer 2025 - MuC ’25</em>, Aug 2025 </div> <div class="periodical"> </div> <div class="links"> <a href="/assets/pdf/zyskaPullRequestsClassroom2025.pdf" class="btn btn-sm z-depth-0" role="button"><i class="fa-solid fa-file-pdf"></i> PDF</a> <a href="https://dx.doi.org/10.1145/3743049.3748581" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="ai ai-acmdl"> ACM</i></a> <a href="http://arxiv.org/abs/2508.00646" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="ai ai-arxiv"></i> arxiv</a> </div> <div class="abstract hidden"> <p>Educational technologies often misalign with instructors’ pedagogical goals, forcing adaptations that compromise teaching efficacy. In this paper, we present a case study on the co-development of curriculum and technology in the context of a university course on scientific writing. Specifically, we examine how a custom-built peer feedback system was iteratively developed alongside the course to support annotation, feedback exchange, and revision. Results show that while co-development fostered stronger alignment between software features and course goals, it also exposed usability limitations and infrastructure-related frustrations, emphasizing the need for closer coordination between teaching and technical teams.</p> </div> </div> </div> </li> </ol> <h2 class="bibliography">2024</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/dietzEmbracerWearableEncounteredType2024-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/dietzEmbracerWearableEncounteredType2024-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/dietzEmbracerWearableEncounteredType2024-1400.webp"></source> <img src="/assets/img/publication_preview/dietzEmbracerWearableEncounteredType2024.jpg?e235db76e648820a84b60847038457a5" class="preview z-depth-1 rounded" width="auto" height="auto" alt="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="dietzEmbracerWearableEncounteredType2024" class="col-sm-8"> <div class="title"><a href="/publications/dietzEmbracerWearableEncounteredType2024">Embracer: A Wearable Encountered-Type Haptic Controller for 3 DoF Input and Feedback</a></div> <div class="author"> Dennis Dietz, <a href="https://posthci.com/" rel="external nofollow noopener" target="_blank">Steeven Villa</a>, Moritz Ziarko, Michael Bonfert, <em>Florian Müller</em>, and Andreas Butz</div> <div class="periodical"> <em>In Proceedings of the 2024 ACM International Symposium on Wearable Computers</em>, Oct 2024 </div> <div class="periodical"> </div> <div class="links"> <a href="/assets/pdf/dietzEmbracerWearableEncounteredType2024.pdf" class="btn btn-sm z-depth-0" role="button"><i class="fa-solid fa-file-pdf"></i> PDF</a> <a href="https://dx.doi.org/10.1145/3675095.3676626" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="ai ai-acmdl"> ACM</i></a> </div> <div class="abstract hidden"> <p>The lack of haptic sensations beyond very simple vibration feedback diminishes the feeling of presence in Virtual Reality. Research suggested various approaches to deliver haptic sensations to the user’s palm. However, these approaches are typically limited in the number of actuation directions and only focus on enhancing the system’s output, ignoring haptic input. We present Embracer, a wrist-mounted encountered-type haptic controller that addresses these gaps by rendering forces along three axes through a sphere-shaped end effector within the user’s palm. Using modified servo motors, we sense user-performed manipulations of the end effector as an input modality. In this paper, we contribute the design and implementation of Embracer together with a preliminary technical evaluation. By providing a more comprehensive haptic feedback system, Embracer enhances the realism and immersion of haptic feedback and user control.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/markyDecideYourselfDelegate2024-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/markyDecideYourselfDelegate2024-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/markyDecideYourselfDelegate2024-1400.webp"></source> <img src="/assets/img/publication_preview/markyDecideYourselfDelegate2024.jpg?4043a822a4f594a5a55c93933051c8bb" class="preview z-depth-1 rounded" width="auto" height="auto" alt="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="markyDecideYourselfDelegate2024" class="col-sm-8"> <div class="title"><a href="/publications/markyDecideYourselfDelegate2024">Decide Yourself or Delegate - User Preferences Regarding the Autonomy of Personal Privacy Assistants in Private IoT-Equipped Environments</a></div> <div class="author"> <a href="https://informatik.rub.de/digisoul/personen/marky/" rel="external nofollow noopener" target="_blank">Karola Marky</a>, Alina Stöver, Sarah Prange, Kira Bleck, Paul Gerber, Verena Zimmermann, <em>Florian Müller</em>, Florian Alt, and <a href="https://www.informatik.tu-darmstadt.de/telekooperation/staff_tk/tk_staff_details_23168.en.jsp" rel="external nofollow noopener" target="_blank">Max Mühlhäuser</a> </div> <div class="periodical"> <em>In Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems</em>, May 2024 </div> <div class="periodical"> </div> <div class="links"> <a href="/assets/pdf/markyDecideYourselfDelegate2024.pdf" class="btn btn-sm z-depth-0" role="button"><i class="fa-solid fa-file-pdf"></i> PDF</a> <a href="https://dx.doi.org/10.1145/3613904.3642591" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="ai ai-acmdl"> ACM</i></a> <a href="https://www.youtube.com/watch?v=-E3HUB7onCs" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="fa-solid fa-person-chalkboard"></i> Talk</a> </div> <div class="abstract hidden"> <p>Personalized privacy assistants (PPAs) communicate privacy-related decisions of their users to Internet of Things (IoT) devices. There are different ways to implement PPAs by varying the degree of autonomy or decision model. This paper investigates user perceptions of PPA autonomy models and privacy profiles – archetypes of individual privacy needs – as a basis for PPA decisions in private environments (e.g., a friend’s home). We first explore how privacy profiles can be assigned to users and propose an assignment method. Next, we investigate user perceptions in 18 usage scenarios with varying contexts, data types and number of decisions in a study with 1126 participants. We found considerable differences between the profiles in settings with few decisions. If the number of decisions gets high (&gt; 1/h), participants exclusively preferred fully autonomous PPAs. Finally, we discuss implications and recommendations for designing scalable PPAs that serve as privacy interfaces for future IoT devices.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/raschJustUndoIt2024-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/raschJustUndoIt2024-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/raschJustUndoIt2024-1400.webp"></source> <img src="/assets/img/publication_preview/raschJustUndoIt2024.jpg?c01e9d55e56d30fee90a54395b1c0b6c" class="preview z-depth-1 rounded" width="auto" height="auto" alt="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="raschJustUndoIt2024" class="col-sm-8"> <div class="title"><a href="/publications/raschJustUndoIt2024">Just Undo It: Exploring Undo Mechanics in Multi-User Virtual Reality</a></div> <div class="author"> <a href="https://julian-rasch.com/" rel="external nofollow noopener" target="_blank">Julian Rasch</a>, Florian Perzl, <a href="https://yannick-weiss.com/" rel="external nofollow noopener" target="_blank">Yannick Weiss</a>, and <em>Florian Müller</em> </div> <div class="periodical"> <em>In Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems</em>, Mar 2024 </div> <div class="periodical"> </div> <div class="links"> <a href="/assets/pdf/raschJustUndoIt2024.pdf" class="btn btn-sm z-depth-0" role="button"><i class="fa-solid fa-file-pdf"></i> PDF</a> <a href="https://dx.doi.org/10.1145/3613904.3642864" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="ai ai-acmdl"> ACM</i></a> <a href="http://arxiv.org/abs/2403.11756" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="ai ai-arxiv"></i> arxiv</a> <a href="https://www.youtube.com/watch?v=6g5sEdy-UFc" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-youtube"></i> Video</a> <a href="https://www.youtube.com/watch?v=COExWxXxy98" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="fa-solid fa-person-chalkboard"></i> Talk</a> </div> <div class="abstract hidden"> <p>With the proliferation of VR and a metaverse on the horizon, many multi-user activities are migrating to the VR world, calling for effective collaboration support. As one key feature, traditional collaborative systems provide users with undo mechanics to reverse errors and other unwanted changes. While undo has been extensively researched in this domain and is now considered industry standard, it is strikingly absent for VR systems in research and industry. This work addresses this research gap by exploring different undo techniques for basic object manipulation in different collaboration modes in VR. We conducted a study involving 32 participants organized in teams of two. Here, we studied users’ performance and preferences in a tower stacking task, varying the available undo techniques and their mode of collaboration. The results suggest that users desire and use undo in VR and that the choice of the undo technique impacts users’ performance and social connection.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/sayffaerthTeleMeMore2024-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/sayffaerthTeleMeMore2024-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/sayffaerthTeleMeMore2024-1400.webp"></source> <img src="/assets/img/publication_preview/sayffaerthTeleMeMore2024.jpg?2dc0cf1823a1b46aa89df0460788070a" class="preview z-depth-1 rounded" width="auto" height="auto" alt="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="sayffaerthTeleMeMore2024" class="col-sm-8"> <div class="title"><a href="/publications/sayffaerthTeleMeMore2024">“Tele” Me More: Using Telepresence Charades to Connect Strangers and Exhibits in Different Museums</a></div> <div class="author"> Clara Sayffaerth, <a href="https://julian-rasch.com/" rel="external nofollow noopener" target="_blank">Julian Rasch</a>, and <em>Florian Müller</em> </div> <div class="periodical"> <em>In Extended Abstracts of the CHI Conference on Human Factors in Computing Systems</em>, May 2024 </div> <div class="periodical"> </div> <div class="links"> <a href="/assets/pdf/sayffaerthTeleMeMore2024.pdf" class="btn btn-sm z-depth-0" role="button"><i class="fa-solid fa-file-pdf"></i> PDF</a> <a href="https://dx.doi.org/10.1145/3613905.3650834" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="ai ai-acmdl"> ACM</i></a> <a href="https://www.youtube.com/watch?v=3zdhw1hreNo" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-youtube"></i> Video</a> <a href="https://www.youtube.com/watch?v=nZFJSmvWjNk" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="fa-solid fa-person-chalkboard"></i> Talk</a> </div> <div class="abstract hidden"> <p>The museum is changing from a place of passive consumption to a place of interactive experiences, opening up new ways of engaging with exhibits and others. As a promising direction, this paper explores the potential of telepresence stations in the museum context to enhance social connectedness among visitors over distance. Emphasizing the significance of social exchange, our research focuses on studying telepresence to foster interactions between strangers, share knowledge, and promote social connectedness. To do so, we first observe exhibitions and then interview individual visitors of a technical museum about their experiences and needs. Based on the results, we design appropriate voiceless and touchless communication channels and test them in a study. The findings of our in-situ user study with 24 visitors unfamiliar with each other in the museum provide insights into behaviors and perceptions, contributing valuable knowledge on seamlessly integrating telepresence technology in exhibitions, with a focus on enhancing learning, social connections, and the museum experience in general.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/schoenDecodingFatigueAnalyzing2024-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/schoenDecodingFatigueAnalyzing2024-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/schoenDecodingFatigueAnalyzing2024-1400.webp"></source> <img src="/assets/img/publication_preview/schoenDecodingFatigueAnalyzing2024.jpg?681152ee9ebe2e4d87be5041244e828e" class="preview z-depth-1 rounded" width="auto" height="auto" alt="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="schoenDecodingFatigueAnalyzing2024" class="col-sm-8"> <div class="title"><a href="/publications/schoenDecodingFatigueAnalyzing2024">Decoding Fatigue: Analyzing Offline Handwriting with Machine Learning to Detect Perceived Exhaustion</a></div> <div class="author"> Dominik Schoen, <a href="https://thomaskosch.com/" rel="external nofollow noopener" target="_blank">Thomas Kosch</a>, Till Becker, Godfred Antwi-Boasiako, Merret Jung, Ana Laura Chioca Vieira, <a href="https://www.informatik.tu-darmstadt.de/telekooperation/staff_tk/tk_staff_details_23168.en.jsp" rel="external nofollow noopener" target="_blank">Max Mühlhäuser</a>, and <em>Florian Müller</em> </div> <div class="periodical"> <em>In Proceedings of the International Conference on Mobile and Ubiquitous Multimedia</em>, Dec 2024 </div> <div class="periodical"> </div> <div class="links"> <a href="/assets/pdf/schoenDecodingFatigueAnalyzing2024.pdf" class="btn btn-sm z-depth-0" role="button"><i class="fa-solid fa-file-pdf"></i> PDF</a> <a href="https://dx.doi.org/10.1145/3701571.3703393" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="ai ai-acmdl"> ACM</i></a> </div> <div class="abstract hidden"> <p>The quality and readability of an individual’s handwriting and drawing can be influenced by various factors, including their level of physical exertion. This enables us to explore the quantification of exertion by observing an individual’s handwriting. To test this hypothesis, we collected data from 17 participants, building a database of handwriting and drawing samples and their corresponding Borg 10 exertion ratings at the time of drawing. In this paper, we investigate using machine learning techniques to estimate perceived exertion before, during, and after physical activity based on handwriting and drawings. We apply a regression model to compare different drawing tasks and demonstrate that perceived exertion can be predicted using simple line drawings. However, more complex sketches and handwriting demand further research. Our findings suggest that interactive systems could use handwriting and drawing to intervene when users experience excessive discomfort.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/villaTouchItIts2024-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/villaTouchItIts2024-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/villaTouchItIts2024-1400.webp"></source> <img src="/assets/img/publication_preview/villaTouchItIts2024.jpg?dda13222bfc24b7ba825b8a6f5f30d69" class="preview z-depth-1 rounded" width="auto" height="auto" alt="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="villaTouchItIts2024" class="col-sm-8"> <div class="title"><a href="/publications/villaTouchItIts2024">Touch It Like It’s Hot: A Thermal Feedback Enabled Encountered-type Haptic Display for Virtual Reality</a></div> <div class="author"> <a href="https://posthci.com/" rel="external nofollow noopener" target="_blank">Steeven Villa</a>, Kenji Ishihara, Moritz Ziarko, <a href="https://www.sebastian-guenther.com/" rel="external nofollow noopener" target="_blank">Sebastian Günther</a>, and <em>Florian Müller</em> </div> <div class="periodical"> <em>In IEEE International Symposium on Mixed and Augmented Reality (ISMAR’24) 2024</em>, Dec 2024 </div> <div class="periodical"> </div> <div class="links"> <a href="/assets/pdf/villaTouchItIts2024.pdf" class="btn btn-sm z-depth-0" role="button"><i class="fa-solid fa-file-pdf"></i> PDF</a> <a href="https://dx.doi.org/10.1109/ISMAR62088.2024.00085" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="ai ai-ieee"> IEEE</i></a> <a href="https://www.youtube.com/watch?v=au-k_FlbuWU&amp;t=2736" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="fa-solid fa-person-chalkboard"></i> Talk</a> </div> <div class="abstract hidden"> <p>In recent years, the community has presented various novel solutions to address the lack of haptic feedback in virtual reality experiences. Yet, it remains a major challenge for Virtual Reality applications. Encountered-type Haptic Displays (ETHDs) have emerged as a promising alternative to enable haptic feedback in VR without requiring the user to wear any device while allowing for sensorily rich experiences such as texture, kinaesthetic feedback, and even ultrasonic tactile feedback. Nevertheless, as important as thermal feedback is for daily life interactions, such as assessing the temperature of a mug or knowing if the microwave is on, thermal feedback in ETHD has remained largely unexplored. In this paper, we present a novel ETHD that provides thermal feedback and explore its potential in VR. We describe the design of our ETHD, and we report the results of a user study that compares different thermal feedback settings in VR. Our results show that thermal feedback can significantly enhance the user immersion and haptic experience in VR, and we discuss the implications of our findings for the design of ETHD and VR experiences.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/weissExploringRedirectionShifting2024-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/weissExploringRedirectionShifting2024-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/weissExploringRedirectionShifting2024-1400.webp"></source> <img src="/assets/img/publication_preview/weissExploringRedirectionShifting2024.jpg?becb95e01a9ce82a3d5c73d6951564c7" class="preview z-depth-1 rounded" width="auto" height="auto" alt="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="weissExploringRedirectionShifting2024" class="col-sm-8"> <div class="title"><a href="/publications/weissExploringRedirectionShifting2024">Exploring Redirection and Shifting Techniques to Mask Hand Movements from Shoulder-Surfing Attacks during PIN Authentication in Virtual Reality</a></div> <div class="author"> <a href="https://yannick-weiss.com/" rel="external nofollow noopener" target="_blank">Yannick Weiss</a>, <a href="https://posthci.com/" rel="external nofollow noopener" target="_blank">Steeven Villa</a>, Jesse W Grootjen, Matthias Hoppe, Yasin Kale, and <em>Florian Müller</em> </div> <div class="periodical"> <em>Proc. ACM Hum.-Comput. Interact.</em>, Sep 2024 </div> <div class="periodical"> </div> <div class="links"> <a href="/assets/pdf/weissExploringRedirectionShifting2024.pdf" class="btn btn-sm z-depth-0" role="button"><i class="fa-solid fa-file-pdf"></i> PDF</a> <a href="https://dx.doi.org/10.1145/3676502" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="ai ai-acmdl"> ACM</i></a> </div> <div class="abstract hidden"> <p>The proliferation of mobile Virtual Reality (VR) headsets shifts our interaction with virtual worlds beyond our living rooms into shared spaces. Consequently, we are entrusting more and more personal data to these devices, calling for strong security measures and authentication. However, the standard authentication method of such devices - entering PINs via virtual keyboards - is vulnerable to shoulder-surfing, as movements to enter keys can be monitored by an unnoticed observer. To address this, we evaluated masking techniques to obscure VR users’ input during PIN authentication by diverting their hand movements. Through two experimental studies, we demonstrate that these methods increase users’ security against shoulder-surfing attacks from observers without excessively impacting their experience and performance. With these discoveries, we aim to enhance the security of future VR authentication without disrupting the virtual experience or necessitating additional hardware or training of users.</p> </div> </div> </div> </li> </ol> <h2 class="bibliography">2023</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/bartkowskiSyncExploringSynchronization2023a-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/bartkowskiSyncExploringSynchronization2023a-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/bartkowskiSyncExploringSynchronization2023a-1400.webp"></source> <img src="/assets/img/publication_preview/bartkowskiSyncExploringSynchronization2023a.jpg?3dc05777716bdaf8c8f29af0c7d4854e" class="preview z-depth-1 rounded" width="auto" height="auto" alt="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="bartkowskiSyncExploringSynchronization2023a" class="col-sm-8"> <div class="title"><a href="/publications/bartkowskiSyncExploringSynchronization2023a">In Sync: Exploring Synchronization to Increase Trust Between Humans and Non-humanoid Robots</a></div> <div class="author"> Wieslaw Bartkowski, Andrzej Nowak, Filip Ignacy Czajkowski, Albrecht Schmidt, and <em>Florian Müller</em> </div> <div class="periodical"> <em>In Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems</em>, Apr 2023 </div> <div class="periodical"> </div> <div class="links"> <a href="/assets/pdf/bartkowskiSyncExploringSynchronization2023a.pdf" class="btn btn-sm z-depth-0" role="button"><i class="fa-solid fa-file-pdf"></i> PDF</a> <a href="https://dx.doi.org/10.1145/3544548.3581193" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="ai ai-acmdl"> ACM</i></a> <a href="http://arxiv.org/abs/2303.15917" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="ai ai-arxiv"></i> arxiv</a> <a href="https://www.youtube.com/watch?v=kdEB9L4OCgk" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-youtube"></i> Video</a> <a href="https://www.youtube.com/watch?v=dEYNV4KaBR8" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="fa-solid fa-person-chalkboard"></i> Talk</a> </div> <div class="abstract hidden"> <p>When we go for a walk with friends, we can observe an interesting effect: From step lengths to arm movements - our movements unconsciously align; they synchronize. Prior research found that this synchronization is a crucial aspect of human relations that strengthens social cohesion and trust. Generalizing from these findings in synchronization theory, we propose a dynamical approach that can be applied in the design of non-humanoid robots to increase trust. We contribute the results of a controlled experiment with 51 participants exploring our concept in a between-subjects design. For this, we built a prototype of a simple non-humanoid robot that can bend to follow human movements and vary the movement synchronization patterns. We found that synchronized movements lead to significantly higher ratings in an established questionnaire on trust between people and automation but did not influence the willingness to spend money in a trust game.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/desoldaDigitalModelingEveryone2023-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/desoldaDigitalModelingEveryone2023-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/desoldaDigitalModelingEveryone2023-1400.webp"></source> <img src="/assets/img/publication_preview/desoldaDigitalModelingEveryone2023.jpg?b8ddaea51f767d0f052ded1120acb9d4" class="preview z-depth-1 rounded" width="auto" height="auto" alt="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="desoldaDigitalModelingEveryone2023" class="col-sm-8"> <div class="title"><a href="/publications/desoldaDigitalModelingEveryone2023">Digital Modeling for Everyone: Exploring How Novices Approach Voice-Based 3D Modeling</a></div> <div class="author"> Giuseppe Desolda, Andrea Esposito, <em>Florian Müller</em>, and <a href="https://sebastian-feger.com/" rel="external nofollow noopener" target="_blank">Sebastian Feger</a> </div> <div class="periodical"> <em>In Human-Computer Interaction – INTERACT 2023</em>, Apr 2023 </div> <div class="periodical"> </div> <div class="links"> <a href="/assets/pdf/desoldaDigitalModelingEveryone2023.pdf" class="btn btn-sm z-depth-0" role="button"><i class="fa-solid fa-file-pdf"></i> PDF</a> <a href="https://dx.doi.org/10.1007/978-3-031-42293-5_11" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="ai ai-springer"> Springer</i></a> </div> <div class="abstract hidden"> <p>Manufacturing tools like 3D printers have become accessible to the wider society, making the promise of digital fabrication for everyone seemingly reachable. While the actual manufacturing process is largely automated today, users still require knowledge of complex design applications to produce ready-designed objects and adapt them to their needs or design new objects from scratch. To lower the barrier to the design and customization of personalized 3D models, we explored novice mental models in voice-based 3D modeling by conducting a high-fidelity Wizard of Oz study with 22 participants. We performed a thematic analysis of the collected data to understand how the mental model of novices translates into voice-based 3D modeling. We conclude with design implications for voice assistants. For example, they have to: deal with vague, incomplete and wrong commands; provide a set of straightforward commands to shape simple and composite objects; and offer different strategies to select 3D objects.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/elsayedUnderstandingStationaryMoving2023-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/elsayedUnderstandingStationaryMoving2023-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/elsayedUnderstandingStationaryMoving2023-1400.webp"></source> <img src="/assets/img/publication_preview/elsayedUnderstandingStationaryMoving2023.jpg?3a6a50f77659f6e2bcb57d826b252783" class="preview z-depth-1 rounded" width="auto" height="auto" alt="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="elsayedUnderstandingStationaryMoving2023" class="col-sm-8"> <div class="title"><a href="/publications/elsayedUnderstandingStationaryMoving2023">Understanding Stationary and Moving Direct Skin Vibrotactile Stimulation on the Palm</a></div> <div class="author"> Hesham Elsayed, Martin Weigel, <em>Florian Müller</em>, George Ibrahim, Jan Gugenheimer, <a href="https://mschmitz.org/" rel="external nofollow noopener" target="_blank">Martin Schmitz</a>, <a href="https://www.sebastian-guenther.com/" rel="external nofollow noopener" target="_blank">Sebastian Günther</a>, and <a href="https://www.informatik.tu-darmstadt.de/telekooperation/staff_tk/tk_staff_details_23168.en.jsp" rel="external nofollow noopener" target="_blank">Max Mühlhäuser</a> </div> <div class="periodical"> Feb 2023 </div> <div class="periodical"> </div> <div class="links"> <a href="/assets/pdf/elsayedUnderstandingStationaryMoving2023.pdf" class="btn btn-sm z-depth-0" role="button"><i class="fa-solid fa-file-pdf"></i> PDF</a> <a href="http://arxiv.org/abs/2302.08820" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="ai ai-arxiv"></i> arxiv</a> </div> <div class="abstract hidden"> <p>Palm-based tactile displays have the potential to evolve from single motor interfaces (e.g., smartphones) to high-resolution tactile displays (e.g., back-of-device haptic interfaces) enabling richer multi-modal experiences with more information. However, we lack a systematic understanding of vibrotactile perception on the palm and the influence of various factors on the core design decisions of tactile displays (number of actuators, resolution, and intensity). In a first experiment (N=16), we investigated the effect of these factors on the users’ ability to localize stationary sensations. In a second experiment (N=20), we explored the influence of resolution on recognition rate for moving tactile sensations.Findings show that for stationary sensations a 9 actuator display offers a good trade-off and a \3}times3 resolution can be accurately localized. For moving sensations, a \2}times4 resolution led to the highest recognition accuracy, while \5}times10 enables higher resolution output with a reasonable accuracy.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/eskaThermoFeetAssessingOnFoot2023-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/eskaThermoFeetAssessingOnFoot2023-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/eskaThermoFeetAssessingOnFoot2023-1400.webp"></source> <img src="/assets/img/publication_preview/eskaThermoFeetAssessingOnFoot2023.jpg?8c84d31b5a23809edd96eca5a7d54620" class="preview z-depth-1 rounded" width="auto" height="auto" alt="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="eskaThermoFeetAssessingOnFoot2023" class="col-sm-8"> <div class="title"><a href="/publications/eskaThermoFeetAssessingOnFoot2023">ThermoFeet: Assessing On-Foot Thermal Stimuli for Directional Cues</a></div> <div class="author"> Bettina Eska, Jeff-Owens Iyalekhue, <a href="https://www.sebastian-guenther.com/" rel="external nofollow noopener" target="_blank">Sebastian Günther</a>, Jasmin Niess, and <em>Florian Müller</em> </div> <div class="periodical"> <em>In Proceedings of the 22nd International Conference on Mobile and Ubiquitous Multimedia</em>, Dec 2023 </div> <div class="periodical"> </div> <div class="links"> <a href="/assets/pdf/eskaThermoFeetAssessingOnFoot2023.pdf" class="btn btn-sm z-depth-0" role="button"><i class="fa-solid fa-file-pdf"></i> PDF</a> <a href="https://dx.doi.org/10.1145/3626705.3627974" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="ai ai-acmdl"> ACM</i></a> </div> <div class="abstract hidden"> <p>Thermal feedback has been studied for navigation purposes with directional cues and a variety of other use cases. Yet, to date, systems providing thermal feedback were primarily designed for the upper body, targeting hands and arms in particular. As these parts are often occupied with other tasks, there is a need to extend the design space of thermal feedback to other body parts. To close this gap, we assess thermal feedback on the user’s feet. This research explores if creating stimuli representing any direction on a circle with only four actuators is possible. To evaluate this concept, we conducted a user study asking the participants to indicate the perceived direction after getting a hot or cold stimulus by direct actuation using one actuator or phantom actuation using two actuators. The results indicate that the detection accuracy was higher for cold signals. In addition, the results showed higher recognition for stimuli linked to actuator distribution than phantom sensation due to spatial summation.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/hirschMyHeartWill2023-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/hirschMyHeartWill2023-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/hirschMyHeartWill2023-1400.webp"></source> <img src="/assets/img/publication_preview/hirschMyHeartWill2023.jpg?55135d1bb7fb3f79a735e016c22e22a5" class="preview z-depth-1 rounded" width="auto" height="auto" alt="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="hirschMyHeartWill2023" class="col-sm-8"> <div class="title"><a href="/publications/hirschMyHeartWill2023">My Heart Will Go On: Implicitly Increasing Social Connectedness by Visualizing Asynchronous Players’ Heartbeats in VR Games</a></div> <div class="author"> <a href="https://www.lindahirsch.de/" rel="external nofollow noopener" target="_blank">Linda Hirsch</a>, <em>Florian Müller</em>, <a href="https://www.francesco-chiossi-hci.com/" rel="external nofollow noopener" target="_blank">Francesco Chiossi</a>, Theodor Benga, and Andreas Martin Butz</div> <div class="periodical"> <em>Proceedings of the ACM on Human-Computer Interaction</em>, Oct 2023 </div> <div class="periodical"> </div> <div class="links"> <a href="/assets/pdf/hirschMyHeartWill2023.pdf" class="btn btn-sm z-depth-0" role="button"><i class="fa-solid fa-file-pdf"></i> PDF</a> <a href="https://dx.doi.org/10.1145/3611057" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="ai ai-acmdl"> ACM</i></a> <a href="https://www.youtube.com/watch?v=PuHfic0rmAs" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="fa-solid fa-person-chalkboard"></i> Talk</a> </div> <div class="abstract hidden"> <p>Social games benefit from social connectedness between players because it improves the gaming experience and increases enjoyment. In virtual reality (VR), various approaches, such as avatars, are developed for multi-player games to increase social connectedness. However, these approaches are lacking in single-player games. To increase social connectedness in such games, our work explores the visualization of physiological data from asynchronous players, i.e., electrocardiogram (ECG). We identified two visualization dimensions, the number of players, and the visualization style, after a design workshop with experts (N=4) and explored them in a single-user virtual escape room game. We spatially and temporally integrated the visualizations and compared two times two visualizations against a baseline condition without visualization in a within-subject lab study (N=34). All but one visualization significantly increased participants’ feelings of social connectedness. Heart icons triggered the strongest feeling of connectedness, understanding, and perceived support in playing the game.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/hirzleWhenXRAI2023-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/hirzleWhenXRAI2023-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/hirzleWhenXRAI2023-1400.webp"></source> <img src="/assets/img/publication_preview/hirzleWhenXRAI2023.jpg?9d688ff284b7f90408f842fe7da88226" class="preview z-depth-1 rounded" width="auto" height="auto" alt="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="hirzleWhenXRAI2023" class="col-sm-8"> <div class="title"><a href="/publications/hirzleWhenXRAI2023">When XR and AI Meet - A Scoping Review on Extended Reality and Artificial Intelligence</a></div> <div class="author"> Teresa Hirzle, <em>Florian Müller</em>, Fiona Draxler, <a href="https://mschmitz.org/" rel="external nofollow noopener" target="_blank">Martin Schmitz</a>, Pascal Knierim, and Kasper Hornbæk</div> <div class="periodical"> <em>In Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems</em>, Apr 2023 </div> <div class="periodical"> </div> <div class="links"> <a href="/assets/pdf/hirzleWhenXRAI2023.pdf" class="btn btn-sm z-depth-0" role="button"><i class="fa-solid fa-file-pdf"></i> PDF</a> <a href="https://dx.doi.org/10.1145/3544548.3581072" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="ai ai-acmdl"> ACM</i></a> <a href="https://www.youtube.com/watch?v=VDg-2Pz9lj8" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="fa-solid fa-person-chalkboard"></i> Talk</a> </div> <div class="abstract hidden"> <p>Research on Extended Reality (XR) and Artificial Intelligence (AI) is booming, which has led to an emerging body of literature in their intersection. However, the main topics in this intersection are unclear, as are the benefits of combining XR and AI. This paper presents a scoping review that highlights how XR is applied in AI research and vice versa. We screened 2619 publications from 203 international venues published between 2017 and 2021, followed by an in-depth review of 311 papers. Based on our review, we identify five main topics at the intersection of XR and AI, showing how research at the intersection can benefit each other. Furthermore, we present a list of commonly used datasets, software, libraries, and models to help researchers interested in this intersection. Finally, we present 13 research opportunities and recommendations for future work in XR and AI research.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/liLocationAwareVirtualReality2023-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/liLocationAwareVirtualReality2023-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/liLocationAwareVirtualReality2023-1400.webp"></source> <img src="/assets/img/publication_preview/liLocationAwareVirtualReality2023.jpg?48acea74fd50e7601018e616297f88a8" class="preview z-depth-1 rounded" width="auto" height="auto" alt="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="liLocationAwareVirtualReality2023" class="col-sm-8"> <div class="title"><a href="/publications/liLocationAwareVirtualReality2023">Location-Aware Virtual Reality for Situational Awareness On the Road</a></div> <div class="author"> Jingyi Li, Alexandra Mayer, <em>Florian Müller</em>, <a href="http://andriimatviienko.com/" rel="external nofollow noopener" target="_blank">Andrii Matviienko</a>, and Andreas Butz</div> <div class="periodical"> <em>In Proceedings of the 2023 ACM Symposium on Spatial User Interaction</em>, Oct 2023 </div> <div class="periodical"> </div> <div class="links"> <a href="/assets/pdf/liLocationAwareVirtualReality2023.pdf" class="btn btn-sm z-depth-0" role="button"><i class="fa-solid fa-file-pdf"></i> PDF</a> <a href="https://dx.doi.org/10.1145/3607822.3614530" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="ai ai-acmdl"> ACM</i></a> </div> <div class="abstract hidden"> <p>When future passengers are immersed in Virtual Reality (VR), the resulting disconnection from the physical world may degrade their situational awareness on the road. We propose incorporating real-world cues into virtual experiences when passing specific locations to address this. We designed two visualizations using points of interest (POIs), street names alone or combined with live street views. We compared them to two baselines, persistently displaying live cues (Always Live) or no cues (Always VR). In a field study (N=17), participants estimated their locations while exposed to VR entertainment during car rides. The results show that adding environmental cues inevitably degrades VR presence compared to Always VR. However, POI-triggered Text&amp;Live preserves VR presence better than Always Live and attracts user attention to the road more than POI-triggered Text. We discuss situational awareness challenges for using mobile VR on the road and potential incorporation strategies across transport contexts.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/mullerTicTacToesAssessingToe2023a-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/mullerTicTacToesAssessingToe2023a-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/mullerTicTacToesAssessingToe2023a-1400.webp"></source> <img src="/assets/img/publication_preview/mullerTicTacToesAssessingToe2023a.jpg?4f5fa35ae4f4ead6fcf7e31a2a1b8812" class="preview z-depth-1 rounded" width="auto" height="auto" alt="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="mullerTicTacToesAssessingToe2023a" class="col-sm-8"> <div class="title"><a href="/publications/mullerTicTacToesAssessingToe2023a">TicTacToes: Assessing Toe Movements as an Input Modality</a></div> <div class="author"> <em>Florian Müller</em>, Daniel Schmitt, <a href="http://andriimatviienko.com/" rel="external nofollow noopener" target="_blank">Andrii Matviienko</a>, Dominik Schön, <a href="https://www.sebastian-guenther.com/" rel="external nofollow noopener" target="_blank">Sebastian Günther</a>, <a href="https://thomaskosch.com/" rel="external nofollow noopener" target="_blank">Thomas Kosch</a>, and <a href="https://mschmitz.org/" rel="external nofollow noopener" target="_blank">Martin Schmitz</a> </div> <div class="periodical"> <em>In Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems</em>, Apr 2023 </div> <div class="periodical"> </div> <div class="links"> <a href="/assets/pdf/mullerTicTacToesAssessingToe2023a.pdf" class="btn btn-sm z-depth-0" role="button"><i class="fa-solid fa-file-pdf"></i> PDF</a> <a href="https://dx.doi.org/10.1145/3544548.3580954" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="ai ai-acmdl"> ACM</i></a> <a href="http://arxiv.org/abs/2303.15811" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="ai ai-arxiv"></i> arxiv</a> <a href="https://www.youtube.com/watch?v=FzA-6F5SJ44" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-youtube"></i> Video</a> <a href="https://www.youtube.com/watch?v=2enVDAGiE8E" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="fa-solid fa-person-chalkboard"></i> Talk</a> </div> <div class="abstract hidden"> <p>From carrying grocery bags to holding onto handles on the bus, there are a variety of situations where one or both hands are busy, hindering the vision of ubiquitous interaction with technology. Voice commands, as a popular hands-free alternative, struggle with ambient noise and privacy issues. As an alternative approach, research explored movements of various body parts (e.g., head, arms) as input modalities, with foot-based techniques proving particularly suitable for hands-free interaction. Whereas previous research only considered the movement of the foot as a whole, in this work, we argue that our toes offer further degrees of freedom that can be leveraged for interaction. To explore the viability of toe-based interaction, we contribute the results of a controlled experiment with 18 participants assessing the impact of five factors on the accuracy, efficiency and user experience of such interfaces. Based on the findings, we provide design recommendations for future toe-based interfaces.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/mullerUndoPortExploringInfluence2023-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/mullerUndoPortExploringInfluence2023-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/mullerUndoPortExploringInfluence2023-1400.webp"></source> <img src="/assets/img/publication_preview/mullerUndoPortExploringInfluence2023.jpg?1c1155ce77f6a5d7a4a48972a6652d74" class="preview z-depth-1 rounded" width="auto" height="auto" alt="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="mullerUndoPortExploringInfluence2023" class="col-sm-8"> <div class="title"><a href="/publications/mullerUndoPortExploringInfluence2023">UndoPort: Exploring the Influence of Undo-Actions for Locomotion in Virtual Reality on the Efficiency, Spatial Understanding and User Experience</a></div> <div class="author"> <em>Florian Müller</em>, Arantxa Ye, Dominik Schön, and <a href="https://julian-rasch.com/" rel="external nofollow noopener" target="_blank">Julian Rasch</a> </div> <div class="periodical"> <em>In Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems</em>, Apr 2023 </div> <div class="periodical"> </div> <div class="links"> <a href="/assets/pdf/mullerUndoPortExploringInfluence2023.pdf" class="btn btn-sm z-depth-0" role="button"><i class="fa-solid fa-file-pdf"></i> PDF</a> <a href="https://dx.doi.org/10.1145/3544548.3581557" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="ai ai-acmdl"> ACM</i></a> <a href="http://arxiv.org/abs/2303.15800" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="ai ai-arxiv"></i> arxiv</a> <a href="https://www.youtube.com/watch?v=BwRc4f8VSEk" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-youtube"></i> Video</a> <a href="https://www.youtube.com/watch?v=INzk1_a2Z3k" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="fa-solid fa-person-chalkboard"></i> Talk</a> </div> <div class="abstract hidden"> <p>When we get lost in Virtual Reality (VR) or want to return to a previous location, we use the same methods of locomotion for the way back as for the way forward. This is time-consuming and requires additional physical orientation changes, increasing the risk of getting tangled in the headsets’ cables. In this paper, we propose the use of undo actions to revert locomotion steps in VR. We explore eight different variations of undo actions as extensions of point&amp;teleport, based on the possibility to undo position and orientation changes together with two different visualizations of the undo step (discrete and continuous). We contribute the results of a controlled experiment with 24 participants investigating the efficiency and orientation of the undo techniques in a radial maze task. We found that the combination of position and orientation undo together with a discrete visualization resulted in the highest efficiency without increasing orientation errors.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/raschGoingGoingGone2023-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/raschGoingGoingGone2023-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/raschGoingGoingGone2023-1400.webp"></source> <img src="/assets/img/publication_preview/raschGoingGoingGone2023.jpg?f37f90ddf7ea855c40757c09ba8ac81e" class="preview z-depth-1 rounded" width="auto" height="auto" alt="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="raschGoingGoingGone2023" class="col-sm-8"> <div class="title"> <i class="fa-solid fa-trophy" style="color: var(--global-theme-color);"></i> <a href="/publications/raschGoingGoingGone2023">Going, Going, Gone: Exploring Intention Communication for Multi-User Locomotion in Virtual Reality</a> </div> <div class="author"> <a href="https://julian-rasch.com/" rel="external nofollow noopener" target="_blank">Julian Rasch</a>, Vladislav Dmitrievic Rusakov, <a href="https://mschmitz.org/" rel="external nofollow noopener" target="_blank">Martin Schmitz</a>, and <em>Florian Müller</em> </div> <div class="periodical"> <em>In Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems</em>, Apr 2023 </div> <div class="periodical"> </div> <div class="links"> <a href="/assets/pdf/raschGoingGoingGone2023.pdf" class="btn btn-sm z-depth-0" role="button"><i class="fa-solid fa-file-pdf"></i> PDF</a> <a href="https://dx.doi.org/10.1145/3544548.3581259" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="ai ai-acmdl"> ACM</i></a> <a href="https://www.youtube.com/watch?v=IC9XBi4Tr34" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-youtube"></i> Video</a> <a href="https://www.youtube.com/watch?v=k9ZvRsWYHDU" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="fa-solid fa-person-chalkboard"></i> Talk</a> </div> <div class="abstract hidden"> <p>Exploring virtual worlds together with others adds a social component to the Virtual Reality (VR) experience that increases connectedness. In the physical world, joint locomotion comes naturally through implicit intention communication and subsequent adjustments of the movement patterns. In VR, however, discrete locomotion techniques such as point&amp;teleport come without prior intention communication, hampering the collective experience. Related work proposes fixed groups, with a single person controlling the group movement, resulting in the loss of individual movement capabilities. To close the gap and mediate between these two extremes, we introduce three intention communication methods and explore them with two baseline methods. We contribute the results of a controlled experiment (n=20) investigating these methods from the perspective of a leader and a follower in a dyadic locomotion task. Our results suggest shared visualizations support the understanding of movement intentions, increasing the group feeling while maintaining individual freedom of movement.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/raschHandsOn3DPrinted2023-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/raschHandsOn3DPrinted2023-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/raschHandsOn3DPrinted2023-1400.webp"></source> <img src="/assets/img/publication_preview/raschHandsOn3DPrinted2023.jpg?fc8dfb7cab2d20f21a1d2b0e7182b613" class="preview z-depth-1 rounded" width="auto" height="auto" alt="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="raschHandsOn3DPrinted2023" class="col-sm-8"> <div class="title"><a href="/publications/raschHandsOn3DPrinted2023">Hands-On 3D Printed Electronics</a></div> <div class="author"> <a href="https://julian-rasch.com/" rel="external nofollow noopener" target="_blank">Julian Rasch</a>, <em>Florian Müller</em>, <a href="https://thomaskosch.com/" rel="external nofollow noopener" target="_blank">Thomas Kosch</a>, <a href="https://mschmitz.org/" rel="external nofollow noopener" target="_blank">Martin Schmitz</a>, and Sebastian S. Feger</div> <div class="periodical"> <em>In Proceedings of the Seventeenth International Conference on Tangible, Embedded, and Embodied Interaction</em>, Feb 2023 </div> <div class="periodical"> </div> <div class="links"> <a href="/assets/pdf/raschHandsOn3DPrinted2023.pdf" class="btn btn-sm z-depth-0" role="button"><i class="fa-solid fa-file-pdf"></i> PDF</a> <a href="https://dx.doi.org/10.1145/3569009.3571846" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="ai ai-acmdl"> ACM</i></a> </div> <div class="abstract hidden"> <p>The parallel improvements in multi-material 3D printers and the quality of conductive filament open new possibilities for the fabrication of tangible and functional objects. In this studio, we discuss best practices for 3D printed electronics, talk about encountered problems, and derive design recommendations. We will guide the participants through a fabrication process by practically designing and printing objects. Consequently, we contemplate individual functional fabricated components, including small printed circuits and multi-material prints. We aim to spark a discussion about individually experienced challenges participants encountered during their design and fabrication process. This discussion includes problemsolving strategies, whose insights benefit other participants. Finally, we show the potential of printed electronics and discuss encouraging new opportunities in this field.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/schonTailorTwistAssessing2023-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/schonTailorTwistAssessing2023-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/schonTailorTwistAssessing2023-1400.webp"></source> <img src="/assets/img/publication_preview/schonTailorTwistAssessing2023.jpg?b7fbccefb537f8a118e10ed4ca796428" class="preview z-depth-1 rounded" width="auto" height="auto" alt="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="schonTailorTwistAssessing2023" class="col-sm-8"> <div class="title"><a href="/publications/schonTailorTwistAssessing2023">Tailor Twist: Assessing Rotational Mid-Air Interactions for Augmented Reality</a></div> <div class="author"> Dominik Schön, <a href="https://thomaskosch.com/" rel="external nofollow noopener" target="_blank">Thomas Kosch</a>, <em>Florian Müller</em>, <a href="https://mschmitz.org/" rel="external nofollow noopener" target="_blank">Martin Schmitz</a>, <a href="https://www.sebastian-guenther.com/" rel="external nofollow noopener" target="_blank">Sebastian Günther</a>, Lukas Bommhardt, and <a href="https://www.informatik.tu-darmstadt.de/telekooperation/staff_tk/tk_staff_details_23168.en.jsp" rel="external nofollow noopener" target="_blank">Max Mühlhäuser</a> </div> <div class="periodical"> <em>In Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems</em>, Apr 2023 </div> <div class="periodical"> </div> <div class="links"> <a href="/assets/pdf/schonTailorTwistAssessing2023.pdf" class="btn btn-sm z-depth-0" role="button"><i class="fa-solid fa-file-pdf"></i> PDF</a> <a href="https://dx.doi.org/10.1145/3544548.3581461" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="ai ai-acmdl"> ACM</i></a> <a href="https://www.youtube.com/watch?v=25Nj5-MSnhA" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-youtube"></i> Video</a> <a href="https://www.youtube.com/watch?v=auieeDWFqjA" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="fa-solid fa-person-chalkboard"></i> Talk</a> </div> <div class="abstract hidden"> <p>Mid-air gestures, widely used in today’s Augmented Reality (AR) applications, are prone to the “gorilla arm” effect, leading to discomfort with prolonged interactions. While prior work has proposed metrics to quantify this effect and means to improve comfort and ergonomics, these works usually only consider simplistic, one-dimensional AR interactions, like reaching for a point or pushing a button. However, interacting with AR environments also involves far more complex tasks, such as rotational knobs, potentially impacting ergonomics. This paper advances the understanding of the ergonomics of rotational mid-air interactions in AR. For this, we contribute the results of a controlled experiment exposing the participants to a rotational task in the interaction space defined by their arms’ reach. Based on the results, we discuss how novel future mid-air gesture modalities benefit from our findings concerning ergonomic-aware rotational interaction.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/vonwillichDensingQueenExplorationMethods2023-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/vonwillichDensingQueenExplorationMethods2023-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/vonwillichDensingQueenExplorationMethods2023-1400.webp"></source> <img src="/assets/img/publication_preview/vonwillichDensingQueenExplorationMethods2023.jpg?58500483ccd1390a162b537079ec227e" class="preview z-depth-1 rounded" width="auto" height="auto" alt="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="vonwillichDensingQueenExplorationMethods2023" class="col-sm-8"> <div class="title"><a href="/publications/vonwillichDensingQueenExplorationMethods2023">DensingQueen: Exploration Methods for Spatial Dense Dynamic Data</a></div> <div class="author"> Julius von Willich, <a href="https://www.sebastian-guenther.com/" rel="external nofollow noopener" target="_blank">Sebastian Günther</a>, <a href="http://andriimatviienko.com/" rel="external nofollow noopener" target="_blank">Andrii Matviienko</a>, <a href="https://mschmitz.org/" rel="external nofollow noopener" target="_blank">Martin Schmitz</a>, <em>Florian Müller</em>, and <a href="https://www.informatik.tu-darmstadt.de/telekooperation/staff_tk/tk_staff_details_23168.en.jsp" rel="external nofollow noopener" target="_blank">Max Mühlhäuser</a> </div> <div class="periodical"> <em>In Proceedings of the 2023 ACM Symposium on Spatial User Interaction</em>, Oct 2023 </div> <div class="periodical"> </div> <div class="links"> <a href="/assets/pdf/vonwillichDensingQueenExplorationMethods2023.pdf" class="btn btn-sm z-depth-0" role="button"><i class="fa-solid fa-file-pdf"></i> PDF</a> <a href="https://dx.doi.org/10.1145/3607822.3614535" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="ai ai-acmdl"> ACM</i></a> <a href="https://github.com/LOEWE-emergenCITY/DensingQueen" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="fa-solid fa-code"></i> Code</a> </div> <div class="abstract hidden"> <p>Research has proposed various interaction techniques to manage the occlusion of 3D data in Virtual Reality (VR), e.g., via gradual refinement. However, tracking dynamically moving data in a dense 3D environment poses the challenge of ever-changing occlusion, especially if motion carries relevant information, which is lost in still images. In this paper, we evaluated two interaction modalities for Spatial Dense Dynamic Data (SDDD), adapted from existing interaction methods for static and spatial data. We evaluated these modalities for exploring SDDD in VR, in an experiment with 18 participants. Furthermore, we investigated the influence of our interaction modalities on different levels of data density on the users’ performance in a no-knowledge task and a prior-knowledge task. Our results indicated significantly degraded performance for higher levels of density. Further, we found that our flashlight-inspired modality successfully improved tracking in SDDD, while a cutting plane-inspired approach was more suitable for highlighting static volumes of interest, particularly in such high-density environments.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/weissUsingPseudoStiffnessEnrich2023a-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/weissUsingPseudoStiffnessEnrich2023a-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/weissUsingPseudoStiffnessEnrich2023a-1400.webp"></source> <img src="/assets/img/publication_preview/weissUsingPseudoStiffnessEnrich2023a.jpg?9a61c7a123617d4c6e39cc55dc93c42c" class="preview z-depth-1 rounded" width="auto" height="auto" alt="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="weissUsingPseudoStiffnessEnrich2023a" class="col-sm-8"> <div class="title"><a href="/publications/weissUsingPseudoStiffnessEnrich2023a">Using Pseudo-Stiffness to Enrich the Haptic Experience in Virtual Reality</a></div> <div class="author"> <a href="https://yannick-weiss.com/" rel="external nofollow noopener" target="_blank">Yannick Weiss</a>, <a href="https://posthci.com/" rel="external nofollow noopener" target="_blank">Steeven Villa</a>, Albrecht Schmidt, <a href="https://sven-mayer.com/" rel="external nofollow noopener" target="_blank">Sven Mayer</a>, and <em>Florian Müller</em> </div> <div class="periodical"> <em>In Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems</em>, Apr 2023 </div> <div class="periodical"> </div> <div class="links"> <a href="/assets/pdf/weissUsingPseudoStiffnessEnrich2023a.pdf" class="btn btn-sm z-depth-0" role="button"><i class="fa-solid fa-file-pdf"></i> PDF</a> <a href="https://dx.doi.org/10.1145/3544548.3581223" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="ai ai-acmdl"> ACM</i></a> <a href="https://www.youtube.com/watch?v=Oex8NlPvcVU" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="fa-solid fa-person-chalkboard"></i> Talk</a> </div> <div class="abstract hidden"> <p>Providing users with a haptic sensation of the hardness and softness of objects in virtual reality is an open challenge. While physical props and haptic devices help, their haptic properties do not allow for dynamic adjustments. To overcome this limitation, we present a novel technique for changing the perceived stiffness of objects based on a visuo-haptic illusion. We achieved this by manipulating the hands’ Control-to-Display (C/D) ratio in virtual reality while pressing down on an object with fixed stiffness. In the first study (N=12), we determine the detection thresholds of the illusion. Our results show that we can exploit a C/D ratio from 0.7 to 3.5 without user detection. In the second study (N=12), we analyze the illusion’s impact on the perceived stiffness. Our results show that participants perceive the objects to be up to 28.1% softer and 8.9% stiffer, allowing for various haptic applications in virtual reality.</p> </div> </div> </div> </li> </ol> <h2 class="bibliography">2022</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/eskaProperPostureDesigning2022-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/eskaProperPostureDesigning2022-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/eskaProperPostureDesigning2022-1400.webp"></source> <img src="/assets/img/publication_preview/eskaProperPostureDesigning2022.jpg?111f6059753ecfc4363c4ca4656b0911" class="preview z-depth-1 rounded" width="auto" height="auto" alt="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="eskaProperPostureDesigning2022" class="col-sm-8"> <div class="title"><a href="/publications/eskaProperPostureDesigning2022">Proper Posture: Designing Posture Feedback Across Musical Instruments</a></div> <div class="author"> Bettina Eska, Jasmin Niess, and <em>Florian Müller</em> </div> <div class="periodical"> May 2022 </div> <div class="periodical"> </div> <div class="links"> <a href="/assets/pdf/eskaProperPostureDesigning2022.pdf" class="btn btn-sm z-depth-0" role="button"><i class="fa-solid fa-file-pdf"></i> PDF</a> <a href="http://arxiv.org/abs/2205.15110" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="ai ai-arxiv"></i> arxiv</a> </div> <div class="abstract hidden"> <p>There is a recommended body posture and hand position for playing every musical instrument, allowing efficient and quick movements without blockage. Due to humans’ limited cognitive capabilities, they struggle to concentrate on several things simultaneously and thus sometimes lose the correct position while playing their instrument. Incorrect positions when playing an instrument can lead to injuries and movement disorders in the long run. Previous work in HCI mainly focused on developing systems to assist in learning an instrument. However, the design space for posture correction when playing a musical instrument has not yet been explored. In this position paper, we present our vision of providing subtle vibrotactile or thermal feedback to guide the focus of attention back to the correct posture when playing a musical instrument. We discuss our concept with a focus on motion recognition and feedback modalities. Finally, we outline the next steps for future research.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/guntherSmoothSteelWool2022-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/guntherSmoothSteelWool2022-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/guntherSmoothSteelWool2022-1400.webp"></source> <img src="/assets/img/publication_preview/guntherSmoothSteelWool2022.jpg?10bc9f37b5f490714bfc006d69cdef93" class="preview z-depth-1 rounded" width="auto" height="auto" alt="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="guntherSmoothSteelWool2022" class="col-sm-8"> <div class="title"><a href="/publications/guntherSmoothSteelWool2022">Smooth as Steel Wool: Effects of Visual Stimuli on the Haptic Perception of Roughness in Virtual Reality</a></div> <div class="author"> <a href="https://www.sebastian-guenther.com/" rel="external nofollow noopener" target="_blank">Sebastian Günther</a>, <a href="https://julian-rasch.com/" rel="external nofollow noopener" target="_blank">Julian Rasch</a>, Dominik Schön, <em>Florian Müller</em>, <a href="https://mschmitz.org/" rel="external nofollow noopener" target="_blank">Martin Schmitz</a>, Jan Riemann, <a href="http://andriimatviienko.com/" rel="external nofollow noopener" target="_blank">Andrii Matviienko</a>, and <a href="https://www.informatik.tu-darmstadt.de/telekooperation/staff_tk/tk_staff_details_23168.en.jsp" rel="external nofollow noopener" target="_blank">Max Mühlhäuser</a> </div> <div class="periodical"> <em>In Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems</em>, Apr 2022 </div> <div class="periodical"> </div> <div class="links"> <a href="/assets/pdf/guntherSmoothSteelWool2022.pdf" class="btn btn-sm z-depth-0" role="button"><i class="fa-solid fa-file-pdf"></i> PDF</a> <a href="https://dx.doi.org/10.1145/3491102.3517454" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="ai ai-acmdl"> ACM</i></a> <a href="https://www.youtube.com/watch?v=IdbIvF004UA" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-youtube"></i> Video</a> <a href="https://www.youtube.com/watch?v=h0FQZQ26uoU" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="fa-solid fa-person-chalkboard"></i> Talk</a> </div> <div class="abstract hidden"> <p>Haptic Feedback is essential for lifelike Virtual Reality (VR) experiences. To provide a wide range of matching sensations of being touched or stroked, current approaches typically need large numbers of different physical textures. However, even advanced devices can only accommodate a limited number of textures to remain wearable. Therefore, a better understanding is necessary of how expectations elicited by different visualizations affect haptic perception, to achieve a balance between physical constraints and great variety of matching physical textures. In this work, we conducted an experiment (N=31) assessing how the perception of roughness is affected within VR. We designed a prototype for arm stroking and compared the effects of different visualizations on the perception of physical textures with distinct roughnesses. Additionally, we used the visualizations’ real-world materials, no-haptics and vibrotactile feedback as baselines. As one result, we found that two levels of roughness can be sufficient to convey a realistic illusion.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/koschNotiBikeAssessingTarget2022-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/koschNotiBikeAssessingTarget2022-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/koschNotiBikeAssessingTarget2022-1400.webp"></source> <img src="/assets/img/publication_preview/koschNotiBikeAssessingTarget2022.jpg?7f3c1cbd4b1671554505e3553dc45657" class="preview z-depth-1 rounded" width="auto" height="auto" alt="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="koschNotiBikeAssessingTarget2022" class="col-sm-8"> <div class="title"><a href="/publications/koschNotiBikeAssessingTarget2022">NotiBike: Assessing Target Selection Techniques for Cyclist Notifications in Augmented Reality</a></div> <div class="author"> <a href="https://thomaskosch.com/" rel="external nofollow noopener" target="_blank">Thomas Kosch</a>, <a href="http://andriimatviienko.com/" rel="external nofollow noopener" target="_blank">Andrii Matviienko</a>, <em>Florian Müller</em>, Jessica Bersch, Christopher Katins, Dominik Schön, and <a href="https://www.informatik.tu-darmstadt.de/telekooperation/staff_tk/tk_staff_details_23168.en.jsp" rel="external nofollow noopener" target="_blank">Max Mühlhäuser</a> </div> <div class="periodical"> <em>Proceedings of the ACM on Human-Computer Interaction</em>, Sep 2022 </div> <div class="periodical"> </div> <div class="links"> <a href="/assets/pdf/koschNotiBikeAssessingTarget2022.pdf" class="btn btn-sm z-depth-0" role="button"><i class="fa-solid fa-file-pdf"></i> PDF</a> <a href="https://dx.doi.org/10.1145/3546732" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="ai ai-acmdl"> ACM</i></a> <a href="https://www.youtube.com/watch?v=hTYBTULau7U" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-youtube"></i> Video</a> </div> <div class="abstract hidden"> <p>Cyclists’ attention is often compromised when interacting with notifications in traffic, hence increasing the likelihood of road accidents. To address this issue, we evaluate three notification interaction modalities and investigate their impact on the interaction performance while cycling: gaze-based Dwell Time, Gestures, and Manual And Gaze Input Cascaded (MAGIC) Pointing. In a user study (N=18), participants confirmed notifications in Augmented Reality (AR) using the three interaction modalities in a simulated biking scenario. We assessed the efficiency regarding reaction times, error rates, and perceived task load. Our results show significantly faster response times for MAGIC Pointing compared to Dwell Time and Gestures, while Dwell Time led to a significantly lower error rate compared to Gestures. Participants favored the MAGIC Pointing approach, supporting cyclists in AR selection tasks. Our research sets the boundaries for more comfortable and easier interaction with notifications and discusses implications for target selections in AR while cycling.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/Matviienko2022-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/Matviienko2022-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/Matviienko2022-1400.webp"></source> <img src="/assets/img/publication_preview/Matviienko2022.jpg?e3c4c358896b7c50c3d62362b58e5cb6" class="preview z-depth-1 rounded" width="auto" height="auto" alt="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Matviienko2022" class="col-sm-8"> <div class="title"><a href="/publications/Matviienko2022">SkyPort: Investigating 3D Teleportation Methods in Virtual Environments</a></div> <div class="author"> <a href="http://andriimatviienko.com/" rel="external nofollow noopener" target="_blank">Andrii Matviienko</a>, <em>Florian Müller</em>, <a href="https://mschmitz.org/" rel="external nofollow noopener" target="_blank">Martin Schmitz</a>, Marco Fendrich, and <a href="https://www.informatik.tu-darmstadt.de/telekooperation/staff_tk/tk_staff_details_23168.en.jsp" rel="external nofollow noopener" target="_blank">Max Mühlhäuser</a> </div> <div class="periodical"> <em>In CHI Conference on Human Factors in Computing Systems</em>, Apr 2022 </div> <div class="periodical"> </div> <div class="links"> <a href="/assets/pdf/Matviienko2022.pdf" class="btn btn-sm z-depth-0" role="button"><i class="fa-solid fa-file-pdf"></i> PDF</a> <a href="https://dx.doi.org/10.1145/3491102.3501983" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="ai ai-acmdl"> ACM</i></a> <a href="https://www.youtube.com/watch?v=sOkzPZnlAeE" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="fa-solid fa-person-chalkboard"></i> Talk</a> </div> <div class="abstract hidden"> <p>Teleportation has become the de facto standard of locomotion in Virtual Reality (VR) environments. However, teleportation with parabolic and linear target aiming methods is restricted to horizontal 2D planes and it is unknown how they transfer to the 3D space. In this paper, we propose six 3D teleportation methods in virtual environments based on the combination of two existing aiming methods (linear and parabolic) and three types of transitioning to a target (instant, interpolated and continuous). To investigate the performance of the proposed teleportation methods, we conducted a controlled lab experiment (N = 24) with a mid-air coin collection task to assess accuracy, efficiency and VR sickness. We discovered that the linear aiming method leads to faster and more accurate target selection. Moreover, a combination of linear aiming and instant transitioning leads to the highest efficiency and accuracy without increasing VR sickness.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/matviienkoBabyYouCan2022-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/matviienkoBabyYouCan2022-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/matviienkoBabyYouCan2022-1400.webp"></source> <img src="/assets/img/publication_preview/matviienkoBabyYouCan2022.jpg?b64898264d289f1784d149427264e03d" class="preview z-depth-1 rounded" width="auto" height="auto" alt="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="matviienkoBabyYouCan2022" class="col-sm-8"> <div class="title"><a href="/publications/matviienkoBabyYouCan2022">Baby, You Can Ride My Bike: Exploring Maneuver Indications of Self-Driving Bicycles Using a Tandem Simulator</a></div> <div class="author"> <a href="http://andriimatviienko.com/" rel="external nofollow noopener" target="_blank">Andrii Matviienko</a>, Damir Mehmedovic, <em>Florian Müller</em>, and <a href="https://www.informatik.tu-darmstadt.de/telekooperation/staff_tk/tk_staff_details_23168.en.jsp" rel="external nofollow noopener" target="_blank">Max Mühlhäuser</a> </div> <div class="periodical"> <em>Proceedings of the ACM on Human-Computer Interaction</em>, Sep 2022 </div> <div class="periodical"> </div> <div class="links"> <a href="/assets/pdf/matviienkoBabyYouCan2022.pdf" class="btn btn-sm z-depth-0" role="button"><i class="fa-solid fa-file-pdf"></i> PDF</a> <a href="https://dx.doi.org/10.1145/3546723" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="ai ai-acmdl"> ACM</i></a> <a href="https://www.youtube.com/watch?v=czOciHFRDk4" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-youtube"></i> Video</a> </div> <div class="abstract hidden"> <p>We envision a future where self-driving bicycles can take us to our destinations. This allows cyclists to use their time on the bike efficiently for work or relaxation without having to focus their attention on traffic. In the related field of self-driving cars, research has shown that communicating the planned route to passengers plays an important role in building trust in automation and situational awareness. For self-driving bicycles, this information transfer will be even more important, as riders will need to actively compensate for the movement of a self-driving bicycle to maintain balance. In this paper, we investigate maneuver indications for self-driving bicycles: (1) ambient light in a helmet, (2) head-up display indications, (3) speech feedback, (4) vibration on the handlebar, and (5) no assistance. To evaluate these indications, we conducted an outdoor experiment (N = 25) in a proposed tandem simulator consisting of a tandem bicycle with a steering and braking control on the back seat and a rider in full control of it. Our results indicate that riders respond faster to visual cues and focus comparably on the reading task while riding with and without maneuver indications. Additionally, we found that the tandem simulator is realistic, safe, and creates an awareness of a human cyclist controlling the tandem.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/matviienkoBikeARUnderstandingCyclists2022-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/matviienkoBikeARUnderstandingCyclists2022-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/matviienkoBikeARUnderstandingCyclists2022-1400.webp"></source> <img src="/assets/img/publication_preview/matviienkoBikeARUnderstandingCyclists2022.jpg?7f3f7bda34ae4657e0f4568b9732803b" class="preview z-depth-1 rounded" width="auto" height="auto" alt="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="matviienkoBikeARUnderstandingCyclists2022" class="col-sm-8"> <div class="title"><a href="/publications/matviienkoBikeARUnderstandingCyclists2022">BikeAR: Understanding Cyclists’ Crossing Decision-Making at Uncontrolled Intersections Using Augmented Reality</a></div> <div class="author"> <a href="http://andriimatviienko.com/" rel="external nofollow noopener" target="_blank">Andrii Matviienko</a>, <em>Florian Müller</em>, Dominik Schön, Paul Seesemann, <a href="https://www.sebastian-guenther.com/" rel="external nofollow noopener" target="_blank">Sebastian Günther</a>, and <a href="https://www.informatik.tu-darmstadt.de/telekooperation/staff_tk/tk_staff_details_23168.en.jsp" rel="external nofollow noopener" target="_blank">Max Mühlhäuser</a> </div> <div class="periodical"> <em>In Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems</em>, Apr 2022 </div> <div class="periodical"> </div> <div class="links"> <a href="/assets/pdf/matviienkoBikeARUnderstandingCyclists2022.pdf" class="btn btn-sm z-depth-0" role="button"><i class="fa-solid fa-file-pdf"></i> PDF</a> <a href="https://dx.doi.org/10.1145/3491102.3517560" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="ai ai-acmdl"> ACM</i></a> <a href="https://www.youtube.com/watch?v=YKsDlPmSd68" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-youtube"></i> Video</a> <a href="https://www.youtube.com/watch?v=eA7D239WOd0" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="fa-solid fa-person-chalkboard"></i> Talk</a> </div> <div class="abstract hidden"> <p>Cycling has become increasingly popular as a means of transportation. However, cyclists remain a highly vulnerable group of road users. According to accident reports, one of the most dangerous situations for cyclists are uncontrolled intersections, where cars approach from both directions. To address this issue and assist cyclists in crossing decision-making at uncontrolled intersections, we designed two visualizations that: (1) highlight occluded cars through an X-ray vision and (2) depict the remaining time the intersection is safe to cross via a Countdown. To investigate the efficiency of these visualizations, we proposed an Augmented Reality simulation as a novel evaluation method, in which the above visualizations are represented as AR, and conducted a controlled experiment with 24 participants indoors. We found that the X-ray ensures a fast selection of shorter gaps between cars, while the Countdown facilitates a feeling of safety and provides a better intersection overview.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/matviienkoEScootARExploringUnimodal2022-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/matviienkoEScootARExploringUnimodal2022-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/matviienkoEScootARExploringUnimodal2022-1400.webp"></source> <img src="/assets/img/publication_preview/matviienkoEScootARExploringUnimodal2022.jpg?ab359f47f396ebd8c04f36bc8f583737" class="preview z-depth-1 rounded" width="auto" height="auto" alt="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="matviienkoEScootARExploringUnimodal2022" class="col-sm-8"> <div class="title"><a href="/publications/matviienkoEScootARExploringUnimodal2022">E-ScootAR: Exploring Unimodal Warnings for E-Scooter Riders in Augmented Reality</a></div> <div class="author"> <a href="http://andriimatviienko.com/" rel="external nofollow noopener" target="_blank">Andrii Matviienko</a>, <em>Florian Müller</em>, Dominik Schön, Régis Fayard, Salar Abaspur, Yi Li, and <a href="https://www.informatik.tu-darmstadt.de/telekooperation/staff_tk/tk_staff_details_23168.en.jsp" rel="external nofollow noopener" target="_blank">Max Mühlhäuser</a> </div> <div class="periodical"> <em>In Extended Abstracts of the 2022 CHI Conference on Human Factors in Computing Systems</em>, Apr 2022 </div> <div class="periodical"> </div> <div class="links"> <a href="/assets/pdf/matviienkoEScootARExploringUnimodal2022.pdf" class="btn btn-sm z-depth-0" role="button"><i class="fa-solid fa-file-pdf"></i> PDF</a> <a href="https://dx.doi.org/10.1145/3491101.3519831" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="ai ai-acmdl"> ACM</i></a> <a href="https://www.youtube.com/watch?v=uYBIB51xgNs" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="fa-solid fa-person-chalkboard"></i> Talk</a> </div> <div class="abstract hidden"> <p>Micro-mobility is becoming a more popular means of transportation. However, this increased popularity brings its challenges. In particular, the accident rates for E-Scooter riders increase, which endangers the riders and other road users. In this paper, we explore the idea of augmenting E-Scooters with unimodal warnings to prevent collisions with other road users, which include Augmented Reality (AR) notifications, vibrotactile feedback on the handlebar, and auditory signals in the AR glasses. We conducted an outdoor experiment (N = 13) using an Augmented Reality simulation and compared these types of warnings in terms of reaction time, accident rate, and feeling of safety. Our results indicate that AR and auditory warnings lead to shorter reaction times, have a better perception, and create a better feeling of safety than vibrotactile warnings. Moreover, auditory signals have a higher acceptance by the riders compared to the other two types of warnings.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/matviienkoReducingVirtualReality2022-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/matviienkoReducingVirtualReality2022-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/matviienkoReducingVirtualReality2022-1400.webp"></source> <img src="/assets/img/publication_preview/matviienkoReducingVirtualReality2022.jpg?7d20f7f2c7992abeb0070c330f02decd" class="preview z-depth-1 rounded" width="auto" height="auto" alt="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="matviienkoReducingVirtualReality2022" class="col-sm-8"> <div class="title"><a href="/publications/matviienkoReducingVirtualReality2022">Reducing Virtual Reality Sickness for Cyclists in VR Bicycle Simulators</a></div> <div class="author"> <a href="http://andriimatviienko.com/" rel="external nofollow noopener" target="_blank">Andrii Matviienko</a>, <em>Florian Müller</em>, Marcel Zickler, Lisa Alina Gasche, Julia Abels, Till Steinert, and <a href="https://www.informatik.tu-darmstadt.de/telekooperation/staff_tk/tk_staff_details_23168.en.jsp" rel="external nofollow noopener" target="_blank">Max Mühlhäuser</a> </div> <div class="periodical"> <em>In Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems</em>, Apr 2022 </div> <div class="periodical"> </div> <div class="links"> <a href="/assets/pdf/matviienkoReducingVirtualReality2022.pdf" class="btn btn-sm z-depth-0" role="button"><i class="fa-solid fa-file-pdf"></i> PDF</a> <a href="https://dx.doi.org/10.1145/3491102.3501959" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="ai ai-acmdl"> ACM</i></a> <a href="https://www.youtube.com/watch?v=BC5fpXVYnnA" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="fa-solid fa-person-chalkboard"></i> Talk</a> </div> <div class="abstract hidden"> <p>Virtual Reality (VR) bicycle simulations aim to recreate the feeling of riding a bicycle and are commonly used in many application areas. However, current solutions still create mismatches between the visuals and physical movement, which causes VR sickness and diminishes the cycling experience. To reduce VR sickness in bicycle simulators, we conducted two controlled lab experiments addressing two main causes of VR sickness: (1) steering methods and (2) cycling trajectory. In the first experiment (N = 18) we compared handlebar, HMD, and upper-body steering methods. In the second experiment (N = 24) we explored three types of movement in VR (1D, 2D, and 3D trajectories) and three countermeasures (airflow, vibration, and dynamic Field-of-View) to reduce VR sickness. We found that handlebar steering leads to the lowest VR sickness without decreasing cycling performance and airflow suggests to be the most promising method to reduce VR sickness for all three types of trajectories.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/muhlhauserMuC22Proceedings2022-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/muhlhauserMuC22Proceedings2022-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/muhlhauserMuC22Proceedings2022-1400.webp"></source> <img src="/assets/img/publication_preview/muhlhauserMuC22Proceedings2022.jpg?cf341dac2758dcffe7a2d62ceeaa2939" class="preview z-depth-1 rounded" width="auto" height="auto" alt="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="muhlhauserMuC22Proceedings2022" class="col-sm-8"> <div class="title"><a href="/publications/muhlhauserMuC22Proceedings2022">MuC ’22: Proceedings of Mensch Und Computer 2022</a></div> <div class="author"> Max Mühlhauser, Christian Reuter, Bastian Pfleging, <a href="https://thomaskosch.com/" rel="external nofollow noopener" target="_blank">Thomas Kosch</a>, <a href="http://andriimatviienko.com/" rel="external nofollow noopener" target="_blank">Andrii Matviienko</a>, Kathrin Gerling, <a href="https://sven-mayer.com/" rel="external nofollow noopener" target="_blank">Sven Mayer</a>, Wilko Heuten, Tanja Döring, <em>Florian Müller</em>, and <a href="https://mschmitz.org/" rel="external nofollow noopener" target="_blank">Martin Schmitz</a> </div> <div class="periodical"> Sep 2022 </div> <div class="periodical"> </div> <div class="links"> <a href="https://dx.doi.org/10.1145/3543758" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="ai ai-acmdl"> ACM</i></a> </div> <div class="abstract hidden"> <p>Die Mensch und Computer 2022 fand vom 4.-7. September 2022 an der Technischen Universität Darmstadt statt. Die 2001 ins Leben gerufene ,,Mensch und Computer“ (MuC) ist die größte Tagungsreihe der Mensch-Computer-Interaktion in Europa. Wir sind dankbar, dass wir im Jahr des 50jährigen Bestehens des Fachbereichs Informatik der TU Darmstadt fast 600 Teilnehmende aus Wissenschaft und Industrie live, in Farbe, physisch, also real in Darmstadt begrüßen konnten – passend zu unserem Konferenzmotto: ,,Facing Realities“. Träger der Konferenz waren der Fachbereich Mensch-Computer-Interaktion der Gesellschaft für Informatik (GI e.V.) und die German UPA – der Berufsverband der Deutschen Usability und User Experience Professionals. Die Technische Universität Darmstadt und insbesondere der Fachbereich Informatik fungierte als Gastgeber.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/schmitz2022squeezy-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/schmitz2022squeezy-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/schmitz2022squeezy-1400.webp"></source> <img src="/assets/img/publication_preview/schmitz2022squeezy.jpg?0b0f943a76e07fc766e1044fc5cae841" class="preview z-depth-1 rounded" width="auto" height="auto" alt="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="schmitz2022squeezy" class="col-sm-8"> <div class="title"><a href="/publications/schmitz2022squeezy">Squeezy-Feely: Investigating Lateral Thumb-Index Pinching as an Input Modality</a></div> <div class="author"> <a href="https://mschmitz.org/" rel="external nofollow noopener" target="_blank">Martin Schmitz</a>, <a href="https://www.sebastian-guenther.com/" rel="external nofollow noopener" target="_blank">Sebastian Günther</a>, Dominik Schön, and <em>Florian Müller</em> </div> <div class="periodical"> <em>In Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems</em>, Apr 2022 </div> <div class="periodical"> </div> <div class="links"> <a href="/assets/pdf/schmitz2022squeezy.pdf" class="btn btn-sm z-depth-0" role="button"><i class="fa-solid fa-file-pdf"></i> PDF</a> <a href="https://dx.doi.org/10.1145/3491102.3501981" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="ai ai-acmdl"> ACM</i></a> <a href="https://www.youtube.com/watch?v=hoHcyrAqTeM" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-youtube"></i> Video</a> <a href="https://www.youtube.com/watch?v=MD3WY1FIUaA" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="fa-solid fa-person-chalkboard"></i> Talk</a> </div> <div class="abstract hidden"> <p>From zooming on smartphones and mid-air gestures to deformable user interfaces, thumb-index pinching grips are used in many interaction techniques. However, there is still a lack of systematic understanding of how the accuracy and efficiency of such grips are affected by various factors such as counterforce, grip span, and grip direction. Therefore, in this paper, we contribute an evaluation (N = 18) of thumb-index pinching performance in a visual targeting task using scales up to 75 items. As part of our findings, we conclude that the pinching interaction between the thumb and index finger is a promising modality also for one-dimensional input on higher scales. Furthermore, we discuss and outline implications for future user interfaces that benefit from pinching as an additional and complementary interaction modality.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/schmitzRethinkingSmartObjects2022-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/schmitzRethinkingSmartObjects2022-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/schmitzRethinkingSmartObjects2022-1400.webp"></source> <img src="/assets/img/publication_preview/schmitzRethinkingSmartObjects2022.jpg?bd0c632da8659b02e4844f839dbad9df" class="preview z-depth-1 rounded" width="auto" height="auto" alt="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="schmitzRethinkingSmartObjects2022" class="col-sm-8"> <div class="title"><a href="/publications/schmitzRethinkingSmartObjects2022">Rethinking Smart Objects: The International Workshop on Interacting with Smart Objects in Interactive Spaces</a></div> <div class="author"> <a href="https://mschmitz.org/" rel="external nofollow noopener" target="_blank">Martin Schmitz</a>, <a href="https://www.sebastian-guenther.com/" rel="external nofollow noopener" target="_blank">Sebastian Günther</a>, <a href="https://informatik.rub.de/digisoul/personen/marky/" rel="external nofollow noopener" target="_blank">Karola Marky</a>, <em>Florian Müller</em>, <a href="http://andriimatviienko.com/" rel="external nofollow noopener" target="_blank">Andrii Matviienko</a>, Alexandra Voit, Roberts Marky, <a href="https://www.informatik.tu-darmstadt.de/telekooperation/staff_tk/tk_staff_details_23168.en.jsp" rel="external nofollow noopener" target="_blank">Max Mühlhäuser</a>, and <a href="https://thomaskosch.com/" rel="external nofollow noopener" target="_blank">Thomas Kosch</a> </div> <div class="periodical"> <em>In Companion Proceedings of the 2022 Conference on Interactive Surfaces and Spaces</em>, Nov 2022 </div> <div class="periodical"> </div> <div class="links"> <a href="/assets/pdf/schmitzRethinkingSmartObjects2022.pdf" class="btn btn-sm z-depth-0" role="button"><i class="fa-solid fa-file-pdf"></i> PDF</a> <a href="https://dx.doi.org/10.1145/3532104.3571470" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="ai ai-acmdl"> ACM</i></a> <a href="https://smart-objects.org/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="fas fa-link"></i> Website</a> </div> <div class="abstract hidden"> <p>The increasing proliferation of smart objects in everyday life has changed how we interact with computers. Instead of concentrating computational capabilities and interaction into one device, everyday objects have naturally integrated parts of interactive features. Although this has led to many practical applications, the possibilities for explicit or implicit interaction with such objects are still limited in interaction spaces. We still often rely on smartphones as interactive hubs for controlling smart objects, hence not fulfilling the vision of truly smart objects. The workshop Rethinking Smart Objects invites practitioners and researchers from both academia and industry to discuss novel interaction paradigms and the integration and societal implications of using smart objects in interactive space. This workshop will include an action plan with leading questions, aiming to move the research field forward.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/schonTrackItPipeFabricationPipeline2022-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/schonTrackItPipeFabricationPipeline2022-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/schonTrackItPipeFabricationPipeline2022-1400.webp"></source> <img src="/assets/img/publication_preview/schonTrackItPipeFabricationPipeline2022.jpg?1f9abd349fc9a777948b504bc3bd287d" class="preview z-depth-1 rounded" width="auto" height="auto" alt="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="schonTrackItPipeFabricationPipeline2022" class="col-sm-8"> <div class="title"><a href="/publications/schonTrackItPipeFabricationPipeline2022">TrackItPipe: A Fabrication Pipeline To Incorporate Location and Rotation Tracking Into 3D Printed Objects</a></div> <div class="author"> Dominik Schön, <a href="https://thomaskosch.com/" rel="external nofollow noopener" target="_blank">Thomas Kosch</a>, <a href="https://mschmitz.org/" rel="external nofollow noopener" target="_blank">Martin Schmitz</a>, <em>Florian Müller</em>, <a href="https://www.sebastian-guenther.com/" rel="external nofollow noopener" target="_blank">Sebastian Günther</a>, Johannes Kreutz, and <a href="https://www.informatik.tu-darmstadt.de/telekooperation/staff_tk/tk_staff_details_23168.en.jsp" rel="external nofollow noopener" target="_blank">Max Mühlhäuser</a> </div> <div class="periodical"> <em>In Adjunct Proceedings of the 35th Annual ACM Symposium on User Interface Software and Technology</em>, Oct 2022 </div> <div class="periodical"> </div> <div class="links"> <a href="/assets/pdf/schonTrackItPipeFabricationPipeline2022.pdf" class="btn btn-sm z-depth-0" role="button"><i class="fa-solid fa-file-pdf"></i> PDF</a> <a href="https://dx.doi.org/10.1145/3526114.3558719" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="ai ai-acmdl"> ACM</i></a> <a href="https://github.com/Dominik-Schoen/TrackItPipe" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="fa-solid fa-code"></i> Code</a> </div> <div class="abstract hidden"> <p>The increasing convergence of the digital and physical world creates a growing urgency to integrate 3D printed physical tangibles with virtual environments. A precise position and rotation tracking are essential to integrate such physical objects with a virtual environment. However, available 3D models commonly do not provide tracking support on their composition, which requires modifications by CAD experts. This poses a challenge for users with no prior CAD experience. This work presents TrackItPipe, a fabrication pipeline supporting users by semi-automatically adding tracking capabilities for 3D printable tangibles tailored to environmental requirements. TrackItPipe integrates modifications to the 3D model, produces the respective tangibles for 3D printing, and provides integration scripts for Mixed Reality. Using TrackItPipe, users can rapidly equip objects with tracking capabilities.</p> </div> </div> </div> </li> </ol> <h2 class="bibliography">2021</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/Elsayed2021a-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/Elsayed2021a-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/Elsayed2021a-1400.webp"></source> <img src="/assets/img/publication_preview/Elsayed2021a.jpg?f3388890b17b274b9ad5f15a4dab8e49" class="preview z-depth-1 rounded" width="auto" height="auto" alt="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Elsayed2021a" class="col-sm-8"> <div class="title"><a href="/publications/Elsayed2021a">CameraReady: Assessing the Influence of Display Types and Visualizations on Posture Guidance</a></div> <div class="author"> Hesham Elsayed, Philipp Hoffmann, <a href="https://www.sebastian-guenther.com/" rel="external nofollow noopener" target="_blank">Sebastian Günther</a>, <a href="https://mschmitz.org/" rel="external nofollow noopener" target="_blank">Martin Schmitz</a>, Martin Weigel, <a href="https://www.informatik.tu-darmstadt.de/telekooperation/staff_tk/tk_staff_details_23168.en.jsp" rel="external nofollow noopener" target="_blank">Max Mühlhäuser</a>, and <em>Florian Müller</em> </div> <div class="periodical"> <em>In Designing Interactive Systems Conference 2021</em>, Jun 2021 </div> <div class="periodical"> </div> <div class="links"> <a href="/assets/pdf/Elsayed2021a.pdf" class="btn btn-sm z-depth-0" role="button"><i class="fa-solid fa-file-pdf"></i> PDF</a> <a href="https://dx.doi.org/10.1145/3461778.3462026" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="ai ai-acmdl"> ACM</i></a> <a href="https://www.youtube.com/watch?v=9OWH4eBrHtk" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-youtube"></i> Video</a> <a href="https://www.youtube.com/watch?v=KAjwxyqAegY" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="fa-solid fa-person-chalkboard"></i> Talk</a> </div> <div class="abstract hidden"> <p>Computer-supported posture guidance is used in sports, dance training, expression of art with movements, and learning gestures for interaction. At present, the influence of display types and visualizations have not been investigated in the literature. These factors are important as they directly impact perception and cognitive load, and hence influence the performance of participants. In this paper, we conducted a controlled experiment with 20 participants to compare the use of five display types with different screen sizes: smartphones, tablets, desktop monitors, TVs, and large displays. On each device, we compared three common visualizations for posture guidance: skeletons, silhouettes, and 3d body models. To conduct our assessment, we developed a mobile and cross-platform system that only requires a single camera. Our results show that compared to a smartphone display, larger displays show a lower error (12%). Regarding the choice of visualization, participants rated 3D body models as significantly more usable in comparison to a skeleton visualization.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/guntherActuBoardOpenRapid2021-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/guntherActuBoardOpenRapid2021-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/guntherActuBoardOpenRapid2021-1400.webp"></source> <img src="/assets/img/publication_preview/guntherActuBoardOpenRapid2021.jpg?70c74cf65d4643bca155bf8384fe5933" class="preview z-depth-1 rounded" width="auto" height="auto" alt="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="guntherActuBoardOpenRapid2021" class="col-sm-8"> <div class="title"><a href="/publications/guntherActuBoardOpenRapid2021">ActuBoard: An Open Rapid Prototyping Platform to Integrate Hardware Actuators in Remote Applications</a></div> <div class="author"> <a href="https://www.sebastian-guenther.com/" rel="external nofollow noopener" target="_blank">Sebastian Günther</a>, <em>Florian Müller</em>, Felix Hübner, <a href="https://www.informatik.tu-darmstadt.de/telekooperation/staff_tk/tk_staff_details_23168.en.jsp" rel="external nofollow noopener" target="_blank">Max Mühlhäuser</a>, and <a href="http://andriimatviienko.com/" rel="external nofollow noopener" target="_blank">Andrii Matviienko</a> </div> <div class="periodical"> <em>In Companion of the 2021 ACM SIGCHI Symposium on Engineering Interactive Computing Systems</em>, Jun 2021 </div> <div class="periodical"> </div> <div class="links"> <a href="/assets/pdf/guntherActuBoardOpenRapid2021.pdf" class="btn btn-sm z-depth-0" role="button"><i class="fa-solid fa-file-pdf"></i> PDF</a> <a href="https://dx.doi.org/10.1145/3459926.3464757" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="ai ai-acmdl"> ACM</i></a> <a href="https://git.tk.informatik.tu-darmstadt.de/sebastian.guenther/actuboard-public" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="fa-solid fa-code"></i> Code</a> </div> <div class="abstract hidden"> <p>Prototyping is an essential step in developing tangible experiences and novel devices, ranging from haptic feedback to wearables. However, prototyping of actuated devices nowadays often requires repetitive and time-consuming steps, such as wiring, soldering, and programming basic communication, before HCI researchers and designers can focus on their primary interest: designing interaction. In this paper, we present ActuBoard, a prototyping platform to support 1) quick assembly, 2) less preparation work, and 3) the inclusion of non-tech-savvy users. With ActuBoard, users are not required to create complex circuitry, write a single line of firmware, or implementing communication protocols. Acknowledging existing systems, our platform combines the flexibility of low-level microcontrollers and ease-of-use of abstracted tinker platforms to control actuators from separate applications. As further contribution, we highlight the technical specifications and published the ActuBoard platform as Open Source.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/markyLetsFretsAssisting2021-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/markyLetsFretsAssisting2021-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/markyLetsFretsAssisting2021-1400.webp"></source> <img src="/assets/img/publication_preview/markyLetsFretsAssisting2021.jpg?e0256944aad12f8f123e8e035ed50d4c" class="preview z-depth-1 rounded" width="auto" height="auto" alt="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="markyLetsFretsAssisting2021" class="col-sm-8"> <div class="title"><a href="/publications/markyLetsFretsAssisting2021">Let’s Frets! Assisting Guitar Students During Practice via Capacitive Sensing</a></div> <div class="author"> <a href="https://informatik.rub.de/digisoul/personen/marky/" rel="external nofollow noopener" target="_blank">Karola Marky</a>, Andreas Weiß, <a href="http://andriimatviienko.com/" rel="external nofollow noopener" target="_blank">Andrii Matviienko</a>, Florian Brandherm, Sebastian Wolf, <a href="https://mschmitz.org/" rel="external nofollow noopener" target="_blank">Martin Schmitz</a>, Florian Krell, <em>Florian Müller</em>, <a href="https://www.informatik.tu-darmstadt.de/telekooperation/staff_tk/tk_staff_details_23168.en.jsp" rel="external nofollow noopener" target="_blank">Max Mühlhäuser</a>, and <a href="https://thomaskosch.com/" rel="external nofollow noopener" target="_blank">Thomas Kosch</a> </div> <div class="periodical"> <em>In Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems</em>, May 2021 </div> <div class="periodical"> </div> <div class="links"> <a href="/assets/pdf/markyLetsFretsAssisting2021.pdf" class="btn btn-sm z-depth-0" role="button"><i class="fa-solid fa-file-pdf"></i> PDF</a> <a href="https://dx.doi.org/10.1145/3411764.3445595" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="ai ai-acmdl"> ACM</i></a> <a href="https://www.youtube.com/watch?v=vFx8c5aF6vA" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-youtube"></i> Video</a> <a href="https://www.youtube.com/watch?v=YhLuCpgnaBg" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="fa-solid fa-person-chalkboard"></i> Talk</a> </div> <div class="abstract hidden"> <p>Learning a musical instrument requires regular exercise. However, students are often on their own during their practice sessions due to the limited time with their teachers, which increases the likelihood of mislearning playing techniques. To address this issue, we present Let’s Frets - a modular guitar learning system that provides visual indicators and capturing of finger positions on a 3D-printed capacitive guitar fretboard. We based the design of Let’s Frets on requirements collected through in-depth interviews with professional guitarists and teachers. In a user study (N=24), we evaluated the feedback modules of Let’s Frets against fretboard charts. Our results show that visual indicators require the least time to realize new finger positions while a combination of visual indicators and position capturing yielded the highest playing accuracy. We conclude how Let’s Frets enables independent practice sessions that can be translated to other musical instruments.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/markyLetsFretsMastering2021-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/markyLetsFretsMastering2021-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/markyLetsFretsMastering2021-1400.webp"></source> <img src="/assets/img/publication_preview/markyLetsFretsMastering2021.jpg?6a42def7a67c50d16c60ca0f25465f47" class="preview z-depth-1 rounded" width="auto" height="auto" alt="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="markyLetsFretsMastering2021" class="col-sm-8"> <div class="title"><a href="/publications/markyLetsFretsMastering2021">Let’s Frets! Mastering Guitar Playing with Capacitive Sensing and Visual Guidance</a></div> <div class="author"> <a href="https://informatik.rub.de/digisoul/personen/marky/" rel="external nofollow noopener" target="_blank">Karola Marky</a>, Andreas Weiß, <em>Florian Müller</em>, <a href="https://mschmitz.org/" rel="external nofollow noopener" target="_blank">Martin Schmitz</a>, <a href="https://www.informatik.tu-darmstadt.de/telekooperation/staff_tk/tk_staff_details_23168.en.jsp" rel="external nofollow noopener" target="_blank">Max Mühlhäuser</a>, and <a href="https://thomaskosch.com/" rel="external nofollow noopener" target="_blank">Thomas Kosch</a> </div> <div class="periodical"> <em>In Extended Abstracts of the 2021 CHI Conference on Human Factors in Computing Systems</em>, May 2021 </div> <div class="periodical"> </div> <div class="links"> <a href="/assets/pdf/markyLetsFretsMastering2021.pdf" class="btn btn-sm z-depth-0" role="button"><i class="fa-solid fa-file-pdf"></i> PDF</a> <a href="https://dx.doi.org/10.1145/3411763.3451536" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="ai ai-acmdl"> ACM</i></a> <a href="https://www.youtube.com/watch?v=a0C4TkbiRfg" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-youtube"></i> Video</a> </div> <div class="abstract hidden"> <p>Mastering the guitar requires regular exercise to develop new skills and maintain existing abilities. We present Let’s Frets - a modular guitar support system that provides visual guidance through LEDs that are integrated into a capacitive fretboard to support the practice of chords, scales, melodies, and exercises. Additional feedback is provided through a 3D-printed fretboard that senses the finger positions through capacitive sensing. We envision Let’s Frets as an integrated guitar support system that raises the awareness of guitarists about their playing styles, their training progress, the composition of new pieces, and facilitating remote collaborations between teachers as well as guitar students. This interactivity demonstrates Let’s Frets with an augmented fretboard and supporting software that runs on a mobile device.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/matviienkoVRtangiblesAssistingChildren2021-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/matviienkoVRtangiblesAssistingChildren2021-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/matviienkoVRtangiblesAssistingChildren2021-1400.webp"></source> <img src="/assets/img/publication_preview/matviienkoVRtangiblesAssistingChildren2021.jpg?aefcf94f2ce570535fa5d8685bbd9f6e" class="preview z-depth-1 rounded" width="auto" height="auto" alt="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="matviienkoVRtangiblesAssistingChildren2021" class="col-sm-8"> <div class="title"><a href="/publications/matviienkoVRtangiblesAssistingChildren2021">VRtangibles: Assisting Children in Creating Virtual Scenes Using Tangible Objects and Touch Input</a></div> <div class="author"> <a href="http://andriimatviienko.com/" rel="external nofollow noopener" target="_blank">Andrii Matviienko</a>, Marcel Langer, <em>Florian Müller</em>, <a href="https://mschmitz.org/" rel="external nofollow noopener" target="_blank">Martin Schmitz</a>, and <a href="https://www.informatik.tu-darmstadt.de/telekooperation/staff_tk/tk_staff_details_23168.en.jsp" rel="external nofollow noopener" target="_blank">Max Mühlhäuser</a> </div> <div class="periodical"> <em>In Extended Abstracts of the 2021 CHI Conference on Human Factors in Computing Systems</em>, May 2021 </div> <div class="periodical"> </div> <div class="links"> <a href="/assets/pdf/matviienkoVRtangiblesAssistingChildren2021.pdf" class="btn btn-sm z-depth-0" role="button"><i class="fa-solid fa-file-pdf"></i> PDF</a> <a href="https://dx.doi.org/10.1145/3411763.3451671" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="ai ai-acmdl"> ACM</i></a> <a href="https://www.youtube.com/watch?v=wW-d9yokhYI" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="fa-solid fa-person-chalkboard"></i> Talk</a> </div> <div class="abstract hidden"> <p>Children are increasingly exposed to virtual reality (VR) technology as end-users. However, they miss an opportunity to become active creators due to the barrier of insufficient technical background. Creating scenes in VR requires considerable programming knowledge and excludes non-tech-savvy users, e.g., school children. In this paper, we showcase a system called VRtangibles, which combines tangible objects and touch input to create virtual scenes without programming. With VRtangibles, we aim to engage children in the active creation of virtual scenes via playful hands-on activities. From the lab study with six school children, we discovered that the majority of children were successful in creating virtual scenes using VRtangibles and found it engaging and fun to use.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/schmitzItsyBitsFabricationRecognition2021-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/schmitzItsyBitsFabricationRecognition2021-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/schmitzItsyBitsFabricationRecognition2021-1400.webp"></source> <img src="/assets/img/publication_preview/schmitzItsyBitsFabricationRecognition2021.jpg?04181ba02244786ce6fd967db16b7940" class="preview z-depth-1 rounded" width="auto" height="auto" alt="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="schmitzItsyBitsFabricationRecognition2021" class="col-sm-8"> <div class="title"><a href="/publications/schmitzItsyBitsFabricationRecognition2021">Itsy-Bits: Fabrication and Recognition of 3D-Printed Tangibles with Small Footprints on Capacitive Touchscreens</a></div> <div class="author"> <a href="https://mschmitz.org/" rel="external nofollow noopener" target="_blank">Martin Schmitz</a>, <em>Florian Müller</em>, <a href="https://www.informatik.tu-darmstadt.de/telekooperation/staff_tk/tk_staff_details_23168.en.jsp" rel="external nofollow noopener" target="_blank">Max Mühlhäuser</a>, Jan Riemann, and Huy Viet Viet Le</div> <div class="periodical"> <em>In Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems</em>, May 2021 </div> <div class="periodical"> </div> <div class="links"> <a href="/assets/pdf/schmitzItsyBitsFabricationRecognition2021.pdf" class="btn btn-sm z-depth-0" role="button"><i class="fa-solid fa-file-pdf"></i> PDF</a> <a href="https://dx.doi.org/10.1145/3411764.3445502" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="ai ai-acmdl"> ACM</i></a> <a href="https://www.youtube.com/watch?v=WrdRQlt2fsA" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-youtube"></i> Video</a> <a href="https://www.youtube.com/watch?v=fI3zZz4fnMY" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="fa-solid fa-person-chalkboard"></i> Talk</a> </div> <div class="abstract hidden"> <p>Tangibles on capacitive touchscreens are a promising approach to overcome the limited expressiveness of touch input. While research has suggested many approaches to detect tangibles, the corresponding tangibles are either costly or have a considerable minimal size. This makes them bulky and unattractive for many applications. At the same time, they obscure valuable display space for interaction. To address these shortcomings, we contribute Itsy-Bits: a fabrication pipeline for 3D printing and recognition of tangibles on capacitive touchscreens with a footprint as small as a fingertip. Each Itsy-Bit consists of an enclosing 3D object and a unique conductive 2D shape on its bottom. Using only raw data of commodity capacitive touchscreens, Itsy-Bits reliably identifies and locates a variety of shapes in different sizes and estimates their orientation. Through example applications and a technical evaluation, we demonstrate the feasibility and applicability of Itsy-Bits for tangibles with small footprints.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/schmitzOhSnapFabrication2021-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/schmitzOhSnapFabrication2021-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/schmitzOhSnapFabrication2021-1400.webp"></source> <img src="/assets/img/publication_preview/schmitzOhSnapFabrication2021.jpg?550018af3814c222da72707d0ef0fbf0" class="preview z-depth-1 rounded" width="auto" height="auto" alt="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="schmitzOhSnapFabrication2021" class="col-sm-8"> <div class="title"><a href="/publications/schmitzOhSnapFabrication2021">Oh, Snap! A Fabrication Pipeline to Magnetically Connect Conventional and 3D-Printed Electronics</a></div> <div class="author"> <a href="https://mschmitz.org/" rel="external nofollow noopener" target="_blank">Martin Schmitz</a>, Jan Riemann, <em>Florian Müller</em>, Steffen Kreis, and <a href="https://www.informatik.tu-darmstadt.de/telekooperation/staff_tk/tk_staff_details_23168.en.jsp" rel="external nofollow noopener" target="_blank">Max Mühlhäuser</a> </div> <div class="periodical"> <em>In Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems</em>, May 2021 </div> <div class="periodical"> </div> <div class="links"> <a href="/assets/pdf/schmitzOhSnapFabrication2021.pdf" class="btn btn-sm z-depth-0" role="button"><i class="fa-solid fa-file-pdf"></i> PDF</a> <a href="https://dx.doi.org/10.1145/3411764.3445641" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="ai ai-acmdl"> ACM</i></a> <a href="https://www.youtube.com/watch?v=JX3ZwKnnJVs" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-youtube"></i> Video</a> <a href="https://www.youtube.com/watch?v=0AUrrtwaPVQ" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="fa-solid fa-person-chalkboard"></i> Talk</a> <a href="https://github.com/Telecooperation/oh-snap" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="fa-solid fa-code"></i> Code</a> </div> <div class="abstract hidden"> <p>3D printing has revolutionized rapid prototyping by speeding up the creation of custom-shaped objects. With the rise of multi-material 3D printers, these custom-shaped objects can now be made interactive in a single pass through passive conductive structures. However, connecting conventional electronics to these conductive structures often still requires time-consuming manual assembly involving many wires, soldering or gluing. To alleviate these shortcomings, we propose : a fabrication pipeline and interfacing concept to magnetically connect a 3D-printed object equipped with passive sensing structures to conventional sensing electronics. To this end, utilizes ferromagnetic and conductive 3D-printed structures, printable in a single pass on standard printers. We further present a proof-of-concept capacitive sensing board that enables easy and robust magnetic assembly to quickly create interactive 3D-printed objects. We evaluate by assessing the robustness and quality of the connection and demonstrate its broad applicability by a series of example applications.</p> </div> </div> </div> </li> </ol> <h2 class="bibliography">2020</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/Elsayed2020-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/Elsayed2020-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/Elsayed2020-1400.webp"></source> <img src="/assets/img/publication_preview/Elsayed2020.jpg?22b785b2a027d77e764e0342556e3317" class="preview z-depth-1 rounded" width="auto" height="auto" alt="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Elsayed2020" class="col-sm-8"> <div class="title"><a href="/publications/Elsayed2020">VRSketchPen: Unconstrained Haptic Assistance for Sketching in Virtual 3D Environments</a></div> <div class="author"> Hesham Elsayed, Mayra Donaji Barrera Machuca, Christian Schaarschmidt, <a href="https://informatik.rub.de/digisoul/personen/marky/" rel="external nofollow noopener" target="_blank">Karola Marky</a>, <em>Florian Müller</em>, Jan Riemann, <a href="http://andriimatviienko.com/" rel="external nofollow noopener" target="_blank">Andrii Matviienko</a>, <a href="https://mschmitz.org/" rel="external nofollow noopener" target="_blank">Martin Schmitz</a>, Martin Weigel, and <a href="https://www.informatik.tu-darmstadt.de/telekooperation/staff_tk/tk_staff_details_23168.en.jsp" rel="external nofollow noopener" target="_blank">Max Mühlhäuser</a> </div> <div class="periodical"> <em>In 26th ACM Symposium on Virtual Reality Software and Technology</em>, Nov 2020 </div> <div class="periodical"> </div> <div class="links"> <a href="/assets/pdf/Elsayed2020.pdf" class="btn btn-sm z-depth-0" role="button"><i class="fa-solid fa-file-pdf"></i> PDF</a> <a href="https://dx.doi.org/10.1145/3385956.3418953" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="ai ai-acmdl"> ACM</i></a> </div> <div class="abstract hidden"> <p>Accurate sketching in virtual 3D environments is challenging due to aspects like limited depth perception or the absence of physical support. To address this issue, we propose VRSketchPen - a pen that uses two haptic modalities to support virtual sketching without constraining user actions: (1) pneumatic force feedback to simulate the contact pressure of the pen against virtual surfaces and (2) vibrotactile feedback to mimic textures while moving the pen over virtual surfaces. To evaluate VRSketchPen, we conducted a lab experiment with 20 participants to compare (1) pneumatic, (2) vibrotactile and (3) a combination of both with (4) snapping and no assistance for flat and curved surfaces in a 3D virtual environment. Our findings show that usage of pneumatic, vibrotactile and their combination significantly improves 2D shape accuracy and leads to diminished depth errors for flat and curved surfaces. Qualitative results indicate that users find the addition of unconstraining haptic feedback to significantly improve convenience, confidence and user experience.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/elsayedVibroMapUnderstandingSpacing2020-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/elsayedVibroMapUnderstandingSpacing2020-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/elsayedVibroMapUnderstandingSpacing2020-1400.webp"></source> <img src="/assets/img/publication_preview/elsayedVibroMapUnderstandingSpacing2020.jpg?26d3c48ce59a922c4f3c7e995b8bae0b" class="preview z-depth-1 rounded" width="auto" height="auto" alt="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="elsayedVibroMapUnderstandingSpacing2020" class="col-sm-8"> <div class="title"><a href="/publications/elsayedVibroMapUnderstandingSpacing2020">VibroMap: Understanding the Spacing of Vibrotactile Actuators across the Body</a></div> <div class="author"> Hesham Elsayed, Martin Weigel, <em>Florian Müller</em>, <a href="https://mschmitz.org/" rel="external nofollow noopener" target="_blank">Martin Schmitz</a>, <a href="https://informatik.rub.de/digisoul/personen/marky/" rel="external nofollow noopener" target="_blank">Karola Marky</a>, <a href="https://www.sebastian-guenther.com/" rel="external nofollow noopener" target="_blank">Sebastian Günther</a>, Jan Riemann, and <a href="https://www.informatik.tu-darmstadt.de/telekooperation/staff_tk/tk_staff_details_23168.en.jsp" rel="external nofollow noopener" target="_blank">Max Mühlhäuser</a> </div> <div class="periodical"> <em>Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies</em>, Dec 2020 </div> <div class="periodical"> </div> <div class="links"> <a href="/assets/pdf/elsayedVibroMapUnderstandingSpacing2020.pdf" class="btn btn-sm z-depth-0" role="button"><i class="fa-solid fa-file-pdf"></i> PDF</a> <a href="https://dx.doi.org/10.1145/3432189" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="ai ai-acmdl"> ACM</i></a> <a href="https://www.youtube.com/watch?v=5vRKEkogg5A" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="fa-solid fa-person-chalkboard"></i> Talk</a> </div> <div class="abstract hidden"> <p>In spite of the great potential of on-body vibrotactile displays for a variety of applications, research lacks an understanding of the spacing between vibrotactile actuators. Through two experiments, we systematically investigate vibrotactile perception on the wrist, forearm, upper arm, back, torso, thigh, and leg, each in transverse and longitudinal body orientation. In the first experiment, we address the maximum distance between vibration motors that still preserves the ability to generate phantom sensations. In the second experiment, we investigate the perceptual accuracy of localizing vibrations in order to establish the minimum distance between vibration motors. Based on the results, we derive VibroMap, a spatial map of the functional range of inter-motor distances across the body. VibroMap supports hardware and interaction designers with design guidelines for constructing body-worn vibrotactile displays.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/Gunther2020-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/Gunther2020-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/Gunther2020-1400.webp"></source> <img src="/assets/img/publication_preview/Gunther2020.jpg?ee4d2cd607865dc09cef591c628bac0d" class="preview z-depth-1 rounded" width="auto" height="auto" alt="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Gunther2020" class="col-sm-8"> <div class="title"><a href="/publications/Gunther2020">Therminator : Understanding the Interdependency of Visual and On-Body Thermal Feedback in Virtual Reality</a></div> <div class="author"> <a href="https://www.sebastian-guenther.com/" rel="external nofollow noopener" target="_blank">Sebastian Günther</a>, Omar Elmoghazy, <em>Florian Müller</em>, <a href="https://www.informatik.tu-darmstadt.de/telekooperation/staff_tk/tk_staff_details_23168.en.jsp" rel="external nofollow noopener" target="_blank">Max Mühlhäuser</a>, Dominik Schön, and <a href="https://mschmitz.org/" rel="external nofollow noopener" target="_blank">Martin Schmitz</a> </div> <div class="periodical"> <em>In Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems - CHI ’20</em>, Dec 2020 </div> <div class="periodical"> </div> <div class="links"> <a href="/assets/pdf/Gunther2020.pdf" class="btn btn-sm z-depth-0" role="button"><i class="fa-solid fa-file-pdf"></i> PDF</a> <a href="https://dx.doi.org/10.1145/3313831.3376195" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="ai ai-acmdl"> ACM</i></a> <a href="https://www.youtube.com/watch?v=q5lkmqAua78" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-youtube"></i> Video</a> <a href="https://www.youtube.com/watch?v=p36TkvjTXfc" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="fa-solid fa-person-chalkboard"></i> Talk</a> </div> <div class="abstract hidden"> <p>Recent advances have made Virtual Reality (VR) more realistic than ever before. This improved realism is attributed to today’s ability to increasingly appeal to human sensations, such as visual, auditory or tactile. While research also examines temperature sensation as an important aspect, the interdependency of visual and thermal perception in VR is still underexplored. In this paper, we propose Therminator, a thermal display concept that provides warm and cold on-body feedback in VR through heat conduction of flowing liquids with different temperatures. Further, we systematically evaluate the interdependency of different visual and thermal stimuli on the temperature perception of arm and abdomen with 25 participants. As part of the results, we found varying temperature perception depending on the stimuli, as well as increasing involvement of users during conditions with matching stimuli.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/Gunther2020a-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/Gunther2020a-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/Gunther2020a-1400.webp"></source> <img src="/assets/img/publication_preview/Gunther2020a.jpg?5ac386b1e61417bda723ffac5a874a4a" class="preview z-depth-1 rounded" width="auto" height="auto" alt="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Gunther2020a" class="col-sm-8"> <div class="title"><a href="/publications/Gunther2020a">PneumoVolley: Pressure-based Haptic Feedback on the Head through Pneumatic Actuation</a></div> <div class="author"> <a href="https://www.sebastian-guenther.com/" rel="external nofollow noopener" target="_blank">Sebastian Günther</a>, Dominik Schön, <em>Florian Müller</em>, <a href="https://www.informatik.tu-darmstadt.de/telekooperation/staff_tk/tk_staff_details_23168.en.jsp" rel="external nofollow noopener" target="_blank">Max Mühlhäuser</a>, and <a href="https://mschmitz.org/" rel="external nofollow noopener" target="_blank">Martin Schmitz</a> </div> <div class="periodical"> <em>In Extended Abstracts of the 2020 CHI Conference on Human Factors in Computing Systems - CHI ’20</em>, Dec 2020 </div> <div class="periodical"> </div> <div class="links"> <a href="/assets/pdf/Gunther2020a.pdf" class="btn btn-sm z-depth-0" role="button"><i class="fa-solid fa-file-pdf"></i> PDF</a> <a href="https://dx.doi.org/10.1145/3334480.3382916" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="ai ai-acmdl"> ACM</i></a> <a href="https://www.youtube.com/watch?v=ZKnV8HrUx9M" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-youtube"></i> Video</a> <a href="https://www.youtube.com/watch?v=QWLsdfNxgeA" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="fa-solid fa-person-chalkboard"></i> Talk</a> </div> <div class="abstract hidden"> <p>Haptic Feedback brings immersion and presence in Virtual Reality (VR) to the next level. While research proposes the usage of various tactile sensations, such as vibration or ultrasound approaches, the potential applicability of pressure feedback on the head is still under-explored. In this paper, we contribute concepts and design considerations for pressure-based feedback on the head through pneumatic actuation. As a proof-of-concept implementing our pressure-based haptics, we further present PneumoVolley: a VR experience similar to the classic Volleyball game but played with the head. In an exploratory user study with 9 participants, we evaluated our concepts and identified a significantly increased involvement compared to a no-haptics baseline along with high realism and enjoyment ratings using pressure-based feedback on the head in VR. LBW033, Page 1 CHI 2020 Late-Breaking Work</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/meurischExploringUserExpectations2020-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/meurischExploringUserExpectations2020-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/meurischExploringUserExpectations2020-1400.webp"></source> <img src="/assets/img/publication_preview/meurischExploringUserExpectations2020.jpg?1f5fd35699404ccaad522d38549af4a4" class="preview z-depth-1 rounded" width="auto" height="auto" alt="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="meurischExploringUserExpectations2020" class="col-sm-8"> <div class="title"><a href="/publications/meurischExploringUserExpectations2020">Exploring User Expectations of Proactive AI Systems</a></div> <div class="author"> Christian Meurisch, Cristina A. Mihale-Wilson, Adrian Hawlitschek, Florian Giger, <em>Florian Müller</em>, Oliver Hinz, and <a href="https://www.informatik.tu-darmstadt.de/telekooperation/staff_tk/tk_staff_details_23168.en.jsp" rel="external nofollow noopener" target="_blank">Max Mühlhäuser</a> </div> <div class="periodical"> <em>Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies</em>, Dec 2020 </div> <div class="periodical"> </div> <div class="links"> <a href="/assets/pdf/meurischExploringUserExpectations2020.pdf" class="btn btn-sm z-depth-0" role="button"><i class="fa-solid fa-file-pdf"></i> PDF</a> <a href="https://dx.doi.org/10.1145/3432193" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="ai ai-acmdl"> ACM</i></a> </div> <div class="abstract hidden"> <p>Recent advances in artificial intelligence (AI) enabled digital assistants to evolve towards proactive user support. However, expectations as to when and to what extent assistants should take the initiative are still unclear; discrepancies to the actual system behavior might negatively affect user acceptance. In this paper, we present an in-the-wild study for exploring user expectations of such user-supporting AI systems in terms of different proactivity levels and use cases. We collected 3,168 in-situ responses from 272 participants through a mixed method of automated user tracking and context-triggered surveying. Using a data-driven approach, we gain insights into initial expectations and how they depend on different human factors and contexts. Our insights can help to design AI systems with varying degree of proactivity and preset to meet individual expectations.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/Mueller_Diss_published-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/Mueller_Diss_published-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/Mueller_Diss_published-1400.webp"></source> <img src="/assets/img/publication_preview/Mueller_Diss_published.jpg?a54ae344509c6900fd55061bc31edb35" class="preview z-depth-1 rounded" width="auto" height="auto" alt="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Mueller_Diss_published" class="col-sm-8"> <div class="title"><a href="/publications/Mueller_Diss_published">Around-Body Interaction: Leveraging Limb-movements for Interacting in a Digitally Augmented Physical World</a></div> <div class="author"> <em>Florian Müller</em> </div> <div class="periodical"> Dec 2020 </div> <div class="periodical"> </div> <div class="links"> <a href="/assets/pdf/Mueller_Diss_published.pdf" class="btn btn-sm z-depth-0" role="button"><i class="fa-solid fa-file-pdf"></i> PDF</a> <a href="https://dx.doi.org/10.25534/tuprints-00011388" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="ai ai-doi"> TUprints</i></a> </div> <div class="abstract hidden"> <p>Recent technological advances have made head-mounted displays (HMDs) smaller and untethered, fostering the vision of ubiquitous interaction with information in a digitally augmented physical world. For interacting with such devices, three main types of input - besides not very intuitive finger gestures - have emerged so far: 1) Touch input on the frame of the devices or 2) on accessories (controller) as well as 3) voice input. While these techniques have both advantages and disadvantages depending on the current situation of the user, they largely ignore the skills and dexterity that we show when interacting with the real world: Throughout our lives, we have trained extensively to use our limbs to interact with and manipulate the physical world around us. This thesis explores how the skills and dexterity of our upper and lower limbs, acquired and trained in interacting with the real world, can be transferred to the interaction with HMDs. Thereby, this thesis develops the vision of around-body interaction, in which we use the space around our body, defined by the reach of our limbs, for fast, accurate, and enjoyable interaction with information.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/Muller2020a-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/Muller2020a-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/Muller2020a-1400.webp"></source> <img src="/assets/img/publication_preview/Muller2020a.jpg?1e4602d02c2c5d74cbfbde505789534e" class="preview z-depth-1 rounded" width="auto" height="auto" alt="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Muller2020a" class="col-sm-8"> <div class="title"><a href="/publications/Muller2020a">Around-Body Interaction: Interacting While on the Go</a></div> <div class="author"> <em>Florian Müller</em>, <a href="https://www.sebastian-guenther.com/" rel="external nofollow noopener" target="_blank">Sebastian Günther</a>, and Max Mühlhauser</div> <div class="periodical"> <em>IEEE Pervasive Computing</em>, Apr 2020 </div> <div class="periodical"> </div> <div class="links"> <a href="https://dx.doi.org/10.1109/MPRV.2020.2977850" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="ai ai-ieee"> IEEE</i></a> </div> <div class="abstract hidden"> <p>We provide a concise overview on how the skills and dexterity of our upper and lower limbs, acquired and trained in interacting with the physical world, can be transferred to the interaction with HMD. We present the vision of around-body interaction, in which we use the space around our body, defined by the reach of our limbs, for fast, accurate, and enjoyable interactions. In the remainder of this article, we first introduce a design space for such around-body interactions and, second, present two examples of such interaction techniques, which use the degrees of freedom of the lower limbs for on-the go interactions. Finally, we conclude with future research challenges.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/Muller2020c-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/Muller2020c-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/Muller2020c-1400.webp"></source> <img src="/assets/img/publication_preview/Muller2020c.jpg?6b3ab2119a32993b25f67d6a21851de9" class="preview z-depth-1 rounded" width="auto" height="auto" alt="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Muller2020c" class="col-sm-8"> <div class="title"><a href="/publications/Muller2020c">Around-Body Interaction: Über Die Nutzung Der Bewegungen von Gliedmaßen Zur Interaktion in Einer Digital Erweiterten Physischen Welt</a></div> <div class="author"> <em>Florian Müller</em> </div> <div class="periodical"> <em>In Ausgezeichnete Informatikdissertationen 2019</em>, May 2020 </div> <div class="periodical"> </div> <div class="links"> <a href="/assets/pdf/Muller2020c.pdf" class="btn btn-sm z-depth-0" role="button"><i class="fa-solid fa-file-pdf"></i> PDF</a> </div> <div class="abstract hidden"> <p>Durch den technischen Fortschritt sind Head-Mounted Displays (HMDs) kleiner und kabellos geworden. Sie leisten so einen Beitrag zur Vision von allgegenwärtiger Interaktion mit Informationen in einer digital erweiterten physischen Welt. Zur Interaktion mit solchen Geräten werden heute eingabeseitig - neben wenig intuitiven Fingergesten in der Luft – vor allem dreierlei Techniken verwendet: 1) Toucheingabe auf dem Gehäuse der Geräte oder 2) auf Zubehör (Controller) sowie 3) Spracheingabe. Während diese Techniken, abhängig von der aktuellen Situation des Benutzers, sowohl Vor- als auch Nachteile haben, so ignorieren sie weitgehend die Fähigkeiten und Geschicklichkeit, die wir im Umgang mit der realen Welt zeigen: Während unseres ganzen Lebens haben wir ausgiebig trainiert unsere Gliedmaßen zu benutzen, um mit der physischen Welt um uns herum zu interagieren und sie zu manipulieren. Diese Arbeit entwickelt eine Vision für eine körperlichere Interaktion mit solchen Geräten, welche die Fähigkeiten und Geschicklichkeit, die wir im Umgang mit der physischen Welt zeigen, auf die Interaktion mit HMDs überträgt.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/Muller2020d-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/Muller2020d-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/Muller2020d-1400.webp"></source> <img src="/assets/img/publication_preview/Muller2020d.jpg?a6512e1e7ec1be26759b485bf96e9cf7" class="preview z-depth-1 rounded" width="auto" height="auto" alt="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Muller2020d" class="col-sm-8"> <div class="title"> <i class="fa-solid fa-award" style="color: var(--global-theme-color);"></i> <a href="/publications/Muller2020d">Walk The Line: Leveraging Lateral Shifts of the Walking Path as an Input Modality for Head-Mounted Displays</a> </div> <div class="author"> <em>Florian Müller</em>, <a href="https://mschmitz.org/" rel="external nofollow noopener" target="_blank">Martin Schmitz</a>, Daniel Schmitt, <a href="https://www.sebastian-guenther.com/" rel="external nofollow noopener" target="_blank">Sebastian Günther</a>, Markus Funk, and <a href="https://www.informatik.tu-darmstadt.de/telekooperation/staff_tk/tk_staff_details_23168.en.jsp" rel="external nofollow noopener" target="_blank">Max Mühlhäuser</a> </div> <div class="periodical"> <em>In Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems</em>, Apr 2020 </div> <div class="periodical"> </div> <div class="links"> <a href="/assets/pdf/Muller2020d.pdf" class="btn btn-sm z-depth-0" role="button"><i class="fa-solid fa-file-pdf"></i> PDF</a> <a href="https://dx.doi.org/10.1145/3313831.3376852" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="ai ai-acmdl"> ACM</i></a> <a href="https://www.youtube.com/watch?v=ylAlzFqWx7g" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-youtube"></i> Video</a> <a href="https://www.youtube.com/watch?v=uQ5w3Wvrb3w" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="fa-solid fa-person-chalkboard"></i> Talk</a> </div> <div class="abstract hidden"> <p>Recent technological advances have made head-mounted displays (HMDs) smaller and untethered, fostering the vision of ubiquitous interaction in a digitally augmented physical world. Consequently, a major part of the interaction with such devices will happen on the go, calling for interaction techniques that allow users to interact while walking. In this paper, we explore lateral shifts of the walking path as a hands-free input modality. The available input options are visualized as lanes on the ground parallel to the user’s walking path. Users can select options by shifting the walking path sideways to the respective lane. We contribute the results of a controlled experiment with 18 participants, confirming the viability of our approach for fast, accurate, and joyful interactions. Further, based on the findings of the controlled experiment, we present three example applications.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/Willich2020-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/Willich2020-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/Willich2020-1400.webp"></source> <img src="/assets/img/publication_preview/Willich2020.jpg?cece7832fa23b57f312564e0a134a9fb" class="preview z-depth-1 rounded" width="auto" height="auto" alt="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Willich2020" class="col-sm-8"> <div class="title"><a href="/publications/Willich2020">Podoportation: Foot-Based Locomotion in Virtual Reality</a></div> <div class="author"> Julius Von Willich, <a href="https://mschmitz.org/" rel="external nofollow noopener" target="_blank">Martin Schmitz</a>, <em>Florian Müller</em>, Daniel Schmitt, and <a href="https://www.informatik.tu-darmstadt.de/telekooperation/staff_tk/tk_staff_details_23168.en.jsp" rel="external nofollow noopener" target="_blank">Max Mühlhäuser</a> </div> <div class="periodical"> <em>In Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems - CHI ’20</em>, Apr 2020 </div> <div class="periodical"> </div> <div class="links"> <a href="/assets/pdf/Willich2020.pdf" class="btn btn-sm z-depth-0" role="button"><i class="fa-solid fa-file-pdf"></i> PDF</a> <a href="https://dx.doi.org/10.1145/3313831.3376626" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="ai ai-acmdl"> ACM</i></a> <a href="https://www.youtube.com/watch?v=xvqZTbJdXYE" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-youtube"></i> Video</a> <a href="https://www.youtube.com/watch?v=6c8JujTvVkY" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="fa-solid fa-person-chalkboard"></i> Talk</a> </div> <div class="abstract hidden"> <p>Virtual Reality (VR) allows for infinitely large environments. However, the physical traversable space is always limited by real-world boundaries. This discrepancy between physical and virtual dimensions renders traditional locomotion methods used in real world unfeasible. To alleviate these limitations, research proposed various artificial locomotion concepts such as teleportation, treadmills, and redirected walking. However, these concepts occupy the user’s hands, require complex hardware or large physical spaces. In this paper, we contribute nine VR locomotion concepts for foot-based locomotion, relying on the 3D position of the user’s feet and the pressure applied to the sole as input modalities. We evaluate our concepts and compare them to state-of-the-art point &amp; teleport technique in a controlled experiment with 20 participants. The results confirm the viability of our approaches for foot-based and engaging locomotion. Further, based on the findings, we contribute a wireless hardware prototype implementation.</p> </div> </div> </div> </li> </ol> <h2 class="bibliography">2019</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/Distante2019-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/Distante2019-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/Distante2019-1400.webp"></source> <img src="/assets/img/publication_preview/Distante2019.jpg?3154c5e68e16917e865f1dd8d2737ab3" class="preview z-depth-1 rounded" width="auto" height="auto" alt="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Distante2019" class="col-sm-8"> <div class="title"><a href="/publications/Distante2019">Trends on Engineering Interactive Systems: An Overview of Works Presented in Workshops at EICS 2019</a></div> <div class="author"> Damiano Distante, Alexandra Voit, Marco Winckler, Regina Bernhaupt, Judy Bowen, José Creissac Campos, <em>Florian Müller</em>, Philippe Palanque, Jan Van den Bergh, and Benjamin Weyers</div> <div class="periodical"> <em>In Proceedings of the ACM SIGCHI Symposium on Engineering Interactive Computing Systems - EICS ’19</em>, Apr 2019 </div> <div class="periodical"> </div> <div class="links"> <a href="/assets/pdf/Distante2019.pdf" class="btn btn-sm z-depth-0" role="button"><i class="fa-solid fa-file-pdf"></i> PDF</a> <a href="https://dx.doi.org/10.1145/3319499.3335655" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="ai ai-acmdl"> ACM</i></a> </div> <div class="abstract hidden"> <p>Workshops are a great opportunity for identifying innovative topics of research that might require discussion and maturation. This paper summarizes the outcomes of the workshops track of the 11th Engineering Interactive Computing Systems conference (EICS 2019), held in Valencia (Spain) on 18-21 June 2019. The track featured three workshops, one half-day, one full-day and one two-days workshop, each focused on specific topics of the ongoing research in engineering usable and effective interactive computing systems. In particular, the list of discussed topics include novel forms of interaction and emerging themes in HCI related to new application domains, more efficient and enjoyable interaction possibilities associated to smart objects and smart environments, challenges faced in designing, developing and using interactive systems involving multiple stakeholders.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/funkAssessingAccuracyPoint2019-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/funkAssessingAccuracyPoint2019-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/funkAssessingAccuracyPoint2019-1400.webp"></source> <img src="/assets/img/publication_preview/funkAssessingAccuracyPoint2019.jpg?902e8f9b060db9cc59629440574649b2" class="preview z-depth-1 rounded" width="auto" height="auto" alt="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="funkAssessingAccuracyPoint2019" class="col-sm-8"> <div class="title"><a href="/publications/funkAssessingAccuracyPoint2019">Assessing the Accuracy of Point &amp; Teleport Locomotion with Orientation Indication for Virtual Reality Using Curved Trajectories</a></div> <div class="author"> Markus Funk, <em>Florian Müller</em>, Marco Fendrich, Megan Shene, Moritz Kolvenbach, Niclas Dobbertin, <a href="https://www.sebastian-guenther.com/" rel="external nofollow noopener" target="_blank">Sebastian Günther</a>, and <a href="https://www.informatik.tu-darmstadt.de/telekooperation/staff_tk/tk_staff_details_23168.en.jsp" rel="external nofollow noopener" target="_blank">Max Mühlhäuser</a> </div> <div class="periodical"> <em>In Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems</em>, May 2019 </div> <div class="periodical"> </div> <div class="links"> <a href="/assets/pdf/funkAssessingAccuracyPoint2019.pdf" class="btn btn-sm z-depth-0" role="button"><i class="fa-solid fa-file-pdf"></i> PDF</a> <a href="https://dx.doi.org/10.1145/3290605.3300377" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="ai ai-acmdl"> ACM</i></a> <a href="https://www.youtube.com/watch?v=uXctClcQu_g" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-youtube"></i> Video</a> </div> <div class="abstract hidden"> <p>Room-scale Virtual Reality (VR) systems have arrived in users’ homes where tracked environments are set up in limited physical spaces. As most Virtual Environments (VEs) are larger than the tracked physical space, locomotion techniques are used to navigate in VEs. Currently, in recent VR games, point &amp; teleport is the most popular locomotion technique. However, it only allows users to select the position of the teleportation and not the orientation that the user is facing after the teleport. This results in users having to manually correct their orientation after teleporting and possibly getting entangled by the cable of the headset. In this paper, we introduce and evaluate three different point &amp; teleport techniques that enable users to specify the target orientation while teleporting. The results show that, although the three teleportation techniques with orientation indication increase the average teleportation time, they lead to a decreased need for correcting the orientation after teleportation.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/Gunther2019-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/Gunther2019-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/Gunther2019-1400.webp"></source> <img src="/assets/img/publication_preview/Gunther2019.jpg?c9f8b116660de3d551050c7ffe75b1c4" class="preview z-depth-1 rounded" width="auto" height="auto" alt="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Gunther2019" class="col-sm-8"> <div class="title"><a href="/publications/Gunther2019">Slappyfications: Towards Ubiquitous Physical and Embodied Notifications</a></div> <div class="author"> <a href="https://www.sebastian-guenther.com/" rel="external nofollow noopener" target="_blank">Sebastian Günther</a>, <em>Florian Müller</em>, Markus Funk, and <a href="https://www.informatik.tu-darmstadt.de/telekooperation/staff_tk/tk_staff_details_23168.en.jsp" rel="external nofollow noopener" target="_blank">Max Mühlhäuser</a> </div> <div class="periodical"> <em>In Extended Abstracts of the 2019 CHI Conference on Human Factors in Computing Systems - CHI ’19</em>, May 2019 </div> <div class="periodical"> </div> <div class="links"> <a href="/assets/pdf/Gunther2019.pdf" class="btn btn-sm z-depth-0" role="button"><i class="fa-solid fa-file-pdf"></i> PDF</a> <a href="https://dx.doi.org/10.1145/3290607.3311780" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="ai ai-acmdl"> ACM</i></a> <a href="https://www.youtube.com/watch?v=qDmrSgyV20s" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-youtube"></i> Video</a> </div> <div class="abstract hidden"> <p>With emerging trends of notifying persons through ubiquitous technologies [2], such as ambient light, vibrotactile, or auditory cues, none of these technologies are truly ubiquitous and have proven to be easily missed or ignored. In this work, we propose Slappyfications, a novel way of sending unmissable embodied and ubiquitous notifications through a palm-based interface [1]. Our prototype enables the users to send three types of Slappyfications: poke, slap, and the STEAM-HAMMER. Through a Wizard-of-Oz study, we show the applicability of our system in real-world scenarios. The results reveal a promising trend, as none of the participants missed a single Slappyfication.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/Gunther2019pneumact-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/Gunther2019pneumact-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/Gunther2019pneumact-1400.webp"></source> <img src="/assets/img/publication_preview/Gunther2019pneumact.jpg?2f255901283c5a0c2bea7f0408be89a6" class="preview z-depth-1 rounded" width="auto" height="auto" alt="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Gunther2019pneumact" class="col-sm-8"> <div class="title"><a href="/publications/Gunther2019pneumact">PneumAct: Pneumatic Kinesthetic Actuation of Body Joints in Virtual Reality Environments</a></div> <div class="author"> <a href="https://www.sebastian-guenther.com/" rel="external nofollow noopener" target="_blank">Sebastian Günther</a>, Mohit Makhija, <em>Florian Müller</em>, Dominik Schön, <a href="https://www.informatik.tu-darmstadt.de/telekooperation/staff_tk/tk_staff_details_23168.en.jsp" rel="external nofollow noopener" target="_blank">Max Mühlhäuser</a>, and Markus Funk</div> <div class="periodical"> <em>In Proceedings of the 2019 on Designing Interactive Systems Conference</em>, Jun 2019 </div> <div class="periodical"> </div> <div class="links"> <a href="/assets/pdf/Gunther2019pneumact.pdf" class="btn btn-sm z-depth-0" role="button"><i class="fa-solid fa-file-pdf"></i> PDF</a> <a href="https://dx.doi.org/10.1145/3322276.3322302" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="ai ai-acmdl"> ACM</i></a> <a href="https://www.youtube.com/watch?v=4lRWxzs4Rgs" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-youtube"></i> Video</a> </div> <div class="abstract hidden"> <p>Virtual Reality Environments (VRE) create an immersive user experience through visual, aural, and haptic sensations. However, the latter is often limited to vibrotactile sensations that are not able to actively provide kinesthetic motion actuation. Further, such sensations do not cover natural representations of physical forces, for example, when lifting a weight. We present PneumAct, a jacket to enable pneumatically actuated kinesthetic movements of arm joints in VRE. It integrates two types of actuators inflated through compressed air: a Contraction Actuator and an Extension Actuator. We evaluate our PneumAct jacket through two user studies with a total of 32 participants: First, we perform a technical evaluation measuring the contraction and extension angles of different inflation patterns and inflation durations. Second, we evaluate PneumAct in three VRE scenarios comparing our system to traditional controller-based vibrotactile and a baseline without haptic feedback.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/Muller2019-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/Muller2019-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/Muller2019-1400.webp"></source> <img src="/assets/img/publication_preview/Muller2019.jpg?20c959ea24bd7d67fcd430a10bfa54fb" class="preview z-depth-1 rounded" width="auto" height="auto" alt="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Muller2019" class="col-sm-8"> <div class="title"><a href="/publications/Muller2019">Mind the Tap: Assessing Foot-Taps for Interacting with Head-Mounted Displays</a></div> <div class="author"> <em>Florian Müller</em>, Joshua McManus, <a href="https://www.sebastian-guenther.com/" rel="external nofollow noopener" target="_blank">Sebastian Günther</a>, <a href="https://mschmitz.org/" rel="external nofollow noopener" target="_blank">Martin Schmitz</a>, <a href="https://www.informatik.tu-darmstadt.de/telekooperation/staff_tk/tk_staff_details_23168.en.jsp" rel="external nofollow noopener" target="_blank">Max Mühlhäuser</a>, and Markus Funk</div> <div class="periodical"> <em>In Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems - CHI ’19</em>, Jun 2019 </div> <div class="periodical"> </div> <div class="links"> <a href="/assets/pdf/Muller2019.pdf" class="btn btn-sm z-depth-0" role="button"><i class="fa-solid fa-file-pdf"></i> PDF</a> <a href="https://dx.doi.org/10.1145/3290605.3300707" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="ai ai-acmdl"> ACM</i></a> <a href="https://www.youtube.com/watch?v=D5hTVIEb7iA" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-youtube"></i> Video</a> </div> <div class="abstract hidden"> <p>From voice commands and air taps to touch gestures on frames: Various techniques for interacting with head-mounted displays (HMDs) have been proposed. While these techniques have both benefits and drawbacks dependent on the current situation of the user, research on interacting with HMDs has not concluded yet. In this paper, we add to the body of research on interacting with HMDs by exploring foot-tapping as an input modality. Through two controlled experiments with a total of 36 participants, we first explore direct interaction with interfaces that are displayed on the floor and require the user to look down to interact. Secondly, we investigate indirect interaction with interfaces that, although operated by the user’s feet, are always visible as they are floating in front of the user. Based on the results of the two experiments, we provide design recommendations for direct and indirect foot-based user interfaces.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/Muller2019soproceedings-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/Muller2019soproceedings-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/Muller2019soproceedings-1400.webp"></source> <img src="/assets/img/publication_preview/Muller2019soproceedings.jpg?3154c5e68e16917e865f1dd8d2737ab3" class="preview z-depth-1 rounded" width="auto" height="auto" alt="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Muller2019soproceedings" class="col-sm-8"> <div class="title"><a href="/publications/Muller2019soproceedings">Proceedings of the 7th Workshop on Interacting with Smart Objects</a></div> <div class="author"> </div> <div class="periodical"> Jun 2019 </div> <div class="periodical"> </div> <div class="links"> <a href="/assets/pdf/Muller2019soproceedings.pdf" class="btn btn-sm z-depth-0" role="button"><i class="fa-solid fa-file-pdf"></i> PDF</a> </div> <div class="abstract hidden"> <p>These are the proceedings of the 7th Workshop on Interacting with Smart Objects (SmartObjects ’19) in conjunction with EICS’19 held on June 18, 2019, in Valencia, Spain. This volume contains the ten accepted papers. Each submission was reviewed by three program committee members. Objects that we use in our everyday life are ever-expanding their interaction capabilities and provide functionalities that go far beyond their original functionality. They feature computing capabilities and are, thus, able to capture, process and store information and interact with their environments, turning them into smart objects. Their wide range was covered by the submissions to this workshop. Smart objects know something about their users and, thus, allow for natural interaction. Natural interaction, in contrast, does not imply smartness. Smartness requires interaction with users and provides help. There are already commercialized products available that expose their properties and interaction capabilities. To enrich their potential and to lower affordances, they need to communicate to each other. Making sense out of the available data in this field is still an open research question. The overall goal should be to build an interactive ecosystem that (i) seamlessly discovers, connects and talks to its environment, (ii) is ubiquitous and (iii) allows the user to be in control. The workshop examined these issues with regards to the following aspects: ∙ Smart Devices ∙ Smart Spaces Putting together SmartObjects ‘19 was a team effort. We would like to send out our thanks to everybody who has helped us to organize this event: ∙ The authors, who have written and submitted their papers to the workshop. ∙ The program committee and the external reviewers, for their time and effort to write substantial and constructive review reports. We hope that you will find this program interesting and thought-provoking and that the workshop will provide you with a valuable opportunity to share ideas with other researchers and practitioners from institutions around the world.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/SchmitzMartinStitzFlorianMuller2019-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/SchmitzMartinStitzFlorianMuller2019-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/SchmitzMartinStitzFlorianMuller2019-1400.webp"></source> <img src="/assets/img/publication_preview/SchmitzMartinStitzFlorianMuller2019.jpg?6f9a6503f3245a05a3f7194d9c6ad258" class="preview z-depth-1 rounded" width="auto" height="auto" alt="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="SchmitzMartinStitzFlorianMuller2019" class="col-sm-8"> <div class="title"><a href="/publications/SchmitzMartinStitzFlorianMuller2019">./Trilaterate: A Fabrication Pipeline to Design and 3D Print Hover-, Touch-, and Force-Sensitive Objects</a></div> <div class="author"> <a href="https://mschmitz.org/" rel="external nofollow noopener" target="_blank">Martin Schmitz</a>, Martin Stitz, <em>Florian Müller</em>, Markus Funk, and <a href="https://www.informatik.tu-darmstadt.de/telekooperation/staff_tk/tk_staff_details_23168.en.jsp" rel="external nofollow noopener" target="_blank">Max Mühlhäuser</a> </div> <div class="periodical"> <em>In Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems - CHI ’19</em>, Jun 2019 </div> <div class="periodical"> </div> <div class="links"> <a href="/assets/pdf/SchmitzMartinStitzFlorianMuller2019.pdf" class="btn btn-sm z-depth-0" role="button"><i class="fa-solid fa-file-pdf"></i> PDF</a> <a href="https://dx.doi.org/10.1145/3290605.3300684" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="ai ai-acmdl"> ACM</i></a> <a href="https://www.youtube.com/watch?v=4BT6Nw2kWPs" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-youtube"></i> Video</a> </div> <div class="abstract hidden"> <p>Hover, touch, and force are promising input modalities that get increasingly integrated into screens and everyday objects. However, these interactions are often limited to fat surfaces and the integration of suitable sensors is time-consuming and costly. To alleviate these limitations, we contribute Tri-laterate: A fabrication pipeline to 3D print custom objects that detect the 3D position of a fnger hovering, touching, or forcing them by combining multiple capacitance measurements via capacitive trilateration. Trilaterate places and routes actively-shielded sensors inside the object and operates on consumer-level 3D printers. We present technical evaluations and example applications that validate and demonstrate the wide applicability of Trilaterate. CCS CONCEPTS ∙ Human-centered computing → Interaction devices; ∙ Hardware → Tactile and hand-based interfaces;</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/VonWillich2019a-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/VonWillich2019a-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/VonWillich2019a-1400.webp"></source> <img src="/assets/img/publication_preview/VonWillich2019a.jpg?8a30f41c013ba2f5ccfd3c5a5b908302" class="preview z-depth-1 rounded" width="auto" height="auto" alt="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="VonWillich2019a" class="col-sm-8"> <div class="title"><a href="/publications/VonWillich2019a">VRChairRacer: Using an Office Chair Backrest as a Locomotion Technique for VR Racing Games</a></div> <div class="author"> Julius von Willich, Dominik Schön, <a href="https://www.sebastian-guenther.com/" rel="external nofollow noopener" target="_blank">Sebastian Günther</a>, <em>Florian Müller</em>, <a href="https://www.informatik.tu-darmstadt.de/telekooperation/staff_tk/tk_staff_details_23168.en.jsp" rel="external nofollow noopener" target="_blank">Max Mühlhäuser</a>, and Markus Funk</div> <div class="periodical"> <em>In Extended Abstracts of the 2019 CHI Conference on Human Factors in Computing Systems - CHI ’19</em>, Jun 2019 </div> <div class="periodical"> </div> <div class="links"> <a href="/assets/pdf/VonWillich2019a.pdf" class="btn btn-sm z-depth-0" role="button"><i class="fa-solid fa-file-pdf"></i> PDF</a> <a href="https://dx.doi.org/10.1145/3290607.3313254" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="ai ai-acmdl"> ACM</i></a> <a href="https://www.youtube.com/watch?v=v906aGntoKY" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-youtube"></i> Video</a> </div> <div class="abstract hidden"> <p>Locomotion in Virtual Reality (VR) is an important topic as there is a mismatch between the size of a Virtual Environment and the physically available tracking space. Although many locomotion techniques have been proposed, research on VR locomotion has not concluded yet. In this demonstration, we contribute to the area of VR locomotion by introducing VRChairRacer. VRChairRacer introduces a novel mapping the velocity of a racing cart on the backrest of an office chair. Further, it maps a users’ rotation onto the steering of a virtual racing cart. VRChairRacer demonstrates this locomotion technique to the community through an immersive multiplayer racing demo.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/vonwillichYouInvadedMy2019-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/vonwillichYouInvadedMy2019-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/vonwillichYouInvadedMy2019-1400.webp"></source> <img src="/assets/img/publication_preview/vonwillichYouInvadedMy2019.jpg?6123cb4d65e5b81fb89a95e81ccd3ba9" class="preview z-depth-1 rounded" width="auto" height="auto" alt="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="vonwillichYouInvadedMy2019" class="col-sm-8"> <div class="title"><a href="/publications/vonwillichYouInvadedMy2019">You Invaded My Tracking Space! Using Augmented Virtuality for Spotting Passersby in Room-Scale Virtual Reality</a></div> <div class="author"> Julius von Willich, Markus Funk, <em>Florian Müller</em>, <a href="https://informatik.rub.de/digisoul/personen/marky/" rel="external nofollow noopener" target="_blank">Karola Marky</a>, Jan Riemann, and <a href="https://www.informatik.tu-darmstadt.de/telekooperation/staff_tk/tk_staff_details_23168.en.jsp" rel="external nofollow noopener" target="_blank">Max Mühlhäuser</a> </div> <div class="periodical"> <em>In Proceedings of the 2019 on Designing Interactive Systems Conference - DIS ’19</em>, Jun 2019 </div> <div class="periodical"> </div> <div class="links"> <a href="/assets/pdf/vonwillichYouInvadedMy2019.pdf" class="btn btn-sm z-depth-0" role="button"><i class="fa-solid fa-file-pdf"></i> PDF</a> <a href="https://dx.doi.org/10.1145/3322276.3322334" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="ai ai-acmdl"> ACM</i></a> <a href="https://www.youtube.com/watch?v=SGOFeRX0tmk" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-youtube"></i> Video</a> </div> <div class="abstract hidden"> <p>With the proliferation of room-scale Virtual Reality (VR), more and more users install a VR system in their homes. When users are in VR, they are usually completely immersed in their application. However, sometimes passersby invade these tracking spaces and walk up to users that are currently immersed in VR to try and interact with them. As this either scares the user in VR or breaks the user’s immersion, research has yet to find a way to seamlessly represent physical passersby in virtual worlds. In this paper, we propose and evaluate three different ways to represent physical passersby in a Virtual Environment using Augmented Virtuality. The representations encompass showing a Pointcloud, showing a 3D-Model, and showing an Image Overlay of the passerby. Our results show that while an Image Overlay and a 3D-Model are the fastest representations to spot passersby, the 3D-Model and the Pointcloud representations were the most accurate.</p> </div> </div> </div> </li> </ol> <h2 class="bibliography">2018</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/gunther2018checkmate-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/gunther2018checkmate-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/gunther2018checkmate-1400.webp"></source> <img src="/assets/img/publication_preview/gunther2018checkmate.jpg?092ccd17ac8edfc60bd4f5e0377ab59b" class="preview z-depth-1 rounded" width="auto" height="auto" alt="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="gunther2018checkmate" class="col-sm-8"> <div class="title"><a href="/publications/gunther2018checkmate">CheckMate: Exploring a Tangible Augmented Reality Interface for Remote Interaction</a></div> <div class="author"> <a href="https://www.sebastian-guenther.com/" rel="external nofollow noopener" target="_blank">Sebastian Günther</a>, <em>Florian Müller</em>, <a href="https://mschmitz.org/" rel="external nofollow noopener" target="_blank">Martin Schmitz</a>, Jan Riemann, Niloofar Dezfuli, Markus Funk, Dominik Schön, and <a href="https://www.informatik.tu-darmstadt.de/telekooperation/staff_tk/tk_staff_details_23168.en.jsp" rel="external nofollow noopener" target="_blank">Max Mühlhäuser</a> </div> <div class="periodical"> <em>In Extended Abstracts of the 2018 CHI Conference on Human Factors in Computing Systems</em>, Apr 2018 </div> <div class="periodical"> </div> <div class="links"> <a href="/assets/pdf/gunther2018checkmate.pdf" class="btn btn-sm z-depth-0" role="button"><i class="fa-solid fa-file-pdf"></i> PDF</a> <a href="https://dx.doi.org/10.1145/3170427.3188647" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="ai ai-acmdl"> ACM</i></a> <a href="https://www.youtube.com/watch?v=ZjG8n9P5sD8" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-youtube"></i> Video</a> </div> <div class="abstract hidden"> <p>The digitalized world comes with increasing Internet capabilities, allowing to connect persons over distance easier than ever before. Video conferencing and similar online applications create great benefits bringing people who physically cannot spend as much time as they want virtually together. However, such remote experiences can also tend to lose the feeling of traditional experiences. People lack direct visual presence and no haptic feedback is available. In this paper, we tackle this problem by introducing our system called CheckMate. We combine Augmented Reality and capacitive 3D printed objects that can be sensed on an interactive surface to enable remote interaction while providing the same tangible experience as in co-located scenarios. As a proof-of-concept, we implemented a sample application based on the traditional chess game.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/gunther2018tactileglove-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/gunther2018tactileglove-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/gunther2018tactileglove-1400.webp"></source> <img src="/assets/img/publication_preview/gunther2018tactileglove.jpg?8708c544f2b74162834d017d4f79ecf8" class="preview z-depth-1 rounded" width="auto" height="auto" alt="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="gunther2018tactileglove" class="col-sm-8"> <div class="title"><a href="/publications/gunther2018tactileglove">TactileGlove: Assistive Spatial Guidance in 3D Space through Vibrotactile Navigation</a></div> <div class="author"> <a href="https://www.sebastian-guenther.com/" rel="external nofollow noopener" target="_blank">Sebastian Günther</a>, <em>Florian Müller</em>, Markus Funk, Jan Kirchner, Niloofar Dezfuli, and <a href="https://www.informatik.tu-darmstadt.de/telekooperation/staff_tk/tk_staff_details_23168.en.jsp" rel="external nofollow noopener" target="_blank">Max Mühlhäuser</a> </div> <div class="periodical"> <em>In Proceedings of the 11th PErvasive Technologies Related to Assistive Environments Conference</em>, Jun 2018 </div> <div class="periodical"> </div> <div class="links"> <a href="/assets/pdf/gunther2018tactileglove.pdf" class="btn btn-sm z-depth-0" role="button"><i class="fa-solid fa-file-pdf"></i> PDF</a> <a href="https://dx.doi.org/10.1145/3197768.3197785" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="ai ai-acmdl"> ACM</i></a> </div> <div class="abstract hidden"> <p>With the recent advance in computing technology, more and more environments are becoming interactive. For interacting with these environments, traditionally 2D input and output elements are being used. However, recently interaction spaces also expanded to 3D space, which enabled new possibilities but also led to challenges in assisting users with interacting in such a 3D space. Usually, this challenge of communicating 3D positions is solved visually. This paper explores a different approach: spatial guidance through vibrotactile instructions. Therefore, we introduce TactileGlove, a smart glove equipped with vibrotactile actuators for providing spatial guidance in 3D space. We contribute a user study with 15 participants to explore how a different number of actuators and metaphors affect the user performance. As a result, we found that using a Pull metaphor for vibrotactile navigation instructions is preferred by our participants. Further, we found that using a higher number of actuators reduces the target acquisition time than when using a low number.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/Meurisch2018-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/Meurisch2018-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/Meurisch2018-1400.webp"></source> <img src="/assets/img/publication_preview/Meurisch2018.jpg?d6a80e8011a2d9e66c8e4caced93e6cc" class="preview z-depth-1 rounded" width="auto" height="auto" alt="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Meurisch2018" class="col-sm-8"> <div class="title"><a href="/publications/Meurisch2018">UPA’18: 3rd International Workshop on Ubiquitous Personal Assistance</a></div> <div class="author"> Christian Meurisch, <a href="https://www.informatik.tu-darmstadt.de/telekooperation/staff_tk/tk_staff_details_23168.en.jsp" rel="external nofollow noopener" target="_blank">Max Mühlhäuser</a>, Philipp M. Scholl, Usman Naeem, Veljko Pejović, <em>Florian Müller</em>, Elena Di Lascio, Pei-Yi Patricia Kuo, Sebastian Kauschke, and Muhammad Awais Azam</div> <div class="periodical"> <em>In Proceedings of the 2018 ACM International Joint Conference and 2018 International Symposium on Pervasive and Ubiquitous Computing and Wearable Computers - UbiComp ’18</em>, Jun 2018 </div> <div class="periodical"> </div> <div class="links"> <a href="/assets/pdf/Meurisch2018.pdf" class="btn btn-sm z-depth-0" role="button"><i class="fa-solid fa-file-pdf"></i> PDF</a> <a href="https://dx.doi.org/10.1145/3267305.3274133" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="ai ai-acmdl"> ACM</i></a> </div> <div class="abstract hidden"> <p>Advancements in ubiquitous technologies and artificial intelligence have paved the way for the recent rise of digital personal assistants in everyday life. The Third International Workshop on Ubiquitous Personal Assistance (UPA’18) aims to build on the success of our both previous workshops (namely SmartGuidance), organized in conjunction with UbiComp’16/17, to continue discussing the latest research outcomes of digital personal assistants. We invite the submission of papers within this emerging, interdisciplinary research field of ubiquitous personal assistance that focuses on understanding, design, and development of such digital helpers. We also welcome contributions that investigate human behaviors, underlying recognition, and prediction models; conduct field studies; as well as propose novel HCI techniques to provide personal assistance. All workshop contributions will be published in the supplemental proceedings of the UbiComp’18 conference and included in the ACM Digital Library.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/muller2018camea-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/muller2018camea-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/muller2018camea-1400.webp"></source> <img src="/assets/img/publication_preview/muller2018camea.jpg?5161828203c956ff7ab54c9dd4bb379a" class="preview z-depth-1 rounded" width="auto" height="auto" alt="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="muller2018camea" class="col-sm-8"> <div class="title"><a href="/publications/muller2018camea">CaMea: Camera-Supported Workpiece Measurement for CNC Milling Machines</a></div> <div class="author"> <em>Florian Müller</em>, Maximilian Barnikol, Markus Funk, <a href="https://mschmitz.org/" rel="external nofollow noopener" target="_blank">Martin Schmitz</a>, and <a href="https://www.informatik.tu-darmstadt.de/telekooperation/staff_tk/tk_staff_details_23168.en.jsp" rel="external nofollow noopener" target="_blank">Max Mühlhäuser</a> </div> <div class="periodical"> <em>In Proceedings of the 11th PErvasive Technologies Related to Assistive Environments Conference on - PETRA ’18</em>, Jun 2018 </div> <div class="periodical"> </div> <div class="links"> <a href="/assets/pdf/muller2018camea.pdf" class="btn btn-sm z-depth-0" role="button"><i class="fa-solid fa-file-pdf"></i> PDF</a> <a href="https://dx.doi.org/10.1145/3197768.3201569" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="ai ai-acmdl"> ACM</i></a> <a href="https://www.youtube.com/watch?v=jzBM_lchKtg" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-youtube"></i> Video</a> </div> <div class="abstract hidden"> <p>We are experiencing a trend of personal fabrication that allows non-experts to produce highly individualized objects. Beyond 3D printing, this maker movement also approaches larger-scale production machines such as computer numerically controlled (CNC) milling machines that are available in local fabrication laboratories (FabLabs). While the user interfaces and interaction techniques of small-scale 3D printers for household use adapted to the new requirements of non-experts in the last years, such an overhaul of the interfaces for larger machinery is still missing. In this work, we explore the use of augmented reality methods to support novice users in the operation of CNC milling machines. As a first step towards better support for users, we provide a camera-supported graphical and easy-to-use interface for the measurement of raw workpieces inside the machine. In this paper, we contribute our concept CaMea alongside its’ prototype implementation. We further report on the findings of a first early user study.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/muller2018personalized-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/muller2018personalized-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/muller2018personalized-1400.webp"></source> <img src="/assets/img/publication_preview/muller2018personalized.jpg?a828d6066254180563ebc200b1d589d1" class="preview z-depth-1 rounded" width="auto" height="auto" alt="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="muller2018personalized" class="col-sm-8"> <div class="title"><a href="/publications/muller2018personalized">Personalized User-Carried Single Button Interfaces as Shortcuts for Interacting with Smart Devices</a></div> <div class="author"> <em>Florian Müller</em>, <a href="https://mschmitz.org/" rel="external nofollow noopener" target="_blank">Martin Schmitz</a>, Markus Funk, <a href="https://www.sebastian-guenther.com/" rel="external nofollow noopener" target="_blank">Sebastian Günther</a>, Niloofar Dezfuli, and <a href="https://www.informatik.tu-darmstadt.de/telekooperation/staff_tk/tk_staff_details_23168.en.jsp" rel="external nofollow noopener" target="_blank">Max Mühlhäuser</a> </div> <div class="periodical"> <em>In Extended Abstracts of the 2018 CHI Conference on Human Factors in Computing Systems - CHI ’18</em>, Jun 2018 </div> <div class="periodical"> </div> <div class="links"> <a href="/assets/pdf/muller2018personalized.pdf" class="btn btn-sm z-depth-0" role="button"><i class="fa-solid fa-file-pdf"></i> PDF</a> <a href="https://dx.doi.org/10.1145/3170427.3188661" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="ai ai-acmdl"> ACM</i></a> <a href="https://www.youtube.com/watch?v=3wNXOfMofXw" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-youtube"></i> Video</a> </div> <div class="abstract hidden"> <p>We are experiencing a trend of integrating computing functionality into more and more common and popular devices. While these so-called smart devices offer many possibilities for automation and personalization of everyday routines, interacting with them and customizing them requires either programming efforts or a smartphone app to control the devices. In this work, we propose and classify Personalized User-Carried Single Button Interfaces as shortcuts for interacting with smart devices. We implement a proof-of-concept of such an interface for a coffee machine. Through an in-the-wild deployment of the coffee machine for approximately three months, we report first initial experiences from 40 participants of using PUCSBIs for interacting with smart devices.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/muller2018smartobjects-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/muller2018smartobjects-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/muller2018smartobjects-1400.webp"></source> <img src="/assets/img/publication_preview/muller2018smartobjects.jpg?a5739719af643c8b4d56d4456f894dc7" class="preview z-depth-1 rounded" width="auto" height="auto" alt="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="muller2018smartobjects" class="col-sm-8"> <div class="title"><a href="/publications/muller2018smartobjects">SmartObjects: Sixth Workshop on Interacting with Smart Objects</a></div> <div class="author"> <em>Florian Müller</em>, Dirk Schnelle-Walka, Tobias Grosse-Puppendahl, <a href="https://www.sebastian-guenther.com/" rel="external nofollow noopener" target="_blank">Sebastian Günther</a>, Markus Funk, Kris Luyten, Oliver Brdiczka, Niloofar Dezfuli, and <a href="https://www.informatik.tu-darmstadt.de/telekooperation/staff_tk/tk_staff_details_23168.en.jsp" rel="external nofollow noopener" target="_blank">Max Mühlhäuser</a> </div> <div class="periodical"> <em>In Extended Abstracts of the 2018 CHI Conference on Human Factors in Computing Systems - CHI ’18</em>, Jun 2018 </div> <div class="periodical"> </div> <div class="links"> <a href="/assets/pdf/muller2018smartobjects.pdf" class="btn btn-sm z-depth-0" role="button"><i class="fa-solid fa-file-pdf"></i> PDF</a> <a href="https://dx.doi.org/10.1145/3170427.3170606" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="ai ai-acmdl"> ACM</i></a> </div> <div class="abstract hidden"> <p>Smart objects are everyday objects that have computing capabilities and give rise to new ways of interaction with our environment. The increasing number of smart objects in our life shapes how we interact beyond the desktop. In this workshop we explore various aspects of the design, development and deployment of smart objects including how one can interact with smart objects.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/murauer2018analysis-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/murauer2018analysis-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/murauer2018analysis-1400.webp"></source> <img src="/assets/img/publication_preview/murauer2018analysis.jpg?e4798ef04712d115e1631fd57b29f4ce" class="preview z-depth-1 rounded" width="auto" height="auto" alt="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="murauer2018analysis" class="col-sm-8"> <div class="title"><a href="/publications/murauer2018analysis">An Analysis of Language Impact on Augmented Reality Order Picking Training</a></div> <div class="author"> Nela Murauer, <em>Florian Müller</em>, <a href="https://www.sebastian-guenther.com/" rel="external nofollow noopener" target="_blank">Sebastian Günther</a>, Dominik Schön, Nerina Pflanz, and Markus Funk</div> <div class="periodical"> <em>In Proceedings of the 11th PErvasive Technologies Related to Assistive Environments Conference</em>, Jun 2018 </div> <div class="periodical"> </div> <div class="links"> <a href="/assets/pdf/murauer2018analysis.pdf" class="btn btn-sm z-depth-0" role="button"><i class="fa-solid fa-file-pdf"></i> PDF</a> <a href="https://dx.doi.org/10.1145/3197768.3201570" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="ai ai-acmdl"> ACM</i></a> </div> <div class="abstract hidden"> <p>Order picking is a difficult and cognitively demanding task. Traditionally textual instructions are helping new workers to learn different picking routines. However, the textual instructions are sometimes not written in the workers ’ native languages. In the area of Industry 4.0, where digital functions are finding their way into manufacturing processes, language-independent instructions are possible. Through a user study with 15 participants, we compare textual feedback in the workers’ native language, textual feedback that is written in an unknown foreign language, and visual Augmented Reality (AR) feedback. We found that AR feedback is significantly faster and leads to a lower perceived cognitive load.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/SmartObjects2018proceedings-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/SmartObjects2018proceedings-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/SmartObjects2018proceedings-1400.webp"></source> <img src="/assets/img/publication_preview/SmartObjects2018proceedings.jpg?a5739719af643c8b4d56d4456f894dc7" class="preview z-depth-1 rounded" width="auto" height="auto" alt="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="SmartObjects2018proceedings" class="col-sm-8"> <div class="title"><a href="/publications/SmartObjects2018proceedings">Proceedings of the 6th Workshop on Interacting with Smart Objects (SmartObjects)</a></div> <div class="author"> </div> <div class="periodical"> Jun 2018 </div> <div class="periodical"> </div> <div class="links"> <a href="/assets/pdf/SmartObjects2018proceedings.pdf" class="btn btn-sm z-depth-0" role="button"><i class="fa-solid fa-file-pdf"></i> PDF</a> <a href="https://ceur-ws.org/Vol-2082/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="fas fa-link"></i> Website</a> </div> <div class="abstract hidden"> <p>These are the proceedings of the 6th Workshop on Interacting with Smart Objects (SmartObjects ’18) in conjunction with CHI’18 held on April 21, 2018, in Montreal, Canada. This volume contains the ten accepted papers. Each submission was reviewed by three program committee members. Objects that we use in our everyday life are ever-expanding their interaction capabilities and provide functionalities that go far beyond their original functionality. They feature computing capabilities and are, thus, able to capture, process and store information and interact with their environments, turning them into smart objects. Their wide range was covered by the submissions to this workshop. Smart objects know something about their users and, thus, allow for natural interaction. Natural interaction, in contrast, does not imply smartness. Smartness requires interaction with users and provides help. There are already commercialized products available that expose their properties and interaction capabilities. To enrich their potential and to lower affordances, they need to communicate to each other. Making sense out of the available data in this field is still an open research question. The overall goal should be to build an interactive ecosystem that (i) seamlessly discovers, connects and talks to its environment, (ii) is ubiquitous and (iii) allows the user to be in control. The workshop examined these issues with regards to the following aspects: ∙ Interactive Experiences ∙ The Future of IoT ∙ Smart Home ∙ AR in the Industry Putting together SmartObjects ‘18 was a team effort. We would like to send out our thanks to everybody who has helped us to organize this event: ∙ The authors, who have written and submitted their papers to the workshop. ∙ The program committee and the external reviewers, for their time and effort to write substantial and constructive review reports. We hope that you will find this program interesting and thought-provoking and that the workshop will provide you with a valuable opportunity to share ideas with other researchers and practitioners from institutions around the world.</p> </div> </div> </div> </li> </ol> <h2 class="bibliography">2017</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/gunther2017byo-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/gunther2017byo-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/gunther2017byo-1400.webp"></source> <img src="/assets/img/publication_preview/gunther2017byo.jpg?ee0c8662e4f066c4430b036034b8fe86" class="preview z-depth-1 rounded" width="auto" height="auto" alt="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="gunther2017byo" class="col-sm-8"> <div class="title"><a href="/publications/gunther2017byo">BYO*: Utilizing 3D Printed Tangible Tools for Interaction on Interactive Surfaces</a></div> <div class="author"> <a href="https://www.sebastian-guenther.com/" rel="external nofollow noopener" target="_blank">Sebastian Günther</a>, <a href="https://mschmitz.org/" rel="external nofollow noopener" target="_blank">Martin Schmitz</a>, <em>Florian Müller</em>, Jan Riemann, and <a href="https://www.informatik.tu-darmstadt.de/telekooperation/staff_tk/tk_staff_details_23168.en.jsp" rel="external nofollow noopener" target="_blank">Max Mühlhäuser</a> </div> <div class="periodical"> <em>In Proceedings of the 2017 ACM Workshop on Interacting with Smart Objects</em>, Mar 2017 </div> <div class="periodical"> </div> <div class="links"> <a href="/assets/pdf/gunther2017byo.pdf" class="btn btn-sm z-depth-0" role="button"><i class="fa-solid fa-file-pdf"></i> PDF</a> <a href="https://dx.doi.org/10.1145/3038450.3038456" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="ai ai-acmdl"> ACM</i></a> </div> <div class="abstract hidden"> <p>Sharing and manipulating information are essential for collaborative work in meeting scenarios. Nowadays, people tend to bring their own devices as a result of increasing mobility possibilities. However, transferring data from one device to another can be cumbersome and tedious if restrictions like different platforms, form factors or environmental limitations apply. In this paper, we present two concepts to enrich interaction on and between devices through 3D printed customized tangibles: 1) Bring your own information, and 2) bring your own tools. For this, we enable interactivity for low-cost and passive tangible 3D printed objects by adding conductive material and make use of touch-enabled surfaces. Our system allows users to easily share digital contents across various devices and to manipulate them with individually designed tools without additional hardware required. Copyright \copyright 2017 ACM.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"></div> <div id="Kim2017" class="col-sm-8"> <div class="title"><a href="/publications/Kim2017">Manufacturing Method of Sensor Using 3d Printing and 3d Printer Thereof</a></div> <div class="author"> Woo Sug Jung, Hwa Suk Kim, Jun Ki Jeon, Seong Kyoun Jo, Hyun Woo Lee, <a href="https://mschmitz.org/" rel="external nofollow noopener" target="_blank">Martin Schmitz</a>, <em>Florian Müller</em>, Andreas Leister, Jan Riemann, Niloofar Dezfuli, <a href="https://www.informatik.tu-darmstadt.de/telekooperation/staff_tk/tk_staff_details_23168.en.jsp" rel="external nofollow noopener" target="_blank">Max Mühlhäuser</a>, and Mohammadreza Khalilbeigi</div> <div class="periodical"> Mar 2017 </div> <div class="periodical"> </div> <div class="links"> <a href="https://patents.google.com/patent/US20180312398A1/en?oq=20180312398" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="fas fa-link"></i> Website</a> </div> <div class="abstract hidden"> <p>Disclosed is a manufacturing method of a sensor by using 3D printing and 3D printer therefor. According to an embodiment of the present disclosure , a manufacturing method of a sensor by using 3D printing includes : forming a first shape having an inner space by using a non-conduc tive material , and simultaneously or sequentially , forming an electrode at a preset location in the inner space by using a conductive material ; injecting conductive liquid into the inner space ; and forming a second shape on the first shape by using the non-conductive material to seal the inner space of the first shape .</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/muller2017cloudbits-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/muller2017cloudbits-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/muller2017cloudbits-1400.webp"></source> <img src="/assets/img/publication_preview/muller2017cloudbits.jpg?f72509c23402c61a5f3d5aaf68a82a56" class="preview z-depth-1 rounded" width="auto" height="auto" alt="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="muller2017cloudbits" class="col-sm-8"> <div class="title"><a href="/publications/muller2017cloudbits">Cloudbits: Supporting Conversations through Augmented Zero-query Search Visualization</a></div> <div class="author"> <em>Florian Müller</em>, <a href="https://www.sebastian-guenther.com/" rel="external nofollow noopener" target="_blank">Sebastian Günther</a>, Azita Hosseini Nejad, Niloofar Dezfuli, Mohammadreza Khalilbeigi, and <a href="https://www.informatik.tu-darmstadt.de/telekooperation/staff_tk/tk_staff_details_23168.en.jsp" rel="external nofollow noopener" target="_blank">Max Mühlhäuser</a> </div> <div class="periodical"> <em>In Proceedings of the 5th Symposium on Spatial User Interaction</em>, Oct 2017 </div> <div class="periodical"> </div> <div class="links"> <a href="/assets/pdf/muller2017cloudbits.pdf" class="btn btn-sm z-depth-0" role="button"><i class="fa-solid fa-file-pdf"></i> PDF</a> <a href="https://dx.doi.org/10.1145/3131277.3132173" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="ai ai-acmdl"> ACM</i></a> <a href="https://www.youtube.com/watch?v=sUe5C8RfON0" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="fa-solid fa-person-chalkboard"></i> Talk</a> </div> <div class="abstract hidden"> <p>The retrieval of additional information from public (e.g., map data) or private (e.g., e-mail) information sources using personal smart devices is a common habit in today’s co-located conversations. This behavior of users imposes challenges in two main areas: 1) cognitive focus switching and 2) information sharing. In this paper, we explore a novel approach for conversation support through augmented information bits, allowing users to see and access information right in front of their eyes. To that end, we investigate the requirements for the design of a user interface to support conversations through proactive information retrieval in an exploratory study. Based on the results, we 2) present CloudBits: A set of visualization and interaction techniques to provide mutual awareness and enhance coupling in conversations through augmented zero-query search visualization along with its prototype implementation. Finally, we 3) report the findings of a qualitative evaluation and conclude with guidelines for the design of user interfaces for conversation support.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/riemann2017evaluation-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/riemann2017evaluation-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/riemann2017evaluation-1400.webp"></source> <img src="/assets/img/publication_preview/riemann2017evaluation.jpg?5fc92610356cb8844165f28ad512d35f" class="preview z-depth-1 rounded" width="auto" height="auto" alt="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="riemann2017evaluation" class="col-sm-8"> <div class="title"><a href="/publications/riemann2017evaluation">An Evaluation of Hybrid Stacking on Interactive Tabletops</a></div> <div class="author"> Jan Riemann, <em>Florian Müller</em>, <a href="https://www.sebastian-guenther.com/" rel="external nofollow noopener" target="_blank">Sebastian Günther</a>, and <a href="https://www.informatik.tu-darmstadt.de/telekooperation/staff_tk/tk_staff_details_23168.en.jsp" rel="external nofollow noopener" target="_blank">Max Mühlhäuser</a> </div> <div class="periodical"> <em>In Proceedings of the 2017 ACM Workshop on Interacting with Smart Objects</em>, Mar 2017 </div> <div class="periodical"> </div> <div class="links"> <a href="/assets/pdf/riemann2017evaluation.pdf" class="btn btn-sm z-depth-0" role="button"><i class="fa-solid fa-file-pdf"></i> PDF</a> <a href="https://dx.doi.org/10.1145/3038450.3038451" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="ai ai-acmdl"> ACM</i></a> </div> <div class="abstract hidden"> <p>Stacking is a common practice of organizing documents in the physical world. With the recent advent of interactive tabletops, physical documents can now coexist with digital documents on the same surface. As a result, systems were developed and studied which allow piling of both types of documents with the physical documents being placed on top of the digital ones. In this paper, we study the concept of true hybrid stacking, allowing users to stack both types of documents in an arbitrary order using a hybrid tabletop system called StackTop. We discuss the results and derive implications for future hybrid tabletop systems with stacking support.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/schnelle2017smartobjects-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/schnelle2017smartobjects-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/schnelle2017smartobjects-1400.webp"></source> <img src="/assets/img/publication_preview/schnelle2017smartobjects.jpg?d9cc237c32f2b143af8d5fd7a6e637ee" class="preview z-depth-1 rounded" width="auto" height="auto" alt="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="schnelle2017smartobjects" class="col-sm-8"> <div class="title"><a href="/publications/schnelle2017smartobjects">SmartObjects: Fifth Workshop on Interacting with Smart Objects</a></div> <div class="author"> Dirk Schnelle-Walka, <em>Florian Müller</em>, Tobias Grosse-Puppendahl, Kris Luyten, <a href="https://www.informatik.tu-darmstadt.de/telekooperation/staff_tk/tk_staff_details_23168.en.jsp" rel="external nofollow noopener" target="_blank">Max Mühlhäuser</a>, and Oliver Brdiczka</div> <div class="periodical"> <em>In Proceedings of the 22nd International Conference on Intelligent User Interfaces Companion</em>, Mar 2017 </div> <div class="periodical"> </div> <div class="links"> <a href="/assets/pdf/schnelle2017smartobjects.pdf" class="btn btn-sm z-depth-0" role="button"><i class="fa-solid fa-file-pdf"></i> PDF</a> <a href="https://dx.doi.org/10.1145/3030024.3040249" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="ai ai-acmdl"> ACM</i></a> </div> <div class="abstract hidden"> <p>Smart objects are everyday objects that have computing capabilities and give rise to new ways of interaction with our environment. The increasing number of smart objects in our life shapes how we interact beyond the desktop. In this workshop we explore various aspects of the design, development and deployment of smart objects including how one can interact with smart objects.</p> </div> </div> </div> </li> </ol> <h2 class="bibliography">2016</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/muller2016proxiwatch-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/muller2016proxiwatch-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/muller2016proxiwatch-1400.webp"></source> <img src="/assets/img/publication_preview/muller2016proxiwatch.jpg?85476a1210c446fef784dffb7ed4113a" class="preview z-depth-1 rounded" width="auto" height="auto" alt="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="muller2016proxiwatch" class="col-sm-8"> <div class="title"><a href="/publications/muller2016proxiwatch">ProxiWatch: Enhancing Smartwatch Interaction through Proximity-Based Hand Input</a></div> <div class="author"> <em>Florian Müller</em>, <a href="https://www.sebastian-guenther.com/" rel="external nofollow noopener" target="_blank">Sebastian Günther</a>, Niloofar Dezfuli, Mohammadreza Khalilbeigi, and <a href="https://www.informatik.tu-darmstadt.de/telekooperation/staff_tk/tk_staff_details_23168.en.jsp" rel="external nofollow noopener" target="_blank">Max Mühlhäuser</a> </div> <div class="periodical"> <em>In Proceedings of the 2016 CHI Conference Extended Abstracts on Human Factors in Computing Systems - CHI EA ’16</em>, Mar 2016 </div> <div class="periodical"> </div> <div class="links"> <a href="/assets/pdf/muller2016proxiwatch.pdf" class="btn btn-sm z-depth-0" role="button"><i class="fa-solid fa-file-pdf"></i> PDF</a> <a href="https://dx.doi.org/10.1145/2851581.2892450" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="ai ai-acmdl"> ACM</i></a> </div> <div class="abstract hidden"> <p>Smartwatches allow ubiquitous and mobile interaction with digital contents. Because of the small screen sizes, tradi-tional interaction techniques are often not applicable. In this work, we show how the degree of freedom offered by the elbow joint, i.e., flexion and extension, can be leveraged as an additional one-handed input modality for smartwatches. By moving the watch towards or away from the body, the user is able to provide input to the smartwatch without a second hand. We present the results of a controlled ex-periment focusing on the human capabilities for proximity-based interaction. Based on the results, we propose guide-lines for designing proximity-based smartwatch interfaces and present ProxiWatch: a one-handed and proximity-based input modality for smartwatches alongside a proto-typical implementation.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/riemann2016freetop-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/riemann2016freetop-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/riemann2016freetop-1400.webp"></source> <img src="/assets/img/publication_preview/riemann2016freetop.jpg?d5df218fd47dc4af5fc740439290a7c4" class="preview z-depth-1 rounded" width="auto" height="auto" alt="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="riemann2016freetop" class="col-sm-8"> <div class="title"><a href="/publications/riemann2016freetop">FreeTop: Finding Free Spots for Projective Augmentation</a></div> <div class="author"> Jan Riemann, Mohammadreza Khalilbeigi, <a href="https://mschmitz.org/" rel="external nofollow noopener" target="_blank">Martin Schmitz</a>, Sebastian Doeweling, <em>Florian Müller</em>, and <a href="https://www.informatik.tu-darmstadt.de/telekooperation/staff_tk/tk_staff_details_23168.en.jsp" rel="external nofollow noopener" target="_blank">Max Mühlhäuser</a> </div> <div class="periodical"> <em>In Proceedings of the 2016 CHI Conference Extended Abstracts on Human Factors in Computing Systems</em>, May 2016 </div> <div class="periodical"> </div> <div class="links"> <a href="/assets/pdf/riemann2016freetop.pdf" class="btn btn-sm z-depth-0" role="button"><i class="fa-solid fa-file-pdf"></i> PDF</a> <a href="https://dx.doi.org/10.1145/2851581.2892321" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="ai ai-acmdl"> ACM</i></a> </div> <div class="abstract hidden"> <p>Augmenting the physical world using projection technologies or head-worn displays becomes increasingly popular in research and commercial applications. However, a common problem is interference between the physical surface’s texture and the projection. In this paper, we present FreeTop, a combined approach to finding areas suitable for projection, which considers multiple aspects influencing projection quality, like visual texture and physical surface structure. FreeTop can be used in stationary and mobile settings for locating free areas in arbitrary physical settings suitable for projective augmentation and touch interaction.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/schmitz2016liquido-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/schmitz2016liquido-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/schmitz2016liquido-1400.webp"></source> <img src="/assets/img/publication_preview/schmitz2016liquido.jpg?45f25cbaf29da93433c204d3c953f799" class="preview z-depth-1 rounded" width="auto" height="auto" alt="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="schmitz2016liquido" class="col-sm-8"> <div class="title"><a href="/publications/schmitz2016liquido">Liquido: Embedding Liquids into 3D Printed Objects to Sense Tilting and Motion</a></div> <div class="author"> <a href="https://mschmitz.org/" rel="external nofollow noopener" target="_blank">Martin Schmitz</a>, Andreas Leister, Niloofar Dezfuli, Jan Riemann, <em>Florian Müller</em>, and <a href="https://www.informatik.tu-darmstadt.de/telekooperation/staff_tk/tk_staff_details_23168.en.jsp" rel="external nofollow noopener" target="_blank">Max Mühlhäuser</a> </div> <div class="periodical"> <em>In Proceedings of the 2016 CHI Conference Extended Abstracts on Human Factors in Computing Systems</em>, May 2016 </div> <div class="periodical"> </div> <div class="links"> <a href="/assets/pdf/schmitz2016liquido.pdf" class="btn btn-sm z-depth-0" role="button"><i class="fa-solid fa-file-pdf"></i> PDF</a> <a href="https://dx.doi.org/10.1145/2851581.2892275" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="ai ai-acmdl"> ACM</i></a> </div> <div class="abstract hidden"> <p>Tilting and motion are widely used as interaction modalities in smart objects such as wearables and smart phones (e.g., to detect posture or shaking). They are often sensed with accelerometers. In this paper, we propose to embed liquids into 3D printed objects while printing to sense various tilting and motion interactions via capacitive sensing. This method reduces the assembly effort after printing and is a low-cost and easy-to-apply way of extending the input capabilities of 3D printed objects. We contribute two liquid sensing patterns and a practical printing process using a standard dual-extrusion 3D printer and commercially available materials. We validate the method by a series of evaluations and provide a set of interactive example applications. \copyright 2016 Authors.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/schnelle2016scwt-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/schnelle2016scwt-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/schnelle2016scwt-1400.webp"></source> <img src="/assets/img/publication_preview/schnelle2016scwt.jpg?d9cc237c32f2b143af8d5fd7a6e637ee" class="preview z-depth-1 rounded" width="auto" height="auto" alt="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="schnelle2016scwt" class="col-sm-8"> <div class="title"><a href="/publications/schnelle2016scwt">SCWT: A Joint Workshop on Smart Connected and Wearable Things</a></div> <div class="author"> Dirk Schnelle-Walka, Lior Limonad, Tobias Grosse-Puppendahl, Joel Lanir, <em>Florian Müller</em>, Massimo Mecella, Kris Luyten, Tsvi Kuflik, Oliver Brdiczka, and <a href="https://www.informatik.tu-darmstadt.de/telekooperation/staff_tk/tk_staff_details_23168.en.jsp" rel="external nofollow noopener" target="_blank">Max Mühlhäuser</a> </div> <div class="periodical"> <em>In Companion Publication of the 21st International Conference on Intelligent User Interfaces</em>, Mar 2016 </div> <div class="periodical"> </div> <div class="links"> <a href="/assets/pdf/schnelle2016scwt.pdf" class="btn btn-sm z-depth-0" role="button"><i class="fa-solid fa-file-pdf"></i> PDF</a> <a href="https://dx.doi.org/10.1145/2876456.2882849" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="ai ai-acmdl"> ACM</i></a> </div> <div class="abstract hidden"> <p>The increasing number of smart objects in our everyday life shapes how we interact beyond the desktop. In this workshop we discuss how advanced interactions with smart objects in the context of the Internet-of-Thingsshould be designed from various perspectives, such as HCI and AI as well as industry and academia.</p> </div> </div> </div> </li> </ol> <h2 class="bibliography">2015</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/joStudyBodyUser2015-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/joStudyBodyUser2015-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/joStudyBodyUser2015-1400.webp"></source> <img src="/assets/img/publication_preview/joStudyBodyUser2015.jpg?11b9293f7bc1df9fd0b9ed4000a80e61" class="preview z-depth-1 rounded" width="auto" height="auto" alt="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="joStudyBodyUser2015" class="col-sm-8"> <div class="title"><a href="/publications/joStudyBodyUser2015">A Study of On-body User Interface: PiAM(Palm interAction Module)</a></div> <div class="author"> Seng-Kyoun Jo,  이현우, Kim Jinsul, <em>Florian Müller</em>, Mohammed Khalilbeigi, and <a href="https://www.informatik.tu-darmstadt.de/telekooperation/staff_tk/tk_staff_details_23168.en.jsp" rel="external nofollow noopener" target="_blank">Max Mühlhäuser</a> </div> <div class="periodical"> <em>Journal of Knowledge Information Technology and Systems</em>, Dec 2015 </div> <div class="periodical"> </div> <div class="links"> <a href="/assets/pdf/joStudyBodyUser2015.pdf" class="btn btn-sm z-depth-0" role="button"><i class="fa-solid fa-file-pdf"></i> PDF</a> </div> <div class="abstract hidden"> <p>User interface providing easy and efficient control environments with user is emerging as a core keyword in ICT market and smart phone industry as the influence and importance of UI has increased recently. To succeed in ICT market, however, UI has to satisfy industrial requirements including simple and easy control, intuitional access and high recognition. Thus study for providing user-friendly interface has been actively researched. In this paper, we investigate user interface based on human’s</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/Muller2015-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/Muller2015-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/Muller2015-1400.webp"></source> <img src="/assets/img/publication_preview/Muller2015.jpg?6b3ab2119a32993b25f67d6a21851de9" class="preview z-depth-1 rounded" width="auto" height="auto" alt="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Muller2015" class="col-sm-8"> <div class="title"><a href="/publications/Muller2015">A Study on Proximity-based Hand Input for One-handed Mobile Interaction</a></div> <div class="author"> <em>Florian Müller</em>, Mohammadreza Khalilbeigi, Niloofar Dezfuli, Alireza Sahami Shirazi, <a href="https://www.sebastian-guenther.com/" rel="external nofollow noopener" target="_blank">Sebastian Günther</a>, and <a href="https://www.informatik.tu-darmstadt.de/telekooperation/staff_tk/tk_staff_details_23168.en.jsp" rel="external nofollow noopener" target="_blank">Max Mühlhäuser</a> </div> <div class="periodical"> <em>In Proceedings of the 3rd ACM Symposium on Spatial User Interaction</em>, Aug 2015 </div> <div class="periodical"> </div> <div class="links"> <a href="/assets/pdf/Muller2015.pdf" class="btn btn-sm z-depth-0" role="button"><i class="fa-solid fa-file-pdf"></i> PDF</a> <a href="https://dx.doi.org/10.1145/2788940.2788955" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="ai ai-acmdl"> ACM</i></a> </div> <div class="abstract hidden"> <p>On-body user interfaces utilize the human’s skin for both sensing input and displaying graphical output. In this paper, we present how the degree of freedom offered by the elbow joint, i.e., exion and extension, can be leveraged to extend the input space of projective user interfaces. The user can move his hand towards or away from himself to browse through a multi-layer information space. We conducted a controlled experiment to investigate how accurately and ef-ficiently users can interact in the space. The results revealed that the accuracy and effciency of proximity-based interactions mainly depend on the traveling distance to the target layer while neither the hand side nor the direction of interaction have a signifcant inuence. Based on our findings, we propose guidelines for designing on-body user interfaces.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/Muller2015b-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/Muller2015b-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/Muller2015b-1400.webp"></source> <img src="/assets/img/publication_preview/Muller2015b.jpg?ef4c495742114bc07c43a8bab0a2f1d5" class="preview z-depth-1 rounded" width="auto" height="auto" alt="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Muller2015b" class="col-sm-8"> <div class="title"><a href="/publications/Muller2015b">Palm-Based Interaction with Head-mounted Displays</a></div> <div class="author"> <em>Florian Müller</em>, Niloofar Dezfuli, <a href="https://www.informatik.tu-darmstadt.de/telekooperation/staff_tk/tk_staff_details_23168.en.jsp" rel="external nofollow noopener" target="_blank">Max Mühlhäuser</a>, <a href="https://mschmitz.org/" rel="external nofollow noopener" target="_blank">Martin Schmitz</a>, and Mohammadreza Khalilbeigi</div> <div class="periodical"> <em>In Proceedings of the 17th International Conference on Human-Computer Interaction with Mobile Devices and Services Adjunct - MobileHCI ’15</em>, Aug 2015 </div> <div class="periodical"> </div> <div class="links"> <a href="/assets/pdf/Muller2015b.pdf" class="btn btn-sm z-depth-0" role="button"><i class="fa-solid fa-file-pdf"></i> PDF</a> <a href="https://dx.doi.org/10.1145/2786567.2794314" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="ai ai-acmdl"> ACM</i></a> </div> <div class="abstract hidden"> <p>Head-mounted displays (HMDs) are an emerging class of wearable devices that allow users to access and alter information right in front of their eyes. However, due to their size and shape, traditional input modalities (e.g., multi-touch sensing on the device) are not practical. In this position paper, we argue that palm-based interactions have a great potential to ease the interaction with HMDs. We outline two interaction concepts and present directions for future research that can lead to more enjoyable and usable interfaces for HMDs.</p> </div> </div> </div> </li> </ol> <h2 class="bibliography">2012</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/Dezfuli2012-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/Dezfuli2012-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/Dezfuli2012-1400.webp"></source> <img src="/assets/img/publication_preview/Dezfuli2012.jpg?dda1c3e232fd24681bea2a479b79f2b6" class="preview z-depth-1 rounded" width="auto" height="auto" alt="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Dezfuli2012" class="col-sm-8"> <div class="title"><a href="/publications/Dezfuli2012">PalmRC: Imaginary Palm-Based Remote Control for Eyes-Free Television Interaction</a></div> <div class="author"> Niloofar Dezfuli, Mohammadreza Khalilbeigi, Jochen Huber, <em>Florian Müller</em>, and <a href="https://www.informatik.tu-darmstadt.de/telekooperation/staff_tk/tk_staff_details_23168.en.jsp" rel="external nofollow noopener" target="_blank">Max Mühlhäuser</a> </div> <div class="periodical"> <em>In EuroiTV’12 - Proceedings of the 10th European Conference on Interactive TV and Video</em>, Jul 2012 </div> <div class="periodical"> </div> <div class="links"> <a href="/assets/pdf/Dezfuli2012.pdf" class="btn btn-sm z-depth-0" role="button"><i class="fa-solid fa-file-pdf"></i> PDF</a> <a href="https://dx.doi.org/10.1145/2325616.2325623" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="ai ai-acmdl"> ACM</i></a> </div> <div class="abstract hidden"> <p>User input on television (TV) typically requires a mediator device, such as a handheld remote control. While being a well-established interaction paradigm, a handheld device has serious drawbacks: it can be easily misplaced due to its mobility and in case of a touch screen interface, it also requires additional visual attention. Emerging interaction paradigms like 3D mid-air gestures using novel depth sensors, such as Microsoft’s Kinect, aim at overcoming these limitations, but are known to be e.g. tiring. In this paper, we propose to leverage the palm as an interactive surface for TV remote control. Our contribution is three-fold: (1) we explore the conceptual design space in an exploratory study. (2) Based upon these results, we investigate the effectiveness and accuracy of such an interface in a controlled experiment. And (3), we contribute PalmRC: an eyes-free, palm-surface-based TV remote control, which in turn is evaluated in an early user feedback session. Our results show that the palm has the potential to be leveraged for device-less and eyes-free TV remote interaction without any third-party mediator device. \copyright 2012 ACM.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/Dezfuli2012a-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/Dezfuli2012a-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/Dezfuli2012a-1400.webp"></source> <img src="/assets/img/publication_preview/Dezfuli2012a.jpg?357b88a1ecbdd63a0019d6ecc812ae95" class="preview z-depth-1 rounded" width="auto" height="auto" alt="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Dezfuli2012a" class="col-sm-8"> <div class="title"><a href="/publications/Dezfuli2012a">Leveraging the Palm Surface as an Eyes-Free Tv Remote Control</a></div> <div class="author"> Niloofar Dezfuli, Mohammadreza Khalilbeigi, Jochen Huber, <em>Florian Müller</em>, and <a href="https://www.informatik.tu-darmstadt.de/telekooperation/staff_tk/tk_staff_details_23168.en.jsp" rel="external nofollow noopener" target="_blank">Max Mühlhäuser</a> </div> <div class="periodical"> <em>In CHI ’12 Extended Abstracts on Human Factors in Computing Systems</em>, May 2012 </div> <div class="periodical"> </div> <div class="links"> <a href="/assets/pdf/Dezfuli2012a.pdf" class="btn btn-sm z-depth-0" role="button"><i class="fa-solid fa-file-pdf"></i> PDF</a> <a href="https://dx.doi.org/10.1145/2212776.2223823" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="ai ai-acmdl"> ACM</i></a> </div> <div class="abstract hidden"> <p>User input on television typically requires a mediator device such as a handheld remote control. While being a well-established interaction paradigm, a handheld device has serious drawbacks: it can be easily misplaced due to its mobility and in case of a touch screen interface, it also requires additional visual attention. Emerging interaction paradigms like 3D mid-air gestures using novel depth sensors such as Microsoft’s Kinect aim at overcoming these limitations, but are known for instance to be tiring. In this paper, we propose to leverage the palm as an interactive surface for TV remote control. Our contribution is two-fold: (1) we have explored the conceptual design space in an exploratory study. (2) Based upon these results, we investigated the accuracy and effectiveness of such an interface in a controlled experiment. Our results show that the palm has the potential to be leveraged for device-less and eyes-free TV interactions without any third-party mediator device.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/Dezfuli2012Couch-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/Dezfuli2012Couch-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/Dezfuli2012Couch-1400.webp"></source> <img src="/assets/img/publication_preview/Dezfuli2012Couch.jpg?2cc73910202bd00303fbd8d524e45f40" class="preview z-depth-1 rounded" width="auto" height="auto" alt="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Dezfuli2012Couch" class="col-sm-8"> <div class="title"><a href="/publications/Dezfuli2012Couch">CouchTV: Leveraging the Spatial Information of Viewers for Social Interactive Television Systems - Poster Presentation</a></div> <div class="author"> Niloofar Dezfuli, Manolis Pavlakis, <em>Florian Müller</em>, Mohammadreza Khalilbeigi, and <a href="https://www.informatik.tu-darmstadt.de/telekooperation/staff_tk/tk_staff_details_23168.en.jsp" rel="external nofollow noopener" target="_blank">Max Mühlhäuser</a> </div> <div class="periodical"> <em>In 10th European Interactive TV Conference 2012</em>, Jul 2012 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> </ol> </div> </article> </div> </div> <footer class="sticky-bottom mt-5"> <div class="container"> © Copyright Florian Müller | <a href="https://www.flomue.com/imprint">Impressum</a> </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script type="text/javascript">$(function(){$('[data-toggle="tooltip"]').tooltip()});</script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?7b30caa5023af4af8408a472dc4e1ebb"></script> <script defer src="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js?d633890033921b33e0ceb13d22340a9c"></script> <script defer src="/assets/js/common.js?acdb9690d7641b2f8d40529018c71a01"></script> <script defer src="/assets/js/copy_code.js?07b8786bab9b4abe90d10e61f7d12ff7" type="text/javascript"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>