<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>publications | Florian Müller</title> <meta name="author" content="Florian Müller"> <meta name="description" content="Florian Müller is a a postdoctoral Human-Computer Interaction researcher at the Human-Centered Ubiquitous Media Lab at the LMU Munich. "> <meta name="keywords" content="hci, research, urban interaction"> <meta property="og:site_name" content="Florian Müller"> <meta property="og:type" content="website"> <meta property="og:title" content="Florian Müller | publications"> <meta property="og:url" content="https://flomuedev.github.io/publications/"> <meta property="og:description" content="Florian Müller is a a postdoctoral Human-Computer Interaction researcher at the Human-Centered Ubiquitous Media Lab at the LMU Munich. "> <meta property="og:locale" content="en"> <meta name="twitter:card" content="summary"> <meta name="twitter:title" content="publications"> <meta name="twitter:description" content="Florian Müller is a a postdoctoral Human-Computer Interaction researcher at the Human-Centered Ubiquitous Media Lab at the LMU Munich. "> <script type="application/ld+json">
    {
        "author":
        {
            "@type": "Person",
            "name": "Florian  Müller"
        },
        "url": "https://flomuedev.github.io/publications/",
        "@type": "WebSite",
        "description": "Florian Müller is a a postdoctoral Human-Computer Interaction researcher at the Human-Centered Ubiquitous Media Lab at the LMU Munich.
",
        "headline": "publications",
        "sameAs": ["https://orcid.org/0000-0002-9621-6214", "https://scholar.google.com/citations?user=slfzfQIAAAAJ", "https://github.com/flomuedev", "https://www.linkedin.com/in/florian-mueller-hci"],
        "name": "Florian  Müller",
        "@context": "https://schema.org"
    }
    </script> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link rel="stylesheet" href="/assets/css/fonts.css?caa062f90620afb533f3e4ca33f9d981"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?19f3075a2d19613090fe9e16b564e1fe" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%98%89&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://flomuedev.github.io/publications/"> <script src="https://cdn.telemetrydeck.com/websdk/telemetrydeck.min.js" data-app-id="A19A11BC-58CD-4EB7-83FA-40FFCFEB64F1"> </script> <link rel="stylesheet" href="/assets/css/yt-consent.css?a6d140f226e1b86136356683f6470ac1"> <script src="/assets/js/yt-consent.js?0f324e6659291dde56fbbbb9aeb6cf60"></script> </head> <body class="fixed-top-nav sticky-bottom-footer"> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Florian </span>Müller</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about</a> </li> <li class="nav-item active"> <a class="nav-link" href="/publications/">publications<span class="sr-only">(current)</span></a> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">publications</h1> <p class="post-description">Distinguished papers are marked with <i class="fa-solid fa-award" style="color: var(--global-theme-color);"></i> for Honourable Mention Awards and <i class="fa-solid fa-trophy" style="color: var(--global-theme-color);"></i> for Best Paper Awards. For a up‑to‑date list of 70+ publications, please see <a href="https://scholar.google.com/citations?user=slfzfQIAAAAJ" rel="external nofollow noopener" target="_blank"><i class="fa-solid fa-trophy"></i> Google Scholar</a>.</p> </header> <article> <div class="publications"> <h2 class="bibliography">2023</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/bartkowskiSyncExploringSynchronization2023a-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/bartkowskiSyncExploringSynchronization2023a-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/bartkowskiSyncExploringSynchronization2023a-1400.webp"></source> <img src="/assets/img/publication_preview/bartkowskiSyncExploringSynchronization2023a.jpg?3dc05777716bdaf8c8f29af0c7d4854e" class="preview z-depth-1 rounded" width="auto" height="auto" alt="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="bartkowskiSyncExploringSynchronization2023a" class="col-sm-8"> <div class="title"><a href="/publications/bartkowskiSyncExploringSynchronization2023a">In Sync: Exploring Synchronization to Increase Trust Between Humans and Non-humanoid Robots</a></div> <div class="author"> Wieslaw Bartkowski, Andrzej Nowak, Filip Ignacy Czajkowski, Albrecht Schmidt, and <em>Florian Müller</em> </div> <div class="periodical"> <em>In Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems</em>, Apr 2023 </div> <div class="periodical"> </div> <div class="links"> <a href="/assets/pdf/bartkowskiSyncExploringSynchronization2023a.pdf" class="btn btn-sm z-depth-0" role="button"><i class="fa-solid fa-file-pdf"></i> PDF</a> <a href="https://dx.doi.org/10.1145/3544548.3581193" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="ai ai-acmdl"> ACM</i></a> <a href="http://arxiv.org/abs/2303.15917" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="ai ai-arxiv"></i> arxiv</a> <a href="https://www.youtube.com/watch?v=kdEB9L4OCgk" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-youtube"></i> Video</a> <a href="https://www.youtube.com/watch?v=dEYNV4KaBR8" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="fa-solid fa-person-chalkboard"></i> Talk</a> </div> <div class="abstract hidden"> <p>When we go for a walk with friends, we can observe an interesting effect: From step lengths to arm movements - our movements unconsciously align; they synchronize. Prior research found that this synchronization is a crucial aspect of human relations that strengthens social cohesion and trust. Generalizing from these findings in synchronization theory, we propose a dynamical approach that can be applied in the design of non-humanoid robots to increase trust. We contribute the results of a controlled experiment with 51 participants exploring our concept in a between-subjects design. For this, we built a prototype of a simple non-humanoid robot that can bend to follow human movements and vary the movement synchronization patterns. We found that synchronized movements lead to significantly higher ratings in an established questionnaire on trust between people and automation but did not influence the willingness to spend money in a trust game.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/desoldaDigitalModelingEveryone2023a-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/desoldaDigitalModelingEveryone2023a-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/desoldaDigitalModelingEveryone2023a-1400.webp"></source> <img src="/assets/img/publication_preview/desoldaDigitalModelingEveryone2023a.jpg?b8ddaea51f767d0f052ded1120acb9d4" class="preview z-depth-1 rounded" width="auto" height="auto" alt="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="desoldaDigitalModelingEveryone2023a" class="col-sm-8"> <div class="title"><a href="/publications/desoldaDigitalModelingEveryone2023a">Digital Modeling for Everyone: Exploring How Novices Approach Voice-Based 3D Modeling</a></div> <div class="author"> Giuseppe Desolda, Andrea Esposito, <em>Florian Müller</em>, and Sebastian Feger</div> <div class="periodical"> <em>In Human-Computer Interaction – INTERACT 2023</em>, Apr 2023 </div> <div class="periodical"> </div> <div class="links"> <a href="/assets/pdf/desoldaDigitalModelingEveryone2023a.pdf" class="btn btn-sm z-depth-0" role="button"><i class="fa-solid fa-file-pdf"></i> PDF</a> <a href="https://dx.doi.org/10.1007/978-3-031-42293-5_11" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="ai ai-springer"> Springer</i></a> </div> <div class="abstract hidden"> <p>Manufacturing tools like 3D printers have become accessible to the wider society, making the promise of digital fabrication for everyone seemingly reachable. While the actual manufacturing process is largely automated today, users still require knowledge of complex design applications to produce ready-designed objects and adapt them to their needs or design new objects from scratch. To lower the barrier to the design and customization of personalized 3D models, we explored novice mental models in voice-based 3D modeling by conducting a high-fidelity Wizard of Oz study with 22 participants. We performed a thematic analysis of the collected data to understand how the mental model of novices translates into voice-based 3D modeling. We conclude with design implications for voice assistants. For example, they have to: deal with vague, incomplete and wrong commands; provide a set of straightforward commands to shape simple and composite objects; and offer different strategies to select 3D objects.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/elsayedUnderstandingStationaryMoving2023-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/elsayedUnderstandingStationaryMoving2023-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/elsayedUnderstandingStationaryMoving2023-1400.webp"></source> <img src="/assets/img/publication_preview/elsayedUnderstandingStationaryMoving2023.jpg?3a6a50f77659f6e2bcb57d826b252783" class="preview z-depth-1 rounded" width="auto" height="auto" alt="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="elsayedUnderstandingStationaryMoving2023" class="col-sm-8"> <div class="title"><a href="/publications/elsayedUnderstandingStationaryMoving2023">Understanding Stationary and Moving Direct Skin Vibrotactile Stimulation on the Palm</a></div> <div class="author"> Hesham Elsayed, Martin Weigel, <em>Florian Müller</em>, George Ibrahim, Jan Gugenheimer, Martin Schmitz, <a href="https://www.sebastian-guenther.com/" rel="external nofollow noopener" target="_blank">Sebastian Günther</a>, and Max Mühlhäuser</div> <div class="periodical"> Feb 2023 </div> <div class="periodical"> </div> <div class="links"> <a href="/assets/pdf/elsayedUnderstandingStationaryMoving2023.pdf" class="btn btn-sm z-depth-0" role="button"><i class="fa-solid fa-file-pdf"></i> PDF</a> <a href="http://arxiv.org/abs/2302.08820" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="ai ai-arxiv"></i> arxiv</a> </div> <div class="abstract hidden"> <p>Palm-based tactile displays have the potential to evolve from single motor interfaces (e.g., smartphones) to high-resolution tactile displays (e.g., back-of-device haptic interfaces) enabling richer multi-modal experiences with more information. However, we lack a systematic understanding of vibrotactile perception on the palm and the influence of various factors on the core design decisions of tactile displays (number of actuators, resolution, and intensity). In a first experiment (N=16), we investigated the effect of these factors on the users’ ability to localize stationary sensations. In a second experiment (N=20), we explored the influence of resolution on recognition rate for moving tactile sensations.Findings show that for stationary sensations a 9 actuator display offers a good trade-off and a \3}times3 resolution can be accurately localized. For moving sensations, a \2}times4 resolution led to the highest recognition accuracy, while \5}times10 enables higher resolution output with a reasonable accuracy.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/eskaThermoFeetAssessingOnFoot2023-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/eskaThermoFeetAssessingOnFoot2023-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/eskaThermoFeetAssessingOnFoot2023-1400.webp"></source> <img src="/assets/img/publication_preview/eskaThermoFeetAssessingOnFoot2023.jpg?8c84d31b5a23809edd96eca5a7d54620" class="preview z-depth-1 rounded" width="auto" height="auto" alt="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="eskaThermoFeetAssessingOnFoot2023" class="col-sm-8"> <div class="title"><a href="/publications/eskaThermoFeetAssessingOnFoot2023">ThermoFeet: Assessing On-Foot Thermal Stimuli for Directional Cues</a></div> <div class="author"> Bettina Eska, Jeff-Owens Iyalekhue, <a href="https://www.sebastian-guenther.com/" rel="external nofollow noopener" target="_blank">Sebastian Günther</a>, Jasmin Niess, and <em>Florian Müller</em> </div> <div class="periodical"> <em>In Proceedings of the 22nd International Conference on Mobile and Ubiquitous Multimedia</em>, Dec 2023 </div> <div class="periodical"> </div> <div class="links"> <a href="/assets/pdf/eskaThermoFeetAssessingOnFoot2023.pdf" class="btn btn-sm z-depth-0" role="button"><i class="fa-solid fa-file-pdf"></i> PDF</a> <a href="https://dx.doi.org/10.1145/3626705.3627974" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="ai ai-acmdl"> ACM</i></a> </div> <div class="abstract hidden"> <p>Thermal feedback has been studied for navigation purposes with directional cues and a variety of other use cases. Yet, to date, systems providing thermal feedback were primarily designed for the upper body, targeting hands and arms in particular. As these parts are often occupied with other tasks, there is a need to extend the design space of thermal feedback to other body parts. To close this gap, we assess thermal feedback on the user’s feet. This research explores if creating stimuli representing any direction on a circle with only four actuators is possible. To evaluate this concept, we conducted a user study asking the participants to indicate the perceived direction after getting a hot or cold stimulus by direct actuation using one actuator or phantom actuation using two actuators. The results indicate that the detection accuracy was higher for cold signals. In addition, the results showed higher recognition for stimuli linked to actuator distribution than phantom sensation due to spatial summation.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/hirschMyHeartWill2023a-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/hirschMyHeartWill2023a-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/hirschMyHeartWill2023a-1400.webp"></source> <img src="/assets/img/publication_preview/hirschMyHeartWill2023a.jpg?55135d1bb7fb3f79a735e016c22e22a5" class="preview z-depth-1 rounded" width="auto" height="auto" alt="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="hirschMyHeartWill2023a" class="col-sm-8"> <div class="title"><a href="/publications/hirschMyHeartWill2023a">My Heart Will Go On: Implicitly Increasing Social Connectedness by Visualizing Asynchronous Players’ Heartbeats in VR Games</a></div> <div class="author"> Linda Hirsch, <em>Florian Müller</em>, Francesco Chiossi, Theodor Benga, and Andreas Martin Butz</div> <div class="periodical"> <em>Proceedings of the ACM on Human-Computer Interaction</em>, Oct 2023 </div> <div class="periodical"> </div> <div class="links"> <a href="/assets/pdf/hirschMyHeartWill2023a.pdf" class="btn btn-sm z-depth-0" role="button"><i class="fa-solid fa-file-pdf"></i> PDF</a> <a href="https://dx.doi.org/10.1145/3611057" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="ai ai-acmdl"> ACM</i></a> <a href="https://www.youtube.com/watch?v=PuHfic0rmAs" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="fa-solid fa-person-chalkboard"></i> Talk</a> </div> <div class="abstract hidden"> <p>Social games benefit from social connectedness between players because it improves the gaming experience and increases enjoyment. In virtual reality (VR), various approaches, such as avatars, are developed for multi-player games to increase social connectedness. However, these approaches are lacking in single-player games. To increase social connectedness in such games, our work explores the visualization of physiological data from asynchronous players, i.e., electrocardiogram (ECG). We identified two visualization dimensions, the number of players, and the visualization style, after a design workshop with experts (N=4) and explored them in a single-user virtual escape room game. We spatially and temporally integrated the visualizations and compared two times two visualizations against a baseline condition without visualization in a within-subject lab study (N=34). All but one visualization significantly increased participants’ feelings of social connectedness. Heart icons triggered the strongest feeling of connectedness, understanding, and perceived support in playing the game.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/hirzleWhenXRAI2023a-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/hirzleWhenXRAI2023a-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/hirzleWhenXRAI2023a-1400.webp"></source> <img src="/assets/img/publication_preview/hirzleWhenXRAI2023a.jpg?9d688ff284b7f90408f842fe7da88226" class="preview z-depth-1 rounded" width="auto" height="auto" alt="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="hirzleWhenXRAI2023a" class="col-sm-8"> <div class="title"><a href="/publications/hirzleWhenXRAI2023a">When XR and AI Meet - A Scoping Review on Extended Reality and Artificial Intelligence</a></div> <div class="author"> Teresa Hirzle, <em>Florian Müller</em>, Fiona Draxler, Martin Schmitz, Pascal Knierim, and Kasper Hornbæk</div> <div class="periodical"> <em>In Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems</em>, Apr 2023 </div> <div class="periodical"> </div> <div class="links"> <a href="/assets/pdf/hirzleWhenXRAI2023a.pdf" class="btn btn-sm z-depth-0" role="button"><i class="fa-solid fa-file-pdf"></i> PDF</a> <a href="https://dx.doi.org/10.1145/3544548.3581072" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="ai ai-acmdl"> ACM</i></a> <a href="https://www.youtube.com/watch?v=VDg-2Pz9lj8" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="fa-solid fa-person-chalkboard"></i> Talk</a> </div> <div class="abstract hidden"> <p>Research on Extended Reality (XR) and Artificial Intelligence (AI) is booming, which has led to an emerging body of literature in their intersection. However, the main topics in this intersection are unclear, as are the benefits of combining XR and AI. This paper presents a scoping review that highlights how XR is applied in AI research and vice versa. We screened 2619 publications from 203 international venues published between 2017 and 2021, followed by an in-depth review of 311 papers. Based on our review, we identify five main topics at the intersection of XR and AI, showing how research at the intersection can benefit each other. Furthermore, we present a list of commonly used datasets, software, libraries, and models to help researchers interested in this intersection. Finally, we present 13 research opportunities and recommendations for future work in XR and AI research.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/liLocationAwareVirtualReality2023a-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/liLocationAwareVirtualReality2023a-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/liLocationAwareVirtualReality2023a-1400.webp"></source> <img src="/assets/img/publication_preview/liLocationAwareVirtualReality2023a.jpg?48acea74fd50e7601018e616297f88a8" class="preview z-depth-1 rounded" width="auto" height="auto" alt="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="liLocationAwareVirtualReality2023a" class="col-sm-8"> <div class="title"><a href="/publications/liLocationAwareVirtualReality2023a">Location-Aware Virtual Reality for Situational Awareness On the Road</a></div> <div class="author"> Jingyi Li, Alexandra Mayer, <em>Florian Müller</em>, Andrii Matviienko, and Andreas Butz</div> <div class="periodical"> <em>In Proceedings of the 2023 ACM Symposium on Spatial User Interaction</em>, Oct 2023 </div> <div class="periodical"> </div> <div class="links"> <a href="/assets/pdf/liLocationAwareVirtualReality2023a.pdf" class="btn btn-sm z-depth-0" role="button"><i class="fa-solid fa-file-pdf"></i> PDF</a> <a href="https://dx.doi.org/10.1145/3607822.3614530" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="ai ai-acmdl"> ACM</i></a> </div> <div class="abstract hidden"> <p>When future passengers are immersed in Virtual Reality (VR), the resulting disconnection from the physical world may degrade their situational awareness on the road. We propose incorporating real-world cues into virtual experiences when passing specific locations to address this. We designed two visualizations using points of interest (POIs), street names alone or combined with live street views. We compared them to two baselines, persistently displaying live cues (Always Live) or no cues (Always VR). In a field study (N=17), participants estimated their locations while exposed to VR entertainment during car rides. The results show that adding environmental cues inevitably degrades VR presence compared to Always VR. However, POI-triggered Text&amp;Live preserves VR presence better than Always Live and attracts user attention to the road more than POI-triggered Text. We discuss situational awareness challenges for using mobile VR on the road and potential incorporation strategies across transport contexts.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/mullerTicTacToesAssessingToe2023a-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/mullerTicTacToesAssessingToe2023a-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/mullerTicTacToesAssessingToe2023a-1400.webp"></source> <img src="/assets/img/publication_preview/mullerTicTacToesAssessingToe2023a.jpg?4f5fa35ae4f4ead6fcf7e31a2a1b8812" class="preview z-depth-1 rounded" width="auto" height="auto" alt="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="mullerTicTacToesAssessingToe2023a" class="col-sm-8"> <div class="title"><a href="/publications/mullerTicTacToesAssessingToe2023a">TicTacToes: Assessing Toe Movements as an Input Modality</a></div> <div class="author"> <em>Florian Müller</em>, Daniel Schmitt, Andrii Matviienko, Dominik Schön, <a href="https://www.sebastian-guenther.com/" rel="external nofollow noopener" target="_blank">Sebastian Günther</a>, Thomas Kosch, and Martin Schmitz</div> <div class="periodical"> <em>In Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems</em>, Apr 2023 </div> <div class="periodical"> </div> <div class="links"> <a href="/assets/pdf/mullerTicTacToesAssessingToe2023a.pdf" class="btn btn-sm z-depth-0" role="button"><i class="fa-solid fa-file-pdf"></i> PDF</a> <a href="https://dx.doi.org/10.1145/3544548.3580954" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="ai ai-acmdl"> ACM</i></a> <a href="http://arxiv.org/abs/2303.15811" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="ai ai-arxiv"></i> arxiv</a> <a href="https://www.youtube.com/watch?v=FzA-6F5SJ44" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-youtube"></i> Video</a> <a href="https://www.youtube.com/watch?v=2enVDAGiE8E" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="fa-solid fa-person-chalkboard"></i> Talk</a> </div> <div class="abstract hidden"> <p>From carrying grocery bags to holding onto handles on the bus, there are a variety of situations where one or both hands are busy, hindering the vision of ubiquitous interaction with technology. Voice commands, as a popular hands-free alternative, struggle with ambient noise and privacy issues. As an alternative approach, research explored movements of various body parts (e.g., head, arms) as input modalities, with foot-based techniques proving particularly suitable for hands-free interaction. Whereas previous research only considered the movement of the foot as a whole, in this work, we argue that our toes offer further degrees of freedom that can be leveraged for interaction. To explore the viability of toe-based interaction, we contribute the results of a controlled experiment with 18 participants assessing the impact of five factors on the accuracy, efficiency and user experience of such interfaces. Based on the findings, we provide design recommendations for future toe-based interfaces.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/mullerUndoPortExploringInfluence2023a-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/mullerUndoPortExploringInfluence2023a-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/mullerUndoPortExploringInfluence2023a-1400.webp"></source> <img src="/assets/img/publication_preview/mullerUndoPortExploringInfluence2023a.jpg?1c1155ce77f6a5d7a4a48972a6652d74" class="preview z-depth-1 rounded" width="auto" height="auto" alt="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="mullerUndoPortExploringInfluence2023a" class="col-sm-8"> <div class="title"><a href="/publications/mullerUndoPortExploringInfluence2023a">UndoPort: Exploring the Influence of Undo-Actions for Locomotion in Virtual Reality on the Efficiency, Spatial Understanding and User Experience</a></div> <div class="author"> <em>Florian Müller</em>, Arantxa Ye, Dominik Schön, and Julian Rasch</div> <div class="periodical"> <em>In Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems</em>, Apr 2023 </div> <div class="periodical"> </div> <div class="links"> <a href="/assets/pdf/mullerUndoPortExploringInfluence2023a.pdf" class="btn btn-sm z-depth-0" role="button"><i class="fa-solid fa-file-pdf"></i> PDF</a> <a href="https://dx.doi.org/10.1145/3544548.3581557" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="ai ai-acmdl"> ACM</i></a> <a href="http://arxiv.org/abs/2303.15800" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="ai ai-arxiv"></i> arxiv</a> <a href="https://www.youtube.com/watch?v=BwRc4f8VSEk" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-youtube"></i> Video</a> <a href="https://www.youtube.com/watch?v=INzk1_a2Z3k" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="fa-solid fa-person-chalkboard"></i> Talk</a> </div> <div class="abstract hidden"> <p>When we get lost in Virtual Reality (VR) or want to return to a previous location, we use the same methods of locomotion for the way back as for the way forward. This is time-consuming and requires additional physical orientation changes, increasing the risk of getting tangled in the headsets’ cables. In this paper, we propose the use of undo actions to revert locomotion steps in VR. We explore eight different variations of undo actions as extensions of point&amp;teleport, based on the possibility to undo position and orientation changes together with two different visualizations of the undo step (discrete and continuous). We contribute the results of a controlled experiment with 24 participants investigating the efficiency and orientation of the undo techniques in a radial maze task. We found that the combination of position and orientation undo together with a discrete visualization resulted in the highest efficiency without increasing orientation errors.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/raschGoingGoingGone2023a-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/raschGoingGoingGone2023a-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/raschGoingGoingGone2023a-1400.webp"></source> <img src="/assets/img/publication_preview/raschGoingGoingGone2023a.jpg?f37f90ddf7ea855c40757c09ba8ac81e" class="preview z-depth-1 rounded" width="auto" height="auto" alt="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="raschGoingGoingGone2023a" class="col-sm-8"> <div class="title"> <i class="fa-solid fa-trophy" style="color: var(--global-theme-color);"></i> <a href="/publications/raschGoingGoingGone2023a">Going, Going, Gone: Exploring Intention Communication for Multi-User Locomotion in Virtual Reality</a> </div> <div class="author"> Julian Rasch, Vladislav Dmitrievic Rusakov, Martin Schmitz, and <em>Florian Müller</em> </div> <div class="periodical"> <em>In Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems</em>, Apr 2023 </div> <div class="periodical"> </div> <div class="links"> <a href="/assets/pdf/raschGoingGoingGone2023a.pdf" class="btn btn-sm z-depth-0" role="button"><i class="fa-solid fa-file-pdf"></i> PDF</a> <a href="https://dx.doi.org/10.1145/3544548.3581259" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="ai ai-acmdl"> ACM</i></a> <a href="https://www.youtube.com/watch?v=IC9XBi4Tr34" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-youtube"></i> Video</a> <a href="https://www.youtube.com/watch?v=k9ZvRsWYHDU" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="fa-solid fa-person-chalkboard"></i> Talk</a> </div> <div class="abstract hidden"> <p>Exploring virtual worlds together with others adds a social component to the Virtual Reality (VR) experience that increases connectedness. In the physical world, joint locomotion comes naturally through implicit intention communication and subsequent adjustments of the movement patterns. In VR, however, discrete locomotion techniques such as point&amp;teleport come without prior intention communication, hampering the collective experience. Related work proposes fixed groups, with a single person controlling the group movement, resulting in the loss of individual movement capabilities. To close the gap and mediate between these two extremes, we introduce three intention communication methods and explore them with two baseline methods. We contribute the results of a controlled experiment (n=20) investigating these methods from the perspective of a leader and a follower in a dyadic locomotion task. Our results suggest shared visualizations support the understanding of movement intentions, increasing the group feeling while maintaining individual freedom of movement.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/raschHandsOn3DPrinted2023a-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/raschHandsOn3DPrinted2023a-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/raschHandsOn3DPrinted2023a-1400.webp"></source> <img src="/assets/img/publication_preview/raschHandsOn3DPrinted2023a.jpg?fc8dfb7cab2d20f21a1d2b0e7182b613" class="preview z-depth-1 rounded" width="auto" height="auto" alt="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="raschHandsOn3DPrinted2023a" class="col-sm-8"> <div class="title"><a href="/publications/raschHandsOn3DPrinted2023a">Hands-On 3D Printed Electronics</a></div> <div class="author"> Julian Rasch, <em>Florian Müller</em>, Thomas Kosch, Martin Schmitz, and Sebastian S. Feger</div> <div class="periodical"> <em>In Proceedings of the Seventeenth International Conference on Tangible, Embedded, and Embodied Interaction</em>, Feb 2023 </div> <div class="periodical"> </div> <div class="links"> <a href="/assets/pdf/raschHandsOn3DPrinted2023a.pdf" class="btn btn-sm z-depth-0" role="button"><i class="fa-solid fa-file-pdf"></i> PDF</a> <a href="https://dx.doi.org/10.1145/3569009.3571846" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="ai ai-acmdl"> ACM</i></a> </div> <div class="abstract hidden"> <p>The parallel improvements in multi-material 3D printers and the quality of conductive filament open new possibilities for the fabrication of tangible and functional objects. In this studio, we discuss best practices for 3D printed electronics, talk about encountered problems, and derive design recommendations. We will guide the participants through a fabrication process by practically designing and printing objects. Consequently, we contemplate individual functional fabricated components, including small printed circuits and multi-material prints. We aim to spark a discussion about individually experienced challenges participants encountered during their design and fabrication process. This discussion includes problem-solving strategies, whose insights benefit other participants. Finally, we show the potential of printed electronics and discuss encouraging new opportunities in this field.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/schonTailorTwistAssessing2023a-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/schonTailorTwistAssessing2023a-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/schonTailorTwistAssessing2023a-1400.webp"></source> <img src="/assets/img/publication_preview/schonTailorTwistAssessing2023a.jpg?b7fbccefb537f8a118e10ed4ca796428" class="preview z-depth-1 rounded" width="auto" height="auto" alt="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="schonTailorTwistAssessing2023a" class="col-sm-8"> <div class="title"><a href="/publications/schonTailorTwistAssessing2023a">Tailor Twist: Assessing Rotational Mid-Air Interactions for Augmented Reality</a></div> <div class="author"> Dominik Schön, Thomas Kosch, <em>Florian Müller</em>, Martin Schmitz, <a href="https://www.sebastian-guenther.com/" rel="external nofollow noopener" target="_blank">Sebastian Günther</a>, Lukas Bommhardt, and Max Mühlhäuser</div> <div class="periodical"> <em>In Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems</em>, Apr 2023 </div> <div class="periodical"> </div> <div class="links"> <a href="/assets/pdf/schonTailorTwistAssessing2023a.pdf" class="btn btn-sm z-depth-0" role="button"><i class="fa-solid fa-file-pdf"></i> PDF</a> <a href="https://dx.doi.org/10.1145/3544548.3581461" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="ai ai-acmdl"> ACM</i></a> <a href="https://www.youtube.com/watch?v=25Nj5-MSnhA" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-youtube"></i> Video</a> <a href="https://www.youtube.com/watch?v=auieeDWFqjA" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="fa-solid fa-person-chalkboard"></i> Talk</a> </div> <div class="abstract hidden"> <p>Mid-air gestures, widely used in today’s Augmented Reality (AR) applications, are prone to the “gorilla arm” effect, leading to discomfort with prolonged interactions. While prior work has proposed metrics to quantify this effect and means to improve comfort and ergonomics, these works usually only consider simplistic, one-dimensional AR interactions, like reaching for a point or pushing a button. However, interacting with AR environments also involves far more complex tasks, such as rotational knobs, potentially impacting ergonomics. This paper advances the understanding of the ergonomics of rotational mid-air interactions in AR. For this, we contribute the results of a controlled experiment exposing the participants to a rotational task in the interaction space defined by their arms’ reach. Based on the results, we discuss how novel future mid-air gesture modalities benefit from our findings concerning ergonomic-aware rotational interaction.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/vonwillichDensingQueenExplorationMethods2023a-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/vonwillichDensingQueenExplorationMethods2023a-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/vonwillichDensingQueenExplorationMethods2023a-1400.webp"></source> <img src="/assets/img/publication_preview/vonwillichDensingQueenExplorationMethods2023a.jpg?58500483ccd1390a162b537079ec227e" class="preview z-depth-1 rounded" width="auto" height="auto" alt="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="vonwillichDensingQueenExplorationMethods2023a" class="col-sm-8"> <div class="title"><a href="/publications/vonwillichDensingQueenExplorationMethods2023a">DensingQueen: Exploration Methods for Spatial Dense Dynamic Data</a></div> <div class="author"> Julius von Willich, <a href="https://www.sebastian-guenther.com/" rel="external nofollow noopener" target="_blank">Sebastian Günther</a>, Andrii Matviienko, Martin Schmitz, <em>Florian Müller</em>, and Max Mühlhäuser</div> <div class="periodical"> <em>In Proceedings of the 2023 ACM Symposium on Spatial User Interaction</em>, Oct 2023 </div> <div class="periodical"> </div> <div class="links"> <a href="/assets/pdf/vonwillichDensingQueenExplorationMethods2023a.pdf" class="btn btn-sm z-depth-0" role="button"><i class="fa-solid fa-file-pdf"></i> PDF</a> <a href="https://dx.doi.org/10.1145/3607822.3614535" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="ai ai-acmdl"> ACM</i></a> <a href="https://github.com/LOEWE-emergenCITY/DensingQueen" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="fa-solid fa-code"></i> Code</a> </div> <div class="abstract hidden"> <p>Research has proposed various interaction techniques to manage the occlusion of 3D data in Virtual Reality (VR), e.g., via gradual refinement. However, tracking dynamically moving data in a dense 3D environment poses the challenge of ever-changing occlusion, especially if motion carries relevant information, which is lost in still images. In this paper, we evaluated two interaction modalities for Spatial Dense Dynamic Data (SDDD), adapted from existing interaction methods for static and spatial data. We evaluated these modalities for exploring SDDD in VR, in an experiment with 18 participants. Furthermore, we investigated the influence of our interaction modalities on different levels of data density on the users’ performance in a no-knowledge task and a prior-knowledge task. Our results indicated significantly degraded performance for higher levels of density. Further, we found that our flashlight-inspired modality successfully improved tracking in SDDD, while a cutting plane-inspired approach was more suitable for highlighting static volumes of interest, particularly in such high-density environments.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/weissUsingPseudoStiffnessEnrich2023a-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/weissUsingPseudoStiffnessEnrich2023a-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/weissUsingPseudoStiffnessEnrich2023a-1400.webp"></source> <img src="/assets/img/publication_preview/weissUsingPseudoStiffnessEnrich2023a.jpg?9a61c7a123617d4c6e39cc55dc93c42c" class="preview z-depth-1 rounded" width="auto" height="auto" alt="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="weissUsingPseudoStiffnessEnrich2023a" class="col-sm-8"> <div class="title"><a href="/publications/weissUsingPseudoStiffnessEnrich2023a">Using Pseudo-Stiffness to Enrich the Haptic Experience in Virtual Reality</a></div> <div class="author"> Yannick Weiss, Steeven Villa, Albrecht Schmidt, Sven Mayer, and <em>Florian Müller</em> </div> <div class="periodical"> <em>In Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems</em>, Apr 2023 </div> <div class="periodical"> </div> <div class="links"> <a href="/assets/pdf/weissUsingPseudoStiffnessEnrich2023a.pdf" class="btn btn-sm z-depth-0" role="button"><i class="fa-solid fa-file-pdf"></i> PDF</a> <a href="https://dx.doi.org/10.1145/3544548.3581223" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="ai ai-acmdl"> ACM</i></a> <a href="https://www.youtube.com/watch?v=Oex8NlPvcVU" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="fa-solid fa-person-chalkboard"></i> Talk</a> </div> <div class="abstract hidden"> <p>Providing users with a haptic sensation of the hardness and softness of objects in virtual reality is an open challenge. While physical props and haptic devices help, their haptic properties do not allow for dynamic adjustments. To overcome this limitation, we present a novel technique for changing the perceived stiffness of objects based on a visuo-haptic illusion. We achieved this by manipulating the hands’ Control-to-Display (C/D) ratio in virtual reality while pressing down on an object with fixed stiffness. In the first study (N=12), we determine the detection thresholds of the illusion. Our results show that we can exploit a C/D ratio from 0.7 to 3.5 without user detection. In the second study (N=12), we analyze the illusion’s impact on the perceived stiffness. Our results show that participants perceive the objects to be up to 28.1% softer and 8.9% stiffer, allowing for various haptic applications in virtual reality.</p> </div> </div> </div> </li> </ol> <h2 class="bibliography">2022</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/eskaProperPostureDesigning2022-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/eskaProperPostureDesigning2022-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/eskaProperPostureDesigning2022-1400.webp"></source> <img src="/assets/img/publication_preview/eskaProperPostureDesigning2022.jpg?111f6059753ecfc4363c4ca4656b0911" class="preview z-depth-1 rounded" width="auto" height="auto" alt="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="eskaProperPostureDesigning2022" class="col-sm-8"> <div class="title"><a href="/publications/eskaProperPostureDesigning2022">Proper Posture: Designing Posture Feedback Across Musical Instruments</a></div> <div class="author"> Bettina Eska, Jasmin Niess, and <em>Florian Müller</em> </div> <div class="periodical"> May 2022 </div> <div class="periodical"> </div> <div class="links"> <a href="/assets/pdf/eskaProperPostureDesigning2022.pdf" class="btn btn-sm z-depth-0" role="button"><i class="fa-solid fa-file-pdf"></i> PDF</a> <a href="http://arxiv.org/abs/2205.15110" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="ai ai-arxiv"></i> arxiv</a> </div> <div class="abstract hidden"> <p>There is a recommended body posture and hand position for playing every musical instrument, allowing efficient and quick movements without blockage. Due to humans’ limited cognitive capabilities, they struggle to concentrate on several things simultaneously and thus sometimes lose the correct position while playing their instrument. Incorrect positions when playing an instrument can lead to injuries and movement disorders in the long run. Previous work in HCI mainly focused on developing systems to assist in learning an instrument. However, the design space for posture correction when playing a musical instrument has not yet been explored. In this position paper, we present our vision of providing subtle vibrotactile or thermal feedback to guide the focus of attention back to the correct posture when playing a musical instrument. We discuss our concept with a focus on motion recognition and feedback modalities. Finally, we outline the next steps for future research.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/guntherSmoothSteelWool2022-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/guntherSmoothSteelWool2022-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/guntherSmoothSteelWool2022-1400.webp"></source> <img src="/assets/img/publication_preview/guntherSmoothSteelWool2022.jpg?10bc9f37b5f490714bfc006d69cdef93" class="preview z-depth-1 rounded" width="auto" height="auto" alt="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="guntherSmoothSteelWool2022" class="col-sm-8"> <div class="title"><a href="/publications/guntherSmoothSteelWool2022">Smooth as Steel Wool: Effects of Visual Stimuli on the Haptic Perception of Roughness in Virtual Reality</a></div> <div class="author"> <a href="https://www.sebastian-guenther.com/" rel="external nofollow noopener" target="_blank">Sebastian Günther</a>, Julian Rasch, Dominik Schön, <em>Florian Müller</em>, Martin Schmitz, Jan Riemann, Andrii Matviienko, and Max Mühlhäuser</div> <div class="periodical"> <em>In Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems</em>, Apr 2022 </div> <div class="periodical"> </div> <div class="links"> <a href="/assets/pdf/guntherSmoothSteelWool2022.pdf" class="btn btn-sm z-depth-0" role="button"><i class="fa-solid fa-file-pdf"></i> PDF</a> <a href="https://dx.doi.org/10.1145/3491102.3517454" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="ai ai-acmdl"> ACM</i></a> <a href="https://www.youtube.com/watch?v=IdbIvF004UA" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-youtube"></i> Video</a> <a href="https://www.youtube.com/watch?v=h0FQZQ26uoU" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="fa-solid fa-person-chalkboard"></i> Talk</a> </div> <div class="abstract hidden"> <p>Haptic Feedback is essential for lifelike Virtual Reality (VR) experiences. To provide a wide range of matching sensations of being touched or stroked, current approaches typically need large numbers of different physical textures. However, even advanced devices can only accommodate a limited number of textures to remain wearable. Therefore, a better understanding is necessary of how expectations elicited by different visualizations affect haptic perception, to achieve a balance between physical constraints and great variety of matching physical textures. In this work, we conducted an experiment (N=31) assessing how the perception of roughness is affected within VR. We designed a prototype for arm stroking and compared the effects of different visualizations on the perception of physical textures with distinct roughnesses. Additionally, we used the visualizations’ real-world materials, no-haptics and vibrotactile feedback as baselines. As one result, we found that two levels of roughness can be sufficient to convey a realistic illusion.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/koschNotiBikeAssessingTarget2022-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/koschNotiBikeAssessingTarget2022-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/koschNotiBikeAssessingTarget2022-1400.webp"></source> <img src="/assets/img/publication_preview/koschNotiBikeAssessingTarget2022.jpg?7f3c1cbd4b1671554505e3553dc45657" class="preview z-depth-1 rounded" width="auto" height="auto" alt="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="koschNotiBikeAssessingTarget2022" class="col-sm-8"> <div class="title"><a href="/publications/koschNotiBikeAssessingTarget2022">NotiBike: Assessing Target Selection Techniques for Cyclist Notifications in Augmented Reality</a></div> <div class="author"> Thomas Kosch, Andrii Matviienko, <em>Florian Müller</em>, Jessica Bersch, Christopher Katins, Dominik Schön, and Max Mühlhäuser</div> <div class="periodical"> <em>Proceedings of the ACM on Human-Computer Interaction</em>, Sep 2022 </div> <div class="periodical"> </div> <div class="links"> <a href="/assets/pdf/koschNotiBikeAssessingTarget2022.pdf" class="btn btn-sm z-depth-0" role="button"><i class="fa-solid fa-file-pdf"></i> PDF</a> <a href="https://dx.doi.org/10.1145/3546732" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="ai ai-acmdl"> ACM</i></a> <a href="https://www.youtube.com/watch?v=hTYBTULau7U" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-youtube"></i> Video</a> </div> <div class="abstract hidden"> <p>Cyclists’ attention is often compromised when interacting with notifications in traffic, hence increasing the likelihood of road accidents. To address this issue, we evaluate three notification interaction modalities and investigate their impact on the interaction performance while cycling: gaze-based Dwell Time, Gestures, and Manual And Gaze Input Cascaded (MAGIC) Pointing. In a user study (N=18), participants confirmed notifications in Augmented Reality (AR) using the three interaction modalities in a simulated biking scenario. We assessed the efficiency regarding reaction times, error rates, and perceived task load. Our results show significantly faster response times for MAGIC Pointing compared to Dwell Time and Gestures, while Dwell Time led to a significantly lower error rate compared to Gestures. Participants favored the MAGIC Pointing approach, supporting cyclists in AR selection tasks. Our research sets the boundaries for more comfortable and easier interaction with notifications and discusses implications for target selections in AR while cycling.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/Matviienko2022-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/Matviienko2022-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/Matviienko2022-1400.webp"></source> <img src="/assets/img/publication_preview/Matviienko2022.jpg?e3c4c358896b7c50c3d62362b58e5cb6" class="preview z-depth-1 rounded" width="auto" height="auto" alt="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Matviienko2022" class="col-sm-8"> <div class="title"><a href="/publications/Matviienko2022">SkyPort: Investigating 3D Teleportation Methods in Virtual Environments</a></div> <div class="author"> Andrii Matviienko, <em>Florian Müller</em>, Martin Schmitz, Marco Fendrich, and Max Mühlhäuser</div> <div class="periodical"> <em>In CHI Conference on Human Factors in Computing Systems</em>, Apr 2022 </div> <div class="periodical"> </div> <div class="links"> <a href="/assets/pdf/Matviienko2022.pdf" class="btn btn-sm z-depth-0" role="button"><i class="fa-solid fa-file-pdf"></i> PDF</a> <a href="https://dx.doi.org/10.1145/3491102.3501983" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="ai ai-acmdl"> ACM</i></a> <a href="https://www.youtube.com/watch?v=sOkzPZnlAeE" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="fa-solid fa-person-chalkboard"></i> Talk</a> </div> <div class="abstract hidden"> <p>Figure 1: Two possible scenarios for teleportation in 3D space: a user is teleporting horizontally to a target using a parabolic aiming method (left) and a user is teleporting vertically to a target using a linear aiming method (right). ABSTRACT Teleportation has become the de facto standard of locomotion in Virtual Reality (VR) environments. However, teleportation with parabolic and linear target aiming methods is restricted to horizontal 2D planes and it is unknown how they transfer to the 3D space. In this paper, we propose six 3D teleportation methods in virtual environments based on the combination of two existing aiming methods (linear and parabolic) and three types of transitioning to a target (instant, interpolated and continuous). To investigate the performance of the proposed teleportation methods, we conducted a controlled lab experiment (N = 24) with a mid-air coin collection task to assess accuracy, efciency and VR sickness. We discovered that the linear aiming method leads to faster and more accurate target selection. Moreover, a combination of linear aiming and instant transitioning leads to the highest efciency and accuracy without increasing VR sickness. • Human-centered computing \textrightarrow Virtual reality; User studies ; Empirical studies in HCI.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/matviienkoBabyYouCan2022-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/matviienkoBabyYouCan2022-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/matviienkoBabyYouCan2022-1400.webp"></source> <img src="/assets/img/publication_preview/matviienkoBabyYouCan2022.jpg?b64898264d289f1784d149427264e03d" class="preview z-depth-1 rounded" width="auto" height="auto" alt="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="matviienkoBabyYouCan2022" class="col-sm-8"> <div class="title"><a href="/publications/matviienkoBabyYouCan2022">Baby, You Can Ride My Bike: Exploring Maneuver Indications of Self-Driving Bicycles Using a Tandem Simulator</a></div> <div class="author"> Andrii Matviienko, Damir Mehmedovic, <em>Florian Müller</em>, and Max Mühlhäuser</div> <div class="periodical"> <em>Proceedings of the ACM on Human-Computer Interaction</em>, Sep 2022 </div> <div class="periodical"> </div> <div class="links"> <a href="/assets/pdf/matviienkoBabyYouCan2022.pdf" class="btn btn-sm z-depth-0" role="button"><i class="fa-solid fa-file-pdf"></i> PDF</a> <a href="https://dx.doi.org/10.1145/3546723" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="ai ai-acmdl"> ACM</i></a> <a href="https://www.youtube.com/watch?v=czOciHFRDk4" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-youtube"></i> Video</a> </div> <div class="abstract hidden"> <p>We envision a future where self-driving bicycles can take us to our destinations. This allows cyclists to use their time on the bike efficiently for work or relaxation without having to focus their attention on traffic. In the related field of self-driving cars, research has shown that communicating the planned route to passengers plays an important role in building trust in automation and situational awareness. For self-driving bicycles, this information transfer will be even more important, as riders will need to actively compensate for the movement of a self-driving bicycle to maintain balance. In this paper, we investigate maneuver indications for self-driving bicycles: (1) ambient light in a helmet, (2) head-up display indications, (3) speech feedback, (4) vibration on the handlebar, and (5) no assistance. To evaluate these indications, we conducted an outdoor experiment (N = 25) in a proposed tandem simulator consisting of a tandem bicycle with a steering and braking control on the back seat and a rider in full control of it. Our results indicate that riders respond faster to visual cues and focus comparably on the reading task while riding with and without maneuver indications. Additionally, we found that the tandem simulator is realistic, safe, and creates an awareness of a human cyclist controlling the tandem.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/matviienkoBikeARUnderstandingCyclists2022-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/matviienkoBikeARUnderstandingCyclists2022-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/matviienkoBikeARUnderstandingCyclists2022-1400.webp"></source> <img src="/assets/img/publication_preview/matviienkoBikeARUnderstandingCyclists2022.jpg?7f3f7bda34ae4657e0f4568b9732803b" class="preview z-depth-1 rounded" width="auto" height="auto" alt="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="matviienkoBikeARUnderstandingCyclists2022" class="col-sm-8"> <div class="title"><a href="/publications/matviienkoBikeARUnderstandingCyclists2022">BikeAR: Understanding Cyclists’ Crossing Decision-Making at Uncontrolled Intersections Using Augmented Reality</a></div> <div class="author"> Andrii Matviienko, <em>Florian Müller</em>, Dominik Schön, Paul Seesemann, <a href="https://www.sebastian-guenther.com/" rel="external nofollow noopener" target="_blank">Sebastian Günther</a>, and Max Mühlhäuser</div> <div class="periodical"> <em>In Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems</em>, Apr 2022 </div> <div class="periodical"> </div> <div class="links"> <a href="/assets/pdf/matviienkoBikeARUnderstandingCyclists2022.pdf" class="btn btn-sm z-depth-0" role="button"><i class="fa-solid fa-file-pdf"></i> PDF</a> <a href="https://dx.doi.org/10.1145/3491102.3517560" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="ai ai-acmdl"> ACM</i></a> <a href="https://www.youtube.com/watch?v=YKsDlPmSd68" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-youtube"></i> Video</a> <a href="https://www.youtube.com/watch?v=eA7D239WOd0" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="fa-solid fa-person-chalkboard"></i> Talk</a> </div> <div class="abstract hidden"> <p>Cycling has become increasingly popular as a means of transportation. However, cyclists remain a highly vulnerable group of road users. According to accident reports, one of the most dangerous situations for cyclists are uncontrolled intersections, where cars approach from both directions. To address this issue and assist cyclists in crossing decision-making at uncontrolled intersections, we designed two visualizations that: (1) highlight occluded cars through an X-ray vision and (2) depict the remaining time the intersection is safe to cross via a Countdown. To investigate the efficiency of these visualizations, we proposed an Augmented Reality simulation as a novel evaluation method, in which the above visualizations are represented as AR, and conducted a controlled experiment with 24 participants indoors. We found that the X-ray ensures a fast selection of shorter gaps between cars, while the Countdown facilitates a feeling of safety and provides a better intersection overview.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/matviienkoEScootARExploringUnimodal2022-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/matviienkoEScootARExploringUnimodal2022-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/matviienkoEScootARExploringUnimodal2022-1400.webp"></source> <img src="/assets/img/publication_preview/matviienkoEScootARExploringUnimodal2022.jpg?ab359f47f396ebd8c04f36bc8f583737" class="preview z-depth-1 rounded" width="auto" height="auto" alt="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="matviienkoEScootARExploringUnimodal2022" class="col-sm-8"> <div class="title"><a href="/publications/matviienkoEScootARExploringUnimodal2022">E-ScootAR: Exploring Unimodal Warnings for E-Scooter Riders in Augmented Reality</a></div> <div class="author"> Andrii Matviienko, <em>Florian Müller</em>, Dominik Schön, Régis Fayard, Salar Abaspur, Yi Li, and Max Mühlhäuser</div> <div class="periodical"> <em>In Extended Abstracts of the 2022 CHI Conference on Human Factors in Computing Systems</em>, Apr 2022 </div> <div class="periodical"> </div> <div class="links"> <a href="/assets/pdf/matviienkoEScootARExploringUnimodal2022.pdf" class="btn btn-sm z-depth-0" role="button"><i class="fa-solid fa-file-pdf"></i> PDF</a> <a href="https://dx.doi.org/10.1145/3491101.3519831" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="ai ai-acmdl"> ACM</i></a> <a href="https://www.youtube.com/watch?v=uYBIB51xgNs" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="fa-solid fa-person-chalkboard"></i> Talk</a> </div> <div class="abstract hidden"> <p>Micro-mobility is becoming a more popular means of transportation. However, this increased popularity brings its challenges. In particular, the accident rates for E-Scooter riders increase, which endangers the riders and other road users. In this paper, we explore the idea of augmenting E-Scooters with unimodal warnings to prevent collisions with other road users, which include Augmented Reality (AR) notifications, vibrotactile feedback on the handlebar, and auditory signals in the AR glasses. We conducted an outdoor experiment (N = 13) using an Augmented Reality simulation and compared these types of warnings in terms of reaction time, accident rate, and feeling of safety. Our results indicate that AR and auditory warnings lead to shorter reaction times, have a better perception, and create a better feeling of safety than vibrotactile warnings. Moreover, auditory signals have a higher acceptance by the riders compared to the other two types of warnings.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/matviienkoReducingVirtualReality2022-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/matviienkoReducingVirtualReality2022-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/matviienkoReducingVirtualReality2022-1400.webp"></source> <img src="/assets/img/publication_preview/matviienkoReducingVirtualReality2022.jpg?7d20f7f2c7992abeb0070c330f02decd" class="preview z-depth-1 rounded" width="auto" height="auto" alt="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="matviienkoReducingVirtualReality2022" class="col-sm-8"> <div class="title"><a href="/publications/matviienkoReducingVirtualReality2022">Reducing Virtual Reality Sickness for Cyclists in VR Bicycle Simulators</a></div> <div class="author"> Andrii Matviienko, <em>Florian Müller</em>, Marcel Zickler, Lisa Alina Gasche, Julia Abels, Till Steinert, and Max Mühlhäuser</div> <div class="periodical"> <em>In Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems</em>, Apr 2022 </div> <div class="periodical"> </div> <div class="links"> <a href="/assets/pdf/matviienkoReducingVirtualReality2022.pdf" class="btn btn-sm z-depth-0" role="button"><i class="fa-solid fa-file-pdf"></i> PDF</a> <a href="https://dx.doi.org/10.1145/3491102.3501959" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="ai ai-acmdl"> ACM</i></a> <a href="https://www.youtube.com/watch?v=BC5fpXVYnnA" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="fa-solid fa-person-chalkboard"></i> Talk</a> </div> <div class="abstract hidden"> <p>Virtual Reality (VR) bicycle simulations aim to recreate the feeling of riding a bicycle and are commonly used in many application areas. However, current solutions still create mismatches between the visuals and physical movement, which causes VR sickness and diminishes the cycling experience. To reduce VR sickness in bicycle simulators, we conducted two controlled lab experiments addressing two main causes of VR sickness: (1) steering methods and (2) cycling trajectory. In the first experiment (N = 18) we compared handlebar, HMD, and upper-body steering methods. In the second experiment (N = 24) we explored three types of movement in VR (1D, 2D, and 3D trajectories) and three countermeasures (airflow, vibration, and dynamic Field-of-View) to reduce VR sickness. We found that handlebar steering leads to the lowest VR sickness without decreasing cycling performance and airflow suggests to be the most promising method to reduce VR sickness for all three types of trajectories.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/muhlhauserMuC22Proceedings2022-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/muhlhauserMuC22Proceedings2022-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/muhlhauserMuC22Proceedings2022-1400.webp"></source> <img src="/assets/img/publication_preview/muhlhauserMuC22Proceedings2022.jpg?cf341dac2758dcffe7a2d62ceeaa2939" class="preview z-depth-1 rounded" width="auto" height="auto" alt="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="muhlhauserMuC22Proceedings2022" class="col-sm-8"> <div class="title"><a href="/publications/muhlhauserMuC22Proceedings2022">MuC ’22: Proceedings of Mensch Und Computer 2022</a></div> <div class="author"> Max Mühlhauser, Christian Reuter, Bastian Pfleging, Thomas Kosch, Andrii Matviienko, Kathrin Gerling, Sven Mayer, Wilko Heuten, Tanja Döring, <em>Florian Müller</em>, and Martin Schmitz</div> <div class="periodical"> Sep 2022 </div> <div class="periodical"> </div> <div class="links"> <a href="https://dx.doi.org/10.1145/3543758" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="ai ai-acmdl"> ACM</i></a> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/schmitz2022squeezy-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/schmitz2022squeezy-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/schmitz2022squeezy-1400.webp"></source> <img src="/assets/img/publication_preview/schmitz2022squeezy.jpg?0b0f943a76e07fc766e1044fc5cae841" class="preview z-depth-1 rounded" width="auto" height="auto" alt="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="schmitz2022squeezy" class="col-sm-8"> <div class="title"><a href="/publications/schmitz2022squeezy">Squeezy-Feely: Investigating Lateral Thumb-Index Pinching as an Input Modality</a></div> <div class="author"> Martin Schmitz, <a href="https://www.sebastian-guenther.com/" rel="external nofollow noopener" target="_blank">Sebastian Günther</a>, Dominik Schön, and <em>Florian Müller</em> </div> <div class="periodical"> <em>In CHI Conference on Human Factors in Computing Systems</em>, Apr 2022 </div> <div class="periodical"> </div> <div class="links"> <a href="/assets/pdf/schmitz2022squeezy.pdf" class="btn btn-sm z-depth-0" role="button"><i class="fa-solid fa-file-pdf"></i> PDF</a> <a href="https://dx.doi.org/10.1145/3491102.3501981" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="ai ai-acmdl"> ACM</i></a> <a href="https://www.youtube.com/watch?v=hoHcyrAqTeM" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-youtube"></i> Video</a> <a href="https://www.youtube.com/watch?v=MD3WY1FIUaA" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="fa-solid fa-person-chalkboard"></i> Talk</a> </div> <div class="abstract hidden"> <p>From zooming on smartphones and mid-air gestures to deformable user interfaces, thumb-index pinching grips are used in many interaction techniques. However, there is still a lack of systematic understanding of how the accuracy and efficiency of such grips are affected by various factors such as counterforce, grip span, and grip direction. Therefore, in this paper, we contribute an evaluation (N = 18) of thumb-index pinching performance in a visual targeting task using scales up to 75 items. As part of our findings, we conclude that the pinching interaction between the thumb and index finger is a promising modality also for one-dimensional input on higher scales. Furthermore, we discuss and outline implications for future user interfaces that benefit from pinching as an additional and complementary interaction modality.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/schmitzRethinkingSmartObjects2022-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/schmitzRethinkingSmartObjects2022-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/schmitzRethinkingSmartObjects2022-1400.webp"></source> <img src="/assets/img/publication_preview/schmitzRethinkingSmartObjects2022.jpg?bd0c632da8659b02e4844f839dbad9df" class="preview z-depth-1 rounded" width="auto" height="auto" alt="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="schmitzRethinkingSmartObjects2022" class="col-sm-8"> <div class="title"><a href="/publications/schmitzRethinkingSmartObjects2022">Rethinking Smart Objects: The International Workshop on Interacting with Smart Objects in Interactive Spaces</a></div> <div class="author"> Martin Schmitz, <a href="https://www.sebastian-guenther.com/" rel="external nofollow noopener" target="_blank">Sebastian Günther</a>, Karola Marky, <em>Florian Müller</em>, Andrii Matviienko, Alexandra Voit, Roberts Marky, Max Mühlhäuser, and Thomas Kosch</div> <div class="periodical"> <em>In Companion Proceedings of the 2022 Conference on Interactive Surfaces and Spaces</em>, Nov 2022 </div> <div class="periodical"> </div> <div class="links"> <a href="/assets/pdf/schmitzRethinkingSmartObjects2022.pdf" class="btn btn-sm z-depth-0" role="button"><i class="fa-solid fa-file-pdf"></i> PDF</a> <a href="https://dx.doi.org/10.1145/3532104.3571470" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="ai ai-acmdl"> ACM</i></a> <a href="https://smart-objects.org/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="fas fa-link"></i> Website</a> </div> <div class="abstract hidden"> <p>The increasing proliferation of smart objects in everyday life has changed how we interact with computers. Instead of concentrating computational capabilities and interaction into one device, everyday objects have naturally integrated parts of interactive features. Although this has led to many practical applications, the possibilities for explicit or implicit interaction with such objects are still limited in interaction spaces. We still often rely on smartphones as interactive hubs for controlling smart objects, hence not fulfilling the vision of truly smart objects. The workshop Rethinking Smart Objects invites practitioners and researchers from both academia and industry to discuss novel interaction paradigms and the integration and societal implications of using smart objects in interactive space. This workshop will include an action plan with leading questions, aiming to move the research field forward.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/schonTrackItPipeFabricationPipeline2022-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/schonTrackItPipeFabricationPipeline2022-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/schonTrackItPipeFabricationPipeline2022-1400.webp"></source> <img src="/assets/img/publication_preview/schonTrackItPipeFabricationPipeline2022.jpg?1f9abd349fc9a777948b504bc3bd287d" class="preview z-depth-1 rounded" width="auto" height="auto" alt="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="schonTrackItPipeFabricationPipeline2022" class="col-sm-8"> <div class="title"><a href="/publications/schonTrackItPipeFabricationPipeline2022">TrackItPipe: A Fabrication Pipeline To Incorporate Location and Rotation Tracking Into 3D Printed Objects</a></div> <div class="author"> Dominik Schön, Thomas Kosch, Martin Schmitz, <em>Florian Müller</em>, <a href="https://www.sebastian-guenther.com/" rel="external nofollow noopener" target="_blank">Sebastian Günther</a>, Johannes Kreutz, and Max Mühlhäuser</div> <div class="periodical"> <em>In Adjunct Proceedings of the 35th Annual ACM Symposium on User Interface Software and Technology</em>, Oct 2022 </div> <div class="periodical"> </div> <div class="links"> <a href="/assets/pdf/schonTrackItPipeFabricationPipeline2022.pdf" class="btn btn-sm z-depth-0" role="button"><i class="fa-solid fa-file-pdf"></i> PDF</a> <a href="https://dx.doi.org/10.1145/3526114.3558719" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="ai ai-acmdl"> ACM</i></a> <a href="https://github.com/Dominik-Schoen/TrackItPipe" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="fa-solid fa-code"></i> Code</a> </div> <div class="abstract hidden"> <p>The increasing convergence of the digital and physical world creates a growing urgency to integrate 3D printed physical tangibles with virtual environments. A precise position and rotation tracking are essential to integrate such physical objects with a virtual environment. However, available 3D models commonly do not provide tracking support on their composition, which requires modifications by CAD experts. This poses a challenge for users with no prior CAD experience. This work presents TrackItPipe, a fabrication pipeline supporting users by semi-automatically adding tracking capabilities for 3D printable tangibles tailored to environmental requirements. TrackItPipe integrates modifications to the 3D model, produces the respective tangibles for 3D printing, and provides integration scripts for Mixed Reality. Using TrackItPipe, users can rapidly equip objects with tracking capabilities.</p> </div> </div> </div> </li> </ol> <h2 class="bibliography">2021</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/Elsayed2021a-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/Elsayed2021a-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/Elsayed2021a-1400.webp"></source> <img src="/assets/img/publication_preview/Elsayed2021a.jpg?f3388890b17b274b9ad5f15a4dab8e49" class="preview z-depth-1 rounded" width="auto" height="auto" alt="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Elsayed2021a" class="col-sm-8"> <div class="title"><a href="/publications/Elsayed2021a">CameraReady: Assessing the Influence of Display Types and Visualizations on Posture Guidance</a></div> <div class="author"> Hesham Elsayed, Philipp Hoffmann, <a href="https://www.sebastian-guenther.com/" rel="external nofollow noopener" target="_blank">Sebastian Günther</a>, Martin Schmitz, Martin Weigel, Max Mühlhäuser, and <em>Florian Müller</em> </div> <div class="periodical"> <em>In Designing Interactive Systems Conference 2021</em>, Jun 2021 </div> <div class="periodical"> </div> <div class="links"> <a href="/assets/pdf/Elsayed2021a.pdf" class="btn btn-sm z-depth-0" role="button"><i class="fa-solid fa-file-pdf"></i> PDF</a> <a href="https://dx.doi.org/10.1145/3461778.3462026" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="ai ai-acmdl"> ACM</i></a> <a href="https://www.youtube.com/watch?v=9OWH4eBrHtk" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-youtube"></i> Video</a> <a href="https://www.youtube.com/watch?v=KAjwxyqAegY" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="fa-solid fa-person-chalkboard"></i> Talk</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/guntherActuBoardOpenRapid2021-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/guntherActuBoardOpenRapid2021-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/guntherActuBoardOpenRapid2021-1400.webp"></source> <img src="/assets/img/publication_preview/guntherActuBoardOpenRapid2021.jpg?70c74cf65d4643bca155bf8384fe5933" class="preview z-depth-1 rounded" width="auto" height="auto" alt="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="guntherActuBoardOpenRapid2021" class="col-sm-8"> <div class="title"><a href="/publications/guntherActuBoardOpenRapid2021">ActuBoard: An Open Rapid Prototyping Platform to Integrate Hardware Actuators in Remote Applications</a></div> <div class="author"> <a href="https://www.sebastian-guenther.com/" rel="external nofollow noopener" target="_blank">Sebastian Günther</a>, <em>Florian Müller</em>, Felix Hübner, Max Mühlhäuser, and Andrii Matviienko</div> <div class="periodical"> <em>In Companion of the 2021 ACM SIGCHI Symposium on Engineering Interactive Computing Systems</em>, Jun 2021 </div> <div class="periodical"> </div> <div class="links"> <a href="/assets/pdf/guntherActuBoardOpenRapid2021.pdf" class="btn btn-sm z-depth-0" role="button"><i class="fa-solid fa-file-pdf"></i> PDF</a> <a href="https://dx.doi.org/10.1145/3459926.3464757" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="ai ai-acmdl"> ACM</i></a> <a href="https://git.tk.informatik.tu-darmstadt.de/sebastian.guenther/actuboard-public" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="fa-solid fa-code"></i> Code</a> </div> <div class="abstract hidden"> <p>Prototyping is an essential step in developing tangible experiences and novel devices, ranging from haptic feedback to wearables. However, prototyping of actuated devices nowadays often requires repetitive and time-consuming steps, such as wiring, soldering, and programming basic communication, before HCI researchers and designers can focus on their primary interest: designing interaction. In this paper, we present ActuBoard, a prototyping platform to support 1) quick assembly, 2) less preparation work, and 3) the inclusion of non-tech-savvy users. With ActuBoard, users are not required to create complex circuitry, write a single line of firmware, or implementing communication protocols. Acknowledging existing systems, our platform combines the flexibility of low-level microcontrollers and ease-of-use of abstracted tinker platforms to control actuators from separate applications. As further contribution, we highlight the technical specifications and published the ActuBoard platform as Open Source.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/markyLetFretsAssisting2021-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/markyLetFretsAssisting2021-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/markyLetFretsAssisting2021-1400.webp"></source> <img src="/assets/img/publication_preview/markyLetFretsAssisting2021.jpg?e0256944aad12f8f123e8e035ed50d4c" class="preview z-depth-1 rounded" width="auto" height="auto" alt="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="markyLetFretsAssisting2021" class="col-sm-8"> <div class="title"><a href="/publications/markyLetFretsAssisting2021">Let’s Frets! Assisting Guitar Students During Practice via Capacitive Sensing</a></div> <div class="author"> Karola Marky, Andreas Weiß, Andrii Matviienko, Florian Brandherm, Sebastian Wolf, Martin Schmitz, Florian Krell, <em>Florian Müller</em>, Max Mühlhäuser, and Thomas Kosch</div> <div class="periodical"> <em>In Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems</em>, May 2021 </div> <div class="periodical"> </div> <div class="links"> <a href="/assets/pdf/markyLetFretsAssisting2021.pdf" class="btn btn-sm z-depth-0" role="button"><i class="fa-solid fa-file-pdf"></i> PDF</a> <a href="https://dx.doi.org/10.1145/3411764.3445595" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="ai ai-acmdl"> ACM</i></a> <a href="https://www.youtube.com/watch?v=vFx8c5aF6vA" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-youtube"></i> Video</a> <a href="https://www.youtube.com/watch?v=YhLuCpgnaBg" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="fa-solid fa-person-chalkboard"></i> Talk</a> </div> <div class="abstract hidden"> <p>Learning a musical instrument requires regular exercise. However, students are often on their own during their practice sessions due to the limited time with their teachers, which increases the likelihood of mislearning playing techniques. To address this issue, we present Let’s Frets - a modular guitar learning system that provides visual indicators and capturing of finger positions on a 3D-printed capacitive guitar fretboard. We based the design of Let’s Frets on requirements collected through in-depth interviews with professional guitarists and teachers. In a user study (N=24), we evaluated the feedback modules of Let’s Frets against fretboard charts. Our results show that visual indicators require the least time to realize new finger positions while a combination of visual indicators and position capturing yielded the highest playing accuracy. We conclude how Let’s Frets enables independent practice sessions that can be translated to other musical instruments.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/markyLetFretsMastering2021-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/markyLetFretsMastering2021-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/markyLetFretsMastering2021-1400.webp"></source> <img src="/assets/img/publication_preview/markyLetFretsMastering2021.jpg?6a42def7a67c50d16c60ca0f25465f47" class="preview z-depth-1 rounded" width="auto" height="auto" alt="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="markyLetFretsMastering2021" class="col-sm-8"> <div class="title"><a href="/publications/markyLetFretsMastering2021">Let’s Frets! Mastering Guitar Playing with Capacitive Sensing and Visual Guidance</a></div> <div class="author"> Karola Marky, Andreas Weiß, <em>Florian Müller</em>, Martin Schmitz, Max Mühlhäuser, and Thomas Kosch</div> <div class="periodical"> <em>In Extended Abstracts of the 2021 CHI Conference on Human Factors in Computing Systems</em>, May 2021 </div> <div class="periodical"> </div> <div class="links"> <a href="/assets/pdf/markyLetFretsMastering2021.pdf" class="btn btn-sm z-depth-0" role="button"><i class="fa-solid fa-file-pdf"></i> PDF</a> <a href="https://dx.doi.org/10.1145/3411763.3451536" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="ai ai-acmdl"> ACM</i></a> <a href="https://www.youtube.com/watch?v=a0C4TkbiRfg" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-youtube"></i> Video</a> </div> <div class="abstract hidden"> <p>Mastering the guitar requires regular exercise to develop new skills and maintain existing abilities. We present Let’s Frets - a modular guitar support system that provides visual guidance through LEDs that are integrated into a capacitive fretboard to support the practice of chords, scales, melodies, and exercises. Additional feedback is provided through a 3D-printed fretboard that senses the finger positions through capacitive sensing. We envision Let’s Frets as an integrated guitar support system that raises the awareness of guitarists about their playing styles, their training progress, the composition of new pieces, and facilitating remote collaborations between teachers as well as guitar students. This interactivity demonstrates Let’s Frets with an augmented fretboard and supporting software that runs on a mobile device.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/matviienkoVRtangiblesAssistingChildren2021-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/matviienkoVRtangiblesAssistingChildren2021-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/matviienkoVRtangiblesAssistingChildren2021-1400.webp"></source> <img src="/assets/img/publication_preview/matviienkoVRtangiblesAssistingChildren2021.jpg?aefcf94f2ce570535fa5d8685bbd9f6e" class="preview z-depth-1 rounded" width="auto" height="auto" alt="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="matviienkoVRtangiblesAssistingChildren2021" class="col-sm-8"> <div class="title"><a href="/publications/matviienkoVRtangiblesAssistingChildren2021">VRtangibles: Assisting Children in Creating Virtual Scenes Using Tangible Objects and Touch Input</a></div> <div class="author"> Andrii Matviienko, Marcel Langer, <em>Florian Müller</em>, Martin Schmitz, and Max Mühlhäuser</div> <div class="periodical"> <em>In Extended Abstracts of the 2021 CHI Conference on Human Factors in Computing Systems</em>, May 2021 </div> <div class="periodical"> </div> <div class="links"> <a href="/assets/pdf/matviienkoVRtangiblesAssistingChildren2021.pdf" class="btn btn-sm z-depth-0" role="button"><i class="fa-solid fa-file-pdf"></i> PDF</a> <a href="https://dx.doi.org/10.1145/3411763.3451671" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="ai ai-acmdl"> ACM</i></a> <a href="https://www.youtube.com/watch?v=wW-d9yokhYI" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="fa-solid fa-person-chalkboard"></i> Talk</a> </div> <div class="abstract hidden"> <p>Children are increasingly exposed to virtual reality (VR) technology as end-users. However, they miss an opportunity to become active creators due to the barrier of insufficient technical background. Creating scenes in VR requires considerable programming knowledge and excludes non-tech-savvy users, e.g., school children. In this paper, we showcase a system called VRtangibles, which combines tangible objects and touch input to create virtual scenes without programming. With VRtangibles, we aim to engage children in the active creation of virtual scenes via playful hands-on activities. From the lab study with six school children, we discovered that the majority of children were successful in creating virtual scenes using VRtangibles and found it engaging and fun to use.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/schmitzItsyBitsFabricationRecognition2021-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/schmitzItsyBitsFabricationRecognition2021-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/schmitzItsyBitsFabricationRecognition2021-1400.webp"></source> <img src="/assets/img/publication_preview/schmitzItsyBitsFabricationRecognition2021.jpg?04181ba02244786ce6fd967db16b7940" class="preview z-depth-1 rounded" width="auto" height="auto" alt="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="schmitzItsyBitsFabricationRecognition2021" class="col-sm-8"> <div class="title"><a href="/publications/schmitzItsyBitsFabricationRecognition2021">Itsy-Bits: Fabrication and Recognition of 3D-Printed Tangibles with Small Footprints on Capacitive Touchscreens</a></div> <div class="author"> Martin Schmitz, <em>Florian Müller</em>, Max Mühlhäuser, Jan Riemann, and Huy Viet Viet Le</div> <div class="periodical"> <em>In Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems</em>, May 2021 </div> <div class="periodical"> </div> <div class="links"> <a href="/assets/pdf/schmitzItsyBitsFabricationRecognition2021.pdf" class="btn btn-sm z-depth-0" role="button"><i class="fa-solid fa-file-pdf"></i> PDF</a> <a href="https://dx.doi.org/10.1145/3411764.3445502" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="ai ai-acmdl"> ACM</i></a> <a href="https://www.youtube.com/watch?v=WrdRQlt2fsA" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-youtube"></i> Video</a> <a href="https://www.youtube.com/watch?v=fI3zZz4fnMY" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="fa-solid fa-person-chalkboard"></i> Talk</a> </div> <div class="abstract hidden"> <p>Tangibles on capacitive touchscreens are a promising approach to overcome the limited expressiveness of touch input. While research has suggested many approaches to detect tangibles, the corresponding tangibles are either costly or have a considerable minimal size. This makes them bulky and unattractive for many applications. At the same time, they obscure valuable display space for interaction. To address these shortcomings, we contribute Itsy-Bits: a fabrication pipeline for 3D printing and recognition of tangibles on capacitive touchscreens with a footprint as small as a fingertip. Each Itsy-Bit consists of an enclosing 3D object and a unique conductive 2D shape on its bottom. Using only raw data of commodity capacitive touchscreens, Itsy-Bits reliably identifies and locates a variety of shapes in different sizes and estimates their orientation. Through example applications and a technical evaluation, we demonstrate the feasibility and applicability of Itsy-Bits for tangibles with small footprints.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/schmitzOhSnapFabrication2021-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/schmitzOhSnapFabrication2021-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/schmitzOhSnapFabrication2021-1400.webp"></source> <img src="/assets/img/publication_preview/schmitzOhSnapFabrication2021.jpg?550018af3814c222da72707d0ef0fbf0" class="preview z-depth-1 rounded" width="auto" height="auto" alt="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="schmitzOhSnapFabrication2021" class="col-sm-8"> <div class="title"><a href="/publications/schmitzOhSnapFabrication2021">Oh, Snap! A Fabrication Pipeline to Magnetically Connect Conventional and 3D-Printed Electronics</a></div> <div class="author"> Martin Schmitz, Jan Riemann, <em>Florian Müller</em>, Steffen Kreis, and Max Mühlhäuser</div> <div class="periodical"> <em>In Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems</em>, May 2021 </div> <div class="periodical"> </div> <div class="links"> <a href="/assets/pdf/schmitzOhSnapFabrication2021.pdf" class="btn btn-sm z-depth-0" role="button"><i class="fa-solid fa-file-pdf"></i> PDF</a> <a href="https://dx.doi.org/10.1145/3411764.3445641" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="ai ai-acmdl"> ACM</i></a> <a href="https://www.youtube.com/watch?v=JX3ZwKnnJVs" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-youtube"></i> Video</a> <a href="https://www.youtube.com/watch?v=0AUrrtwaPVQ" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="fa-solid fa-person-chalkboard"></i> Talk</a> <a href="https://github.com/Telecooperation/oh-snap" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="fa-solid fa-code"></i> Code</a> </div> <div class="abstract hidden"> <p>3D printing has revolutionized rapid prototyping by speeding up the creation of custom-shaped objects. With the rise of multi-material 3D printers, these custom-shaped objects can now be made interactive in a single pass through passive conductive structures. However, connecting conventional electronics to these conductive structures often still requires time-consuming manual assembly involving many wires, soldering or gluing. To alleviate these shortcomings, we propose : a fabrication pipeline and interfacing concept to magnetically connect a 3D-printed object equipped with passive sensing structures to conventional sensing electronics. To this end, utilizes ferromagnetic and conductive 3D-printed structures, printable in a single pass on standard printers. We further present a proof-of-concept capacitive sensing board that enables easy and robust magnetic assembly to quickly create interactive 3D-printed objects. We evaluate by assessing the robustness and quality of the connection and demonstrate its broad applicability by a series of example applications.</p> </div> </div> </div> </li> </ol> <h2 class="bibliography">2020</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/Elsayed2020-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/Elsayed2020-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/Elsayed2020-1400.webp"></source> <img src="/assets/img/publication_preview/Elsayed2020.jpg?22b785b2a027d77e764e0342556e3317" class="preview z-depth-1 rounded" width="auto" height="auto" alt="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Elsayed2020" class="col-sm-8"> <div class="title"><a href="/publications/Elsayed2020">VRSketchPen: Unconstrained Haptic Assistance for Sketching in Virtual 3D Environments</a></div> <div class="author"> Hesham Elsayed, Mayra Donaji Barrera Machuca, Christian Schaarschmidt, Karola Marky, <em>Florian Müller</em>, Jan Riemann, Andrii Matviienko, Martin Schmitz, Martin Weigel, and Max Mühlhäuser</div> <div class="periodical"> <em>In 26th ACM Symposium on Virtual Reality Software and Technology</em>, Nov 2020 </div> <div class="periodical"> </div> <div class="links"> <a href="/assets/pdf/Elsayed2020.pdf" class="btn btn-sm z-depth-0" role="button"><i class="fa-solid fa-file-pdf"></i> PDF</a> <a href="https://dx.doi.org/10.1145/3385956.3418953" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="ai ai-acmdl"> ACM</i></a> </div> <div class="abstract hidden"> <p>Accurate sketching in virtual 3D environments is challenging due to aspects like limited depth perception or the absence of physical support. To address this issue, we propose VRSketchPen - a pen that uses two haptic modalities to support virtual sketching without constraining user actions: (1) pneumatic force feedback to simulate the contact pressure of the pen against virtual surfaces and (2) vibrotactile feedback to mimic textures while moving the pen over virtual surfaces. To evaluate VRSketchPen, we conducted a lab experiment with 20 participants to compare (1) pneumatic, (2) vibrotactile and (3) a combination of both with (4) snapping and no assistance for flat and curved surfaces in a 3D virtual environment. Our findings show that usage of pneumatic, vibrotactile and their combination significantly improves 2D shape accuracy and leads to diminished depth errors for flat and curved surfaces. Qualitative results indicate that users find the addition of unconstraining haptic feedback to significantly improve convenience, confidence and user experience.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/elsayedVibroMapUnderstandingSpacing2020-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/elsayedVibroMapUnderstandingSpacing2020-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/elsayedVibroMapUnderstandingSpacing2020-1400.webp"></source> <img src="/assets/img/publication_preview/elsayedVibroMapUnderstandingSpacing2020.jpg?26d3c48ce59a922c4f3c7e995b8bae0b" class="preview z-depth-1 rounded" width="auto" height="auto" alt="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="elsayedVibroMapUnderstandingSpacing2020" class="col-sm-8"> <div class="title"><a href="/publications/elsayedVibroMapUnderstandingSpacing2020">VibroMap: Understanding the Spacing of Vibrotactile Actuators across the Body</a></div> <div class="author"> Hesham Elsayed, Martin Weigel, <em>Florian Müller</em>, Martin Schmitz, Karola Marky, <a href="https://www.sebastian-guenther.com/" rel="external nofollow noopener" target="_blank">Sebastian Günther</a>, Jan Riemann, and Max Mühlhäuser</div> <div class="periodical"> <em>Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies</em>, Dec 2020 </div> <div class="periodical"> </div> <div class="links"> <a href="/assets/pdf/elsayedVibroMapUnderstandingSpacing2020.pdf" class="btn btn-sm z-depth-0" role="button"><i class="fa-solid fa-file-pdf"></i> PDF</a> <a href="https://dx.doi.org/10.1145/3432189" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="ai ai-acmdl"> ACM</i></a> <a href="https://www.youtube.com/watch?v=5vRKEkogg5A" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="fa-solid fa-person-chalkboard"></i> Talk</a> </div> <div class="abstract hidden"> <p>In spite of the great potential of on-body vibrotactile displays for a variety of applications, research lacks an understanding of the spacing between vibrotactile actuators. Through two experiments, we systematically investigate vibrotactile perception on the wrist, forearm, upper arm, back, torso, thigh, and leg, each in transverse and longitudinal body orientation. In the first experiment, we address the maximum distance between vibration motors that still preserves the ability to generate phantom sensations. In the second experiment, we investigate the perceptual accuracy of localizing vibrations in order to establish the minimum distance between vibration motors. Based on the results, we derive VibroMap, a spatial map of the functional range of inter-motor distances across the body. VibroMap supports hardware and interaction designers with design guidelines for constructing body-worn vibrotactile displays.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/Gunther2020-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/Gunther2020-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/Gunther2020-1400.webp"></source> <img src="/assets/img/publication_preview/Gunther2020.jpg?ee4d2cd607865dc09cef591c628bac0d" class="preview z-depth-1 rounded" width="auto" height="auto" alt="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Gunther2020" class="col-sm-8"> <div class="title"><a href="/publications/Gunther2020">Therminator : Understanding the Interdependency of Visual and On-Body Thermal Feedback in Virtual Reality</a></div> <div class="author"> <a href="https://www.sebastian-guenther.com/" rel="external nofollow noopener" target="_blank">Sebastian Günther</a>, Omar Elmoghazy, <em>Florian Müller</em>, Max Mühlhäuser, Dominik Schön, and Martin Schmitz</div> <div class="periodical"> <em>In Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems - CHI ’20</em>, Dec 2020 </div> <div class="periodical"> </div> <div class="links"> <a href="/assets/pdf/Gunther2020.pdf" class="btn btn-sm z-depth-0" role="button"><i class="fa-solid fa-file-pdf"></i> PDF</a> <a href="https://dx.doi.org/10.1145/3313831.3376195" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="ai ai-acmdl"> ACM</i></a> <a href="https://www.youtube.com/watch?v=q5lkmqAua78" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-youtube"></i> Video</a> <a href="https://www.youtube.com/watch?v=p36TkvjTXfc" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="fa-solid fa-person-chalkboard"></i> Talk</a> </div> <div class="abstract hidden"> <p>Figure 1. Therminator concepts and example VR applications showing (a) a user during our experiment with a snow visual stimulus, (b) a cold game environment with a user throwing snowballs, (c) a warm tropical islands, and (d) a firefighting simulation with a user extinguishing flames. ABSTRACT Recent advances have made Virtual Reality (VR) more realistic than ever before. This improved realism is attributed to to-day’s ability to increasingly appeal to human sensations, such as visual, auditory or tactile. While research also examines temperature sensation as an important aspect, the interdepen-dency of visual and thermal perception in VR is still underex-plored. In this paper, we propose Therminator, a thermal display concept that provides warm and cold on-body feedback in VR through heat conduction of flowing liquids with different temperatures. Further, we systematically evaluate the interdependency of different visual and thermal stimuli on the temperature perception of arm and abdomen with 25 participants. As part of the results, we found varying temperature perception depending on the stimuli, as well as increasing involvement of users during conditions with matching stimuli.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/Gunther2020a-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/Gunther2020a-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/Gunther2020a-1400.webp"></source> <img src="/assets/img/publication_preview/Gunther2020a.jpg?5ac386b1e61417bda723ffac5a874a4a" class="preview z-depth-1 rounded" width="auto" height="auto" alt="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Gunther2020a" class="col-sm-8"> <div class="title"><a href="/publications/Gunther2020a">PneumoVolley: Pressure-based Haptic Feedback on the Head through Pneumatic Actuation</a></div> <div class="author"> <a href="https://www.sebastian-guenther.com/" rel="external nofollow noopener" target="_blank">Sebastian Günther</a>, Dominik Schön, <em>Florian Müller</em>, Max Mühlhäuser, and Martin Schmitz</div> <div class="periodical"> <em>In Extended Abstracts of the 2020 CHI Conference on Human Factors in Computing Systems - CHI ’20</em>, Dec 2020 </div> <div class="periodical"> </div> <div class="links"> <a href="/assets/pdf/Gunther2020a.pdf" class="btn btn-sm z-depth-0" role="button"><i class="fa-solid fa-file-pdf"></i> PDF</a> <a href="https://dx.doi.org/10.1145/3334480.3382916" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="ai ai-acmdl"> ACM</i></a> <a href="https://www.youtube.com/watch?v=ZKnV8HrUx9M" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-youtube"></i> Video</a> <a href="https://www.youtube.com/watch?v=QWLsdfNxgeA" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="fa-solid fa-person-chalkboard"></i> Talk</a> </div> <div class="abstract hidden"> <p>Haptic Feedback brings immersion and presence in Virtual Reality (VR) to the next level. While research proposes the usage of various tactile sensations, such as vibration or ultrasound approaches, the potential applicability of pressure feedback on the head is still under-explored. In this paper, we contribute concepts and design considerations for pressure-based feedback on the head through pneumatic actuation. As a proof-of-concept implementing our pressure-based haptics, we further present PneumoVolley: a VR experience similar to the classic Volleyball game but played with the head. In an exploratory user study with 9 participants, we evaluated our concepts and identified a significantly increased involvement compared to a no-haptics baseline along with high realism and enjoyment ratings using pressure-based feedback on the head in VR. LBW033, Page 1 CHI 2020 Late-Breaking Work</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/meurischExploringUserExpectations2020-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/meurischExploringUserExpectations2020-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/meurischExploringUserExpectations2020-1400.webp"></source> <img src="/assets/img/publication_preview/meurischExploringUserExpectations2020.jpg?1f5fd35699404ccaad522d38549af4a4" class="preview z-depth-1 rounded" width="auto" height="auto" alt="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="meurischExploringUserExpectations2020" class="col-sm-8"> <div class="title"><a href="/publications/meurischExploringUserExpectations2020">Exploring User Expectations of Proactive AI Systems</a></div> <div class="author"> Christian Meurisch, Cristina A. Mihale-Wilson, Adrian Hawlitschek, Florian Giger, <em>Florian Müller</em>, Oliver Hinz, and Max Mühlhäuser</div> <div class="periodical"> <em>Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies</em>, Dec 2020 </div> <div class="periodical"> </div> <div class="links"> <a href="/assets/pdf/meurischExploringUserExpectations2020.pdf" class="btn btn-sm z-depth-0" role="button"><i class="fa-solid fa-file-pdf"></i> PDF</a> <a href="https://dx.doi.org/10.1145/3432193" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="ai ai-acmdl"> ACM</i></a> </div> <div class="abstract hidden"> <p>Recent advances in artificial intelligence (AI) enabled digital assistants to evolve towards proactive user support. However, expectations as to when and to what extent assistants should take the initiative are still unclear; discrepancies to the actual system behavior might negatively affect user acceptance. In this paper, we present an in-the-wild study for exploring user expectations of such user-supporting AI systems in terms of different proactivity levels and use cases. We collected 3,168 in-situ responses from 272 participants through a mixed method of automated user tracking and context-triggered surveying. Using a data-driven approach, we gain insights into initial expectations and how they depend on different human factors and contexts. Our insights can help to design AI systems with varying degree of proactivity and preset to meet individual expectations.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/Mueller_Diss_published-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/Mueller_Diss_published-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/Mueller_Diss_published-1400.webp"></source> <img src="/assets/img/publication_preview/Mueller_Diss_published.jpg?a54ae344509c6900fd55061bc31edb35" class="preview z-depth-1 rounded" width="auto" height="auto" alt="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Mueller_Diss_published" class="col-sm-8"> <div class="title"><a href="/publications/Mueller_Diss_published">Around-Body Interaction: Leveraging Limb-movements for Interacting in a Digitally Augmented Physical World</a></div> <div class="author"> <em>Florian Müller</em> </div> <div class="periodical"> Dec 2020 </div> <div class="periodical"> </div> <div class="links"> <a href="/assets/pdf/Mueller_Diss_published.pdf" class="btn btn-sm z-depth-0" role="button"><i class="fa-solid fa-file-pdf"></i> PDF</a> <a href="https://dx.doi.org/10.25534/tuprints-00011388" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="ai ai-doi"> TUprints</i></a> </div> <div class="abstract hidden"> <p>Recent technological advances have made head-mounted displays (HMDs) smaller and untethered, fostering the vision of ubiquitous interaction with information in a digitally augmented physical world. For interacting with such devices, three main types of input - besides not very intuitive finger gestures - have emerged so far: 1) Touch input on the frame of the devices or 2) on accessories (controller) as well as 3) voice input. While these techniques have both advantages and disadvantages depending on the current situation of the user, they largely ignore the skills and dexterity that we show when interacting with the real world: Throughout our lives, we have trained extensively to use our limbs to interact with and manipulate the physical world around us. This thesis explores how the skills and dexterity of our upper and lower limbs, acquired and trained in interacting with the real world, can be transferred to the interaction with HMDs. Thereby, this thesis develops the vision of around-body interaction, in which we use the space around our body, defined by the reach of our limbs, for fast, accurate, and enjoyable interaction with information.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/Muller2020a-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/Muller2020a-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/Muller2020a-1400.webp"></source> <img src="/assets/img/publication_preview/Muller2020a.jpg?1e4602d02c2c5d74cbfbde505789534e" class="preview z-depth-1 rounded" width="auto" height="auto" alt="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Muller2020a" class="col-sm-8"> <div class="title"><a href="/publications/Muller2020a">Around-Body Interaction: Interacting While on the Go</a></div> <div class="author"> <em>Florian Müller</em>, <a href="https://www.sebastian-guenther.com/" rel="external nofollow noopener" target="_blank">Sebastian Günther</a>, and Max Mühlhauser</div> <div class="periodical"> <em>IEEE Pervasive Computing</em>, Apr 2020 </div> <div class="periodical"> </div> <div class="links"> <a href="https://dx.doi.org/10.1109/MPRV.2020.2977850" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="ai ai-doi"> IEEE</i></a> </div> <div class="abstract hidden"> <p>We provide a concise overview on how the skills and dexterity of our upper and lower limbs, acquired and trained in interacting with the physical world, can be transferred to the interaction with HMD. We present the vision of around-body interaction, in which we use the space around our body, defined by the reach of our limbs, for fast, accurate, and enjoyable interactions. In the remainder of this article, we first introduce a design space for such around-body interactions and, second, present two examples of such interaction techniques, which use the degrees of freedom of the lower limbs for on-the go interactions. Finally, we conclude with future research challenges.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/Muller2020c-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/Muller2020c-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/Muller2020c-1400.webp"></source> <img src="/assets/img/publication_preview/Muller2020c.jpg?6b3ab2119a32993b25f67d6a21851de9" class="preview z-depth-1 rounded" width="auto" height="auto" alt="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Muller2020c" class="col-sm-8"> <div class="title"><a href="/publications/Muller2020c">Around-Body Interaction: Über Die Nutzung Der Bewegungen von Gliedmaßen Zur Interaktion in Einer Digital Erweiterten Physischen Welt</a></div> <div class="author"> <em>Florian Müller</em> </div> <div class="periodical"> <em>In Ausgezeichnete Informatikdissertationen 2019</em>, May 2020 </div> <div class="periodical"> </div> <div class="links"> <a href="/assets/pdf/Muller2020c.pdf" class="btn btn-sm z-depth-0" role="button"><i class="fa-solid fa-file-pdf"></i> PDF</a> </div> <div class="abstract hidden"> <p>Durch den technischen Fortschritt sind Head-Mounted Displays (HMDs) kleiner und kabellos geworden. Sie leisten so einen Beitrag zur Vision von allgegenwärtiger Interaktion mit Informationen in einer digital erweiterten physischen Welt. Zur Interaktion mit solchen Geräten werden heute eingabeseitig - neben wenig intuitiven Fingergesten in der Luft – vor allem dreierlei Techniken verwendet: 1) Toucheingabe auf dem Gehäuse der Geräte oder 2) auf Zubehör (Controller) sowie 3) Spracheingabe. Während diese Techniken, abhängig von der aktuellen Situation des Benutzers, sowohl Vor- als auch Nachteile haben, so ignorieren sie weitgehend die Fähigkeiten und Geschicklichkeit, die wir im Umgang mit der realen Welt zeigen: Während unseres ganzen Lebens haben wir ausgiebig trainiert unsere Gliedmaßen zu benutzen, um mit der physischen Welt um uns herum zu interagieren und sie zu manipulieren. Diese Arbeit entwickelt eine Vision für eine körperlichere Interaktion mit solchen Geräten, welche die Fähigkeiten und Geschicklichkeit, die wir im Umgang mit der physischen Welt zeigen, auf die Interaktion mit HMDs überträgt.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/Muller2020d-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/Muller2020d-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/Muller2020d-1400.webp"></source> <img src="/assets/img/publication_preview/Muller2020d.jpg?a6512e1e7ec1be26759b485bf96e9cf7" class="preview z-depth-1 rounded" width="auto" height="auto" alt="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Muller2020d" class="col-sm-8"> <div class="title"> <i class="fa-solid fa-award" style="color: var(--global-theme-color);"></i> <a href="/publications/Muller2020d">Walk The Line: Leveraging Lateral Shifts of the Walking Path as an Input Modality for Head-Mounted Displays</a> </div> <div class="author"> <em>Florian Müller</em>, Martin Schmitz, Daniel Schmitt, <a href="https://www.sebastian-guenther.com/" rel="external nofollow noopener" target="_blank">Sebastian Günther</a>, Markus Funk, and Max Mühlhäuser</div> <div class="periodical"> <em>In Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems</em>, Apr 2020 </div> <div class="periodical"> </div> <div class="links"> <a href="/assets/pdf/Muller2020d.pdf" class="btn btn-sm z-depth-0" role="button"><i class="fa-solid fa-file-pdf"></i> PDF</a> <a href="https://dx.doi.org/10.1145/3313831.3376852" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="ai ai-acmdl"> ACM</i></a> <a href="https://www.youtube.com/watch?v=ylAlzFqWx7g" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-youtube"></i> Video</a> <a href="https://www.youtube.com/watch?v=uQ5w3Wvrb3w" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="fa-solid fa-person-chalkboard"></i> Talk</a> </div> <div class="abstract hidden"> <p>Recent technological advances have made head-mounted displays (HMDs) smaller and untethered, fostering the vision of ubiquitous interaction in a digitally augmented physical world. Consequently, a major part of the interaction with such devices will happen on the go, calling for interaction techniques that allow users to interact while walking. In this paper, we explore lateral shifts of the walking path as a hands-free input modality. The available input options are visualized as lanes on the ground parallel to the user’s walking path. Users can select options by shifting the walking path sideways to the respective lane. We contribute the results of a controlled experiment with 18 participants, confirming the viability of our approach for fast, accurate, and joyful interactions. Further, based on the findings of the controlled experiment, we present three example applications.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/Willich2020-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/Willich2020-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/Willich2020-1400.webp"></source> <img src="/assets/img/publication_preview/Willich2020.jpg?cece7832fa23b57f312564e0a134a9fb" class="preview z-depth-1 rounded" width="auto" height="auto" alt="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Willich2020" class="col-sm-8"> <div class="title"><a href="/publications/Willich2020">Podoportation: Foot-Based Locomotion in Virtual Reality</a></div> <div class="author"> Julius Von Willich, Martin Schmitz, <em>Florian Müller</em>, Daniel Schmitt, and Max Mühlhäuser</div> <div class="periodical"> <em>In Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems - CHI ’20</em>, Apr 2020 </div> <div class="periodical"> </div> <div class="links"> <a href="/assets/pdf/Willich2020.pdf" class="btn btn-sm z-depth-0" role="button"><i class="fa-solid fa-file-pdf"></i> PDF</a> <a href="https://dx.doi.org/10.1145/3313831.3376626" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="ai ai-acmdl"> ACM</i></a> <a href="https://www.youtube.com/watch?v=xvqZTbJdXYE" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-youtube"></i> Video</a> <a href="https://www.youtube.com/watch?v=6c8JujTvVkY" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="fa-solid fa-person-chalkboard"></i> Talk</a> </div> <div class="abstract hidden"> <p>Virtual Reality (VR) allows for infinitely large environments. However, the physical traversable space is always limited by real-world boundaries. This discrepancy between physical and virtual dimensions renders traditional locomotion methods used in real world unfeasible. To alleviate these limitations, research proposed various artificial locomotion concepts such as teleportation, treadmills, and redirected walking. However, these concepts occupy the user’s hands, require complex hardware or large physical spaces. In this paper, we contribute nine VR locomotion concepts for foot-based locomotion, relying on the 3D position of the user’s feet and the pressure applied to the sole as input modalities. We evaluate our concepts and compare them to state-of-the-art point &amp; teleport technique in a controlled experiment with 20 participants. The results confirm the viability of our approaches for foot-based and engaging locomotion. Further, based on the findings, we contribute a wireless hardware prototype implementation.</p> </div> </div> </div> </li> </ol> <h2 class="bibliography">2019</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/Distante2019-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/Distante2019-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/Distante2019-1400.webp"></source> <img src="/assets/img/publication_preview/Distante2019.jpg?3154c5e68e16917e865f1dd8d2737ab3" class="preview z-depth-1 rounded" width="auto" height="auto" alt="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Distante2019" class="col-sm-8"> <div class="title"><a href="/publications/Distante2019">Trends on Engineering Interactive Systems: An Overview of Works Presented in Workshops at EICS 2019</a></div> <div class="author"> Damiano Distante, Alexandra Voit, Marco Winckler, Regina Bernhaupt, Judy Bowen, José Creissac Campos, <em>Florian Müller</em>, Philippe Palanque, Jan Van den Bergh, and Benjamin Weyers</div> <div class="periodical"> <em>In Proceedings of the ACM SIGCHI Symposium on Engineering Interactive Computing Systems - EICS ’19</em>, Apr 2019 </div> <div class="periodical"> </div> <div class="links"> <a href="/assets/pdf/Distante2019.pdf" class="btn btn-sm z-depth-0" role="button"><i class="fa-solid fa-file-pdf"></i> PDF</a> <a href="https://dx.doi.org/10.1145/3319499.3335655" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="ai ai-acmdl"> ACM</i></a> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/funkAssessingAccuracyPoint2019-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/funkAssessingAccuracyPoint2019-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/funkAssessingAccuracyPoint2019-1400.webp"></source> <img src="/assets/img/publication_preview/funkAssessingAccuracyPoint2019.jpg?902e8f9b060db9cc59629440574649b2" class="preview z-depth-1 rounded" width="auto" height="auto" alt="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="funkAssessingAccuracyPoint2019" class="col-sm-8"> <div class="title"><a href="/publications/funkAssessingAccuracyPoint2019">Assessing the Accuracy of Point &amp; Teleport Locomotion with Orientation Indication for Virtual Reality Using Curved Trajectories</a></div> <div class="author"> Markus Funk, <em>Florian Müller</em>, Marco Fendrich, Megan Shene, Moritz Kolvenbach, Niclas Dobbertin, <a href="https://www.sebastian-guenther.com/" rel="external nofollow noopener" target="_blank">Sebastian Günther</a>, and Max Mühlhäuser</div> <div class="periodical"> <em>In Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems</em>, May 2019 </div> <div class="periodical"> </div> <div class="links"> <a href="/assets/pdf/funkAssessingAccuracyPoint2019.pdf" class="btn btn-sm z-depth-0" role="button"><i class="fa-solid fa-file-pdf"></i> PDF</a> <a href="https://dx.doi.org/10.1145/3290605.3300377" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="ai ai-acmdl"> ACM</i></a> <a href="https://www.youtube.com/watch?v=uXctClcQu_g" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-youtube"></i> Video</a> </div> <div class="abstract hidden"> <p>Room-scale Virtual Reality (VR) systems have arrived in users’ homes where tracked environments are set up in limited physical spaces. As most Virtual Environments (VEs) are larger than the tracked physical space, locomotion techniques are used to navigate in VEs. Currently, in recent VR games, point &amp; teleport is the most popular locomotion technique. However, it only allows users to select the position of the teleportation and not the orientation that the user is facing after the teleport. This results in users having to manually correct their orientation after teleporting and possibly getting entangled by the cable of the headset. In this paper, we introduce and evaluate three different point &amp; teleport techniques that enable users to specify the target orientation while teleporting. The results show that, although the three teleportation techniques with orientation indication increase the average teleportation time, they lead to a decreased need for correcting the orientation after teleportation.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/Gunther2019-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/Gunther2019-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/Gunther2019-1400.webp"></source> <img src="/assets/img/publication_preview/Gunther2019.jpg?c9f8b116660de3d551050c7ffe75b1c4" class="preview z-depth-1 rounded" width="auto" height="auto" alt="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Gunther2019" class="col-sm-8"> <div class="title"><a href="/publications/Gunther2019">Slappyfications: Towards Ubiquitous Physical and Embodied Notifications</a></div> <div class="author"> <a href="https://www.sebastian-guenther.com/" rel="external nofollow noopener" target="_blank">Sebastian Günther</a>, <em>Florian Müller</em>, Markus Funk, and Max Mühlhäuser</div> <div class="periodical"> <em>In Extended Abstracts of the 2019 CHI Conference on Human Factors in Computing Systems - CHI ’19</em>, May 2019 </div> <div class="periodical"> </div> <div class="links"> <a href="/assets/pdf/Gunther2019.pdf" class="btn btn-sm z-depth-0" role="button"><i class="fa-solid fa-file-pdf"></i> PDF</a> <a href="https://dx.doi.org/10.1145/3290607.3311780" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="ai ai-acmdl"> ACM</i></a> <a href="https://www.youtube.com/watch?v=qDmrSgyV20s" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-youtube"></i> Video</a> </div> <div class="abstract hidden"> <p>With emerging trends of notifying persons through ubiquitous technologies [2], such as ambient light, vibrotactile, or auditory cues, none of these technologies are truly ubiquitous and have proven to be easily missed or ignored. In this work, we propose Slappyfications, a novel way of sending unmissable embodied and ubiquitous notifications through a palm-based interface [1]. Our prototype enables the users to send three types of Slappyfications: poke, slap, and the STEAM-HAMMER. Through a Wizard-of-Oz study, we show the applicability of our system in real-world scenarios. The results reveal a promising trend, as none of the participants missed a single Slappyfication.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/Gunther2019pneumact-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/Gunther2019pneumact-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/Gunther2019pneumact-1400.webp"></source> <img src="/assets/img/publication_preview/Gunther2019pneumact.jpg?2f255901283c5a0c2bea7f0408be89a6" class="preview z-depth-1 rounded" width="auto" height="auto" alt="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Gunther2019pneumact" class="col-sm-8"> <div class="title"><a href="/publications/Gunther2019pneumact">PneumAct: Pneumatic Kinesthetic Actuation of Body Joints in Virtual Reality Environments</a></div> <div class="author"> <a href="https://www.sebastian-guenther.com/" rel="external nofollow noopener" target="_blank">Sebastian Günther</a>, Mohit Makhija, <em>Florian Müller</em>, Dominik Schön, Max Mühlhäuser, and Markus Funk</div> <div class="periodical"> <em>In Proceedings of the 2019 on Designing Interactive Systems Conference</em>, Jun 2019 </div> <div class="periodical"> </div> <div class="links"> <a href="/assets/pdf/Gunther2019pneumact.pdf" class="btn btn-sm z-depth-0" role="button"><i class="fa-solid fa-file-pdf"></i> PDF</a> <a href="https://dx.doi.org/10.1145/3322276.3322302" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="ai ai-acmdl"> ACM</i></a> <a href="https://www.youtube.com/watch?v=4lRWxzs4Rgs" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-youtube"></i> Video</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/Muller2019-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/Muller2019-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/Muller2019-1400.webp"></source> <img src="/assets/img/publication_preview/Muller2019.jpg?20c959ea24bd7d67fcd430a10bfa54fb" class="preview z-depth-1 rounded" width="auto" height="auto" alt="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Muller2019" class="col-sm-8"> <div class="title"><a href="/publications/Muller2019">Mind the Tap: Assessing Foot-Taps for Interacting with Head-Mounted Displays</a></div> <div class="author"> <em>Florian Müller</em>, Joshua McManus, <a href="https://www.sebastian-guenther.com/" rel="external nofollow noopener" target="_blank">Sebastian Günther</a>, Martin Schmitz, Max Mühlhäuser, and Markus Funk</div> <div class="periodical"> <em>In Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems - CHI ’19</em>, Jun 2019 </div> <div class="periodical"> </div> <div class="links"> <a href="/assets/pdf/Muller2019.pdf" class="btn btn-sm z-depth-0" role="button"><i class="fa-solid fa-file-pdf"></i> PDF</a> <a href="https://dx.doi.org/10.1145/3290605.3300707" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="ai ai-acmdl"> ACM</i></a> <a href="https://www.youtube.com/watch?v=D5hTVIEb7iA" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-youtube"></i> Video</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/Muller2019soproceedings-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/Muller2019soproceedings-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/Muller2019soproceedings-1400.webp"></source> <img src="/assets/img/publication_preview/Muller2019soproceedings.jpg?3154c5e68e16917e865f1dd8d2737ab3" class="preview z-depth-1 rounded" width="auto" height="auto" alt="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Muller2019soproceedings" class="col-sm-8"> <div class="title"><a href="/publications/Muller2019soproceedings">Proceedings of the 7th Workshop on Interacting with Smart Objects</a></div> <div class="author"> </div> <div class="periodical"> Jun 2019 </div> <div class="periodical"> </div> <div class="links"> <a href="/assets/pdf/Muller2019soproceedings.pdf" class="btn btn-sm z-depth-0" role="button"><i class="fa-solid fa-file-pdf"></i> PDF</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/SchmitzMartinStitzFlorianMuller2019-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/SchmitzMartinStitzFlorianMuller2019-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/SchmitzMartinStitzFlorianMuller2019-1400.webp"></source> <img src="/assets/img/publication_preview/SchmitzMartinStitzFlorianMuller2019.jpg?6f9a6503f3245a05a3f7194d9c6ad258" class="preview z-depth-1 rounded" width="auto" height="auto" alt="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="SchmitzMartinStitzFlorianMuller2019" class="col-sm-8"> <div class="title"><a href="/publications/SchmitzMartinStitzFlorianMuller2019">./Trilaterate: A Fabrication Pipeline to Design and 3D Print Hover-, Touch-, and Force-Sensitive Objects</a></div> <div class="author"> Martin Schmitz, Martin Stitz, <em>Florian Müller</em>, Markus Funk, and Max Mühlhäuser</div> <div class="periodical"> <em>In Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems - CHI ’19</em>, Jun 2019 </div> <div class="periodical"> </div> <div class="links"> <a href="/assets/pdf/SchmitzMartinStitzFlorianMuller2019.pdf" class="btn btn-sm z-depth-0" role="button"><i class="fa-solid fa-file-pdf"></i> PDF</a> <a href="https://dx.doi.org/10.1145/3290605.3300684" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="ai ai-acmdl"> ACM</i></a> <a href="https://www.youtube.com/watch?v=4BT6Nw2kWPs" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-youtube"></i> Video</a> </div> <div class="abstract hidden"> <p>Hover, touch, and force are promising input modalities that get increasingly integrated into screens and everyday objects. However, these interactions are often limited to fat surfaces and the integration of suitable sensors is time-consuming and costly. To alleviate these limitations, we contribute Tri-laterate: A fabrication pipeline to 3D print custom objects that detect the 3D position of a fnger hovering, touching, or forcing them by combining multiple capacitance measurements via capacitive trilateration. Trilaterate places and routes actively-shielded sensors inside the object and operates on consumer-level 3D printers. We present technical evaluations and example applications that validate and demonstrate the wide applicability of Trilaterate. CCS CONCEPTS • Human-centered computing \textrightarrow Interaction devices; • Hardware \textrightarrow Tactile and hand-based interfaces;</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/VonWillich2019a-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/VonWillich2019a-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/VonWillich2019a-1400.webp"></source> <img src="/assets/img/publication_preview/VonWillich2019a.jpg?8a30f41c013ba2f5ccfd3c5a5b908302" class="preview z-depth-1 rounded" width="auto" height="auto" alt="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="VonWillich2019a" class="col-sm-8"> <div class="title"><a href="/publications/VonWillich2019a">VRChairRacer: Using an Office Chair Backrest as a Locomotion Technique for VR Racing Games</a></div> <div class="author"> Julius von Willich, Dominik Schön, <a href="https://www.sebastian-guenther.com/" rel="external nofollow noopener" target="_blank">Sebastian Günther</a>, <em>Florian Müller</em>, Max Mühlhäuser, and Markus Funk</div> <div class="periodical"> <em>In Extended Abstracts of the 2019 CHI Conference on Human Factors in Computing Systems - CHI ’19</em>, Jun 2019 </div> <div class="periodical"> </div> <div class="links"> <a href="/assets/pdf/VonWillich2019a.pdf" class="btn btn-sm z-depth-0" role="button"><i class="fa-solid fa-file-pdf"></i> PDF</a> <a href="https://dx.doi.org/10.1145/3290607.3313254" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="ai ai-acmdl"> ACM</i></a> <a href="https://www.youtube.com/watch?v=v906aGntoKY" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-youtube"></i> Video</a> </div> <div class="abstract hidden"> <p>Locomotion in Virtual Reality (VR) is an important topic as there is a mismatch between the size of a Virtual Environment and the physically available tracking space. Although many locomotion techniques have been proposed, research on VR locomotion has not concluded yet. In this demonstration, we contribute to the area of VR locomotion by introducing VRChairRacer. VRChairRacer introduces a novel mapping the velocity of a racing cart on the backrest of an office chair. Further, it maps a users’ rotation onto the steering of a virtual racing cart. VRChairRacer demonstrates this locomotion technique to the community through an immersive multiplayer racing demo.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/vonwillichYouInvadedMy2019-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/vonwillichYouInvadedMy2019-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/vonwillichYouInvadedMy2019-1400.webp"></source> <img src="/assets/img/publication_preview/vonwillichYouInvadedMy2019.jpg?6123cb4d65e5b81fb89a95e81ccd3ba9" class="preview z-depth-1 rounded" width="auto" height="auto" alt="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="vonwillichYouInvadedMy2019" class="col-sm-8"> <div class="title"><a href="/publications/vonwillichYouInvadedMy2019">You Invaded My Tracking Space! Using Augmented Virtuality for Spotting Passersby in Room-Scale Virtual Reality</a></div> <div class="author"> Julius von Willich, Markus Funk, <em>Florian Müller</em>, Karola Marky, Jan Riemann, and Max Mühlhäuser</div> <div class="periodical"> <em>In Proceedings of the 2019 on Designing Interactive Systems Conference - DIS ’19</em>, Jun 2019 </div> <div class="periodical"> </div> <div class="links"> <a href="/assets/pdf/vonwillichYouInvadedMy2019.pdf" class="btn btn-sm z-depth-0" role="button"><i class="fa-solid fa-file-pdf"></i> PDF</a> <a href="https://dx.doi.org/10.1145/3322276.3322334" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="ai ai-acmdl"> ACM</i></a> <a href="https://www.youtube.com/watch?v=SGOFeRX0tmk" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-youtube"></i> Video</a> </div> </div> </div> </li> </ol> <h2 class="bibliography">2018</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/gunther2018checkmate-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/gunther2018checkmate-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/gunther2018checkmate-1400.webp"></source> <img src="/assets/img/publication_preview/gunther2018checkmate.jpg?092ccd17ac8edfc60bd4f5e0377ab59b" class="preview z-depth-1 rounded" width="auto" height="auto" alt="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="gunther2018checkmate" class="col-sm-8"> <div class="title"><a href="/publications/gunther2018checkmate">CheckMate: Exploring a Tangible Augmented Reality Interface for Remote Interaction</a></div> <div class="author"> <a href="https://www.sebastian-guenther.com/" rel="external nofollow noopener" target="_blank">Sebastian Günther</a>, <em>Florian Müller</em>, Martin Schmitz, Jan Riemann, Niloofar Dezfuli, Markus Funk, Dominik Schön, and Max Mühlhäuser</div> <div class="periodical"> <em>In Extended Abstracts of the 2018 CHI Conference on Human Factors in Computing Systems</em>, Apr 2018 </div> <div class="periodical"> </div> <div class="links"> <a href="/assets/pdf/gunther2018checkmate.pdf" class="btn btn-sm z-depth-0" role="button"><i class="fa-solid fa-file-pdf"></i> PDF</a> <a href="https://dx.doi.org/10.1145/3170427.3188647" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="ai ai-acmdl"> ACM</i></a> <a href="https://www.youtube.com/watch?v=ZjG8n9P5sD8" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-youtube"></i> Video</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/gunther2018tactileglove-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/gunther2018tactileglove-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/gunther2018tactileglove-1400.webp"></source> <img src="/assets/img/publication_preview/gunther2018tactileglove.jpg?8708c544f2b74162834d017d4f79ecf8" class="preview z-depth-1 rounded" width="auto" height="auto" alt="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="gunther2018tactileglove" class="col-sm-8"> <div class="title"><a href="/publications/gunther2018tactileglove">TactileGlove: Assistive Spatial Guidance in 3D Space through Vibrotactile Navigation</a></div> <div class="author"> <a href="https://www.sebastian-guenther.com/" rel="external nofollow noopener" target="_blank">Sebastian Günther</a>, <em>Florian Müller</em>, Markus Funk, Jan Kirchner, Niloofar Dezfuli, and Max Mühlhäuser</div> <div class="periodical"> <em>In Proceedings of the 11th PErvasive Technologies Related to Assistive Environments Conference</em>, Jun 2018 </div> <div class="periodical"> </div> <div class="links"> <a href="/assets/pdf/gunther2018tactileglove.pdf" class="btn btn-sm z-depth-0" role="button"><i class="fa-solid fa-file-pdf"></i> PDF</a> <a href="https://dx.doi.org/10.1145/3197768.3197785" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="ai ai-acmdl"> ACM</i></a> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/Meurisch2018-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/Meurisch2018-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/Meurisch2018-1400.webp"></source> <img src="/assets/img/publication_preview/Meurisch2018.jpg?d6a80e8011a2d9e66c8e4caced93e6cc" class="preview z-depth-1 rounded" width="auto" height="auto" alt="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Meurisch2018" class="col-sm-8"> <div class="title"><a href="/publications/Meurisch2018">UPA’18: 3rd International Workshop on Ubiquitous Personal Assistance</a></div> <div class="author"> Christian Meurisch, Max Mühlhäuser, Philipp M. Scholl, Usman Naeem, Veljko Pejović, <em>Florian Müller</em>, Elena Di Lascio, Pei-Yi Patricia Kuo, Sebastian Kauschke, and Muhammad Awais Azam</div> <div class="periodical"> <em>In Proceedings of the 2018 ACM International Joint Conference and 2018 International Symposium on Pervasive and Ubiquitous Computing and Wearable Computers - UbiComp ’18</em>, Jun 2018 </div> <div class="periodical"> </div> <div class="links"> <a href="/assets/pdf/Meurisch2018.pdf" class="btn btn-sm z-depth-0" role="button"><i class="fa-solid fa-file-pdf"></i> PDF</a> <a href="https://dx.doi.org/10.1145/3267305.3274133" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="ai ai-acmdl"> ACM</i></a> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/muller2018camea-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/muller2018camea-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/muller2018camea-1400.webp"></source> <img src="/assets/img/publication_preview/muller2018camea.jpg?5161828203c956ff7ab54c9dd4bb379a" class="preview z-depth-1 rounded" width="auto" height="auto" alt="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="muller2018camea" class="col-sm-8"> <div class="title"><a href="/publications/muller2018camea">CaMea: Camera-Supported Workpiece Measurement for CNC Milling Machines</a></div> <div class="author"> <em>Florian Müller</em>, Maximilian Barnikol, Markus Funk, Martin Schmitz, and Max Mühlhäuser</div> <div class="periodical"> <em>In Proceedings of the 11th PErvasive Technologies Related to Assistive Environments Conference on - PETRA ’18</em>, Jun 2018 </div> <div class="periodical"> </div> <div class="links"> <a href="/assets/pdf/muller2018camea.pdf" class="btn btn-sm z-depth-0" role="button"><i class="fa-solid fa-file-pdf"></i> PDF</a> <a href="https://dx.doi.org/10.1145/3197768.3201569" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="ai ai-acmdl"> ACM</i></a> <a href="https://www.youtube.com/watch?v=jzBM_lchKtg" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-youtube"></i> Video</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/muller2018personalized-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/muller2018personalized-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/muller2018personalized-1400.webp"></source> <img src="/assets/img/publication_preview/muller2018personalized.jpg?a828d6066254180563ebc200b1d589d1" class="preview z-depth-1 rounded" width="auto" height="auto" alt="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="muller2018personalized" class="col-sm-8"> <div class="title"><a href="/publications/muller2018personalized">Personalized User-Carried Single Button Interfaces as Shortcuts for Interacting with Smart Devices</a></div> <div class="author"> <em>Florian Müller</em>, Martin Schmitz, Markus Funk, <a href="https://www.sebastian-guenther.com/" rel="external nofollow noopener" target="_blank">Sebastian Günther</a>, Niloofar Dezfuli, and Max Mühlhäuser</div> <div class="periodical"> <em>In Extended Abstracts of the 2018 CHI Conference on Human Factors in Computing Systems - CHI ’18</em>, Jun 2018 </div> <div class="periodical"> </div> <div class="links"> <a href="/assets/pdf/muller2018personalized.pdf" class="btn btn-sm z-depth-0" role="button"><i class="fa-solid fa-file-pdf"></i> PDF</a> <a href="https://dx.doi.org/10.1145/3170427.3188661" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="ai ai-acmdl"> ACM</i></a> <a href="https://www.youtube.com/watch?v=3wNXOfMofXw" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-youtube"></i> Video</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/muller2018smartobjects-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/muller2018smartobjects-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/muller2018smartobjects-1400.webp"></source> <img src="/assets/img/publication_preview/muller2018smartobjects.jpg?a5739719af643c8b4d56d4456f894dc7" class="preview z-depth-1 rounded" width="auto" height="auto" alt="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="muller2018smartobjects" class="col-sm-8"> <div class="title"><a href="/publications/muller2018smartobjects">SmartObjects: Sixth Workshop on Interacting with Smart Objects</a></div> <div class="author"> <em>Florian Müller</em>, Dirk Schnelle-Walka, Tobias Grosse-Puppendahl, <a href="https://www.sebastian-guenther.com/" rel="external nofollow noopener" target="_blank">Sebastian Günther</a>, Markus Funk, Kris Luyten, Oliver Brdiczka, Niloofar Dezfuli, and Max Mühlhäuser</div> <div class="periodical"> <em>In Extended Abstracts of the 2018 CHI Conference on Human Factors in Computing Systems - CHI ’18</em>, Jun 2018 </div> <div class="periodical"> </div> <div class="links"> <a href="/assets/pdf/muller2018smartobjects.pdf" class="btn btn-sm z-depth-0" role="button"><i class="fa-solid fa-file-pdf"></i> PDF</a> <a href="https://dx.doi.org/10.1145/3170427.3170606" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="ai ai-acmdl"> ACM</i></a> </div> <div class="abstract hidden"> <p>Smart objects are everyday objects that have computing capabilities and give rise to new ways of interaction with our environment. The increasing number of smart objects in our life shapes how we interact beyond the desktop. In this workshop we explore various aspects of the design, development and deployment of smart objects including how one can interact with smart objects.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/murauer2018analysis-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/murauer2018analysis-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/murauer2018analysis-1400.webp"></source> <img src="/assets/img/publication_preview/murauer2018analysis.jpg?e4798ef04712d115e1631fd57b29f4ce" class="preview z-depth-1 rounded" width="auto" height="auto" alt="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="murauer2018analysis" class="col-sm-8"> <div class="title"><a href="/publications/murauer2018analysis">An Analysis of Language Impact on Augmented Reality Order Picking Training</a></div> <div class="author"> Nela Murauer, <em>Florian Müller</em>, <a href="https://www.sebastian-guenther.com/" rel="external nofollow noopener" target="_blank">Sebastian Günther</a>, Dominik Schön, Nerina Pflanz, and Markus Funk</div> <div class="periodical"> <em>In Proceedings of the 11th PErvasive Technologies Related to Assistive Environments Conference</em>, Jun 2018 </div> <div class="periodical"> </div> <div class="links"> <a href="/assets/pdf/murauer2018analysis.pdf" class="btn btn-sm z-depth-0" role="button"><i class="fa-solid fa-file-pdf"></i> PDF</a> <a href="https://dx.doi.org/10.1145/3197768.3201570" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="ai ai-acmdl"> ACM</i></a> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/SmartObjects2018proceedings-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/SmartObjects2018proceedings-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/SmartObjects2018proceedings-1400.webp"></source> <img src="/assets/img/publication_preview/SmartObjects2018proceedings.jpg?a5739719af643c8b4d56d4456f894dc7" class="preview z-depth-1 rounded" width="auto" height="auto" alt="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="SmartObjects2018proceedings" class="col-sm-8"> <div class="title"><a href="/publications/SmartObjects2018proceedings">Proceedings of the 6th Workshop on Interacting with Smart Objects (SmartObjects)</a></div> <div class="author"> </div> <div class="periodical"> Jun 2018 </div> <div class="periodical"> </div> <div class="links"> <a href="/assets/pdf/SmartObjects2018proceedings.pdf" class="btn btn-sm z-depth-0" role="button"><i class="fa-solid fa-file-pdf"></i> PDF</a> <a href="https://ceur-ws.org/Vol-2082/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="fas fa-link"></i> Website</a> </div> </div> </div> </li> </ol> <h2 class="bibliography">2017</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/gunther2017byo-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/gunther2017byo-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/gunther2017byo-1400.webp"></source> <img src="/assets/img/publication_preview/gunther2017byo.jpg?ee0c8662e4f066c4430b036034b8fe86" class="preview z-depth-1 rounded" width="auto" height="auto" alt="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="gunther2017byo" class="col-sm-8"> <div class="title"><a href="/publications/gunther2017byo">BYO*: Utilizing 3D Printed Tangible Tools for Interaction on Interactive Surfaces</a></div> <div class="author"> <a href="https://www.sebastian-guenther.com/" rel="external nofollow noopener" target="_blank">Sebastian Günther</a>, Martin Schmitz, <em>Florian Müller</em>, Jan Riemann, and Max Mühlhäuser</div> <div class="periodical"> <em>In Proceedings of the 2017 ACM Workshop on Interacting with Smart Objects</em>, Mar 2017 </div> <div class="periodical"> </div> <div class="links"> <a href="/assets/pdf/gunther2017byo.pdf" class="btn btn-sm z-depth-0" role="button"><i class="fa-solid fa-file-pdf"></i> PDF</a> <a href="https://dx.doi.org/10.1145/3038450.3038456" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="ai ai-acmdl"> ACM</i></a> </div> <div class="abstract hidden"> <p>Sharing and manipulating information are essential for collaborative work in meeting scenarios. Nowadays, people tend to bring their own devices as a result of increasing mobility possibilities. However, transferring data from one device to another can be cumbersome and tedious if restrictions like different platforms, form factors or environmental limitations apply. In this paper, we present two concepts to enrich interaction on and between devices through 3D printed customized tangibles: 1) Bring your own information, and 2) bring your own tools. For this, we enable interactivity for low-cost and passive tangible 3D printed objects by adding conductive material and make use of touch-enabled surfaces. Our system allows users to easily share digital contents across various devices and to manipulate them with individually designed tools without additional hardware required. Copyright \textcopyright 2017 ACM.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"></div> <div id="Kim2017" class="col-sm-8"> <div class="title"><a href="/publications/Kim2017">Manufacturing Method of Sensor Using 3d Printing and 3d Printer Thereof</a></div> <div class="author"> Woo Sug Jung, Hwa Suk Kim, Jun Ki Jeon, Seong Kyoun Jo, Hyun Woo Lee, Martin Schmitz, <em>Florian Müller</em>, Andreas Leister, Jan Riemann, Niloofar Dezfuli, Max Mühlhäuser, and Mohammadreza Khalilbeigi</div> <div class="periodical"> Mar 2017 </div> <div class="periodical"> </div> <div class="links"> <a href="https://patents.google.com/patent/US20180312398A1/en?oq=20180312398" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="fas fa-link"></i> Website</a> </div> <div class="abstract hidden"> <p>Disclosed is a manufacturing method of a sensor by using 3D printing and 3D printer therefor. According to an embodiment of the present disclosure , a manufacturing method of a sensor by using 3D printing includes : forming a first shape having an inner space by using a non-conduc tive material , and simultaneously or sequentially , forming an electrode at a preset location in the inner space by using a conductive material ; injecting conductive liquid into the inner space ; and forming a second shape on the first shape by using the non-conductive material to seal the inner space of the first shape .</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/muller2017cloudbits-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/muller2017cloudbits-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/muller2017cloudbits-1400.webp"></source> <img src="/assets/img/publication_preview/muller2017cloudbits.jpg?f72509c23402c61a5f3d5aaf68a82a56" class="preview z-depth-1 rounded" width="auto" height="auto" alt="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="muller2017cloudbits" class="col-sm-8"> <div class="title"><a href="/publications/muller2017cloudbits">Cloudbits: Supporting Conversations through Augmented Zero-query Search Visualization</a></div> <div class="author"> <em>Florian Müller</em>, <a href="https://www.sebastian-guenther.com/" rel="external nofollow noopener" target="_blank">Sebastian Günther</a>, Azita Hosseini Nejad, Niloofar Dezfuli, Mohammadreza Khalilbeigi, and Max Mühlhäuser</div> <div class="periodical"> <em>In Proceedings of the 5th Symposium on Spatial User Interaction</em>, Oct 2017 </div> <div class="periodical"> </div> <div class="links"> <a href="/assets/pdf/muller2017cloudbits.pdf" class="btn btn-sm z-depth-0" role="button"><i class="fa-solid fa-file-pdf"></i> PDF</a> <a href="https://dx.doi.org/10.1145/3131277.3132173" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="ai ai-acmdl"> ACM</i></a> <a href="https://www.youtube.com/watch?v=sUe5C8RfON0" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="fa-solid fa-person-chalkboard"></i> Talk</a> </div> <div class="abstract hidden"> <p>\textcopyright 2017 Copyright held by the owner/author(s). The retrieval of additional information from public (e.g., map data) or private (e.g., e-mail) information sources using personal smart devices is a common habit in today’s co-located conversations. This behavior of users imposes challenges in two main areas: 1) cognitive focus switching and 2) information sharing.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/riemann2017evaluation-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/riemann2017evaluation-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/riemann2017evaluation-1400.webp"></source> <img src="/assets/img/publication_preview/riemann2017evaluation.jpg?5fc92610356cb8844165f28ad512d35f" class="preview z-depth-1 rounded" width="auto" height="auto" alt="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="riemann2017evaluation" class="col-sm-8"> <div class="title"><a href="/publications/riemann2017evaluation">An Evaluation of Hybrid Stacking on Interactive Tabletops</a></div> <div class="author"> Jan Riemann, <em>Florian Müller</em>, <a href="https://www.sebastian-guenther.com/" rel="external nofollow noopener" target="_blank">Sebastian Günther</a>, and Max Mühlhäuser</div> <div class="periodical"> <em>In Proceedings of the 2017 ACM Workshop on Interacting with Smart Objects</em>, Mar 2017 </div> <div class="periodical"> </div> <div class="links"> <a href="/assets/pdf/riemann2017evaluation.pdf" class="btn btn-sm z-depth-0" role="button"><i class="fa-solid fa-file-pdf"></i> PDF</a> <a href="https://dx.doi.org/10.1145/3038450.3038451" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="ai ai-acmdl"> ACM</i></a> </div> <div class="abstract hidden"> <p>Stacking is a common practice of organizing documents in the physical world. With the recent advent of interactive tabletops, physical documents can now coexist with digital documents on the same surface. As a result, systems were developed and studied which allow piling of both types of documents with the physical documents being placed on top of the digital ones. In this paper, we study the concept of true hybrid stacking, allowing users to stack both types of documents in an arbitrary order using a hybrid tabletop system called StackTop. We discuss the results and derive implications for future hybrid tabletop systems with stacking support.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/schnelle2017smartobjects-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/schnelle2017smartobjects-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/schnelle2017smartobjects-1400.webp"></source> <img src="/assets/img/publication_preview/schnelle2017smartobjects.jpg?d9cc237c32f2b143af8d5fd7a6e637ee" class="preview z-depth-1 rounded" width="auto" height="auto" alt="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="schnelle2017smartobjects" class="col-sm-8"> <div class="title"><a href="/publications/schnelle2017smartobjects">SmartObjects: Fifth Workshop on Interacting with Smart Objects</a></div> <div class="author"> Dirk Schnelle-Walka, <em>Florian Müller</em>, Tobias Grosse-Puppendahl, Kris Luyten, Max Mühlhäuser, and Oliver Brdiczka</div> <div class="periodical"> <em>In Proceedings of the 22nd International Conference on Intelligent User Interfaces Companion</em>, Mar 2017 </div> <div class="periodical"> </div> <div class="links"> <a href="/assets/pdf/schnelle2017smartobjects.pdf" class="btn btn-sm z-depth-0" role="button"><i class="fa-solid fa-file-pdf"></i> PDF</a> <a href="https://dx.doi.org/10.1145/3030024.3040249" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="ai ai-acmdl"> ACM</i></a> </div> <div class="abstract hidden"> <p>Smart objects are everyday objects that have computing capabilities and give rise to new ways of interaction with our environment. The increasing number of smart objects in our life shapes how we interact beyond the desktop. In this workshop we explore various aspects of the design, development and deployment of smart objects including how one can interact with smart objects.</p> </div> </div> </div> </li> </ol> <h2 class="bibliography">2016</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/muller2016proxiwatch-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/muller2016proxiwatch-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/muller2016proxiwatch-1400.webp"></source> <img src="/assets/img/publication_preview/muller2016proxiwatch.jpg?85476a1210c446fef784dffb7ed4113a" class="preview z-depth-1 rounded" width="auto" height="auto" alt="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="muller2016proxiwatch" class="col-sm-8"> <div class="title"><a href="/publications/muller2016proxiwatch">ProxiWatch: Enhancing Smartwatch Interaction through Proximity-Based Hand Input</a></div> <div class="author"> <em>Florian Müller</em>, <a href="https://www.sebastian-guenther.com/" rel="external nofollow noopener" target="_blank">Sebastian Günther</a>, Niloofar Dezfuli, Mohammadreza Khalilbeigi, and Max Mühlhäuser</div> <div class="periodical"> <em>In Proceedings of the 2016 CHI Conference Extended Abstracts on Human Factors in Computing Systems - CHI EA ’16</em>, Mar 2016 </div> <div class="periodical"> </div> <div class="links"> <a href="/assets/pdf/muller2016proxiwatch.pdf" class="btn btn-sm z-depth-0" role="button"><i class="fa-solid fa-file-pdf"></i> PDF</a> <a href="https://dx.doi.org/10.1145/2851581.2892450" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="ai ai-acmdl"> ACM</i></a> </div> <div class="abstract hidden"> <p>Smartwatches allow ubiquitous and mobile interaction with digital contents. Because of the small screen sizes, tradi-tional interaction techniques are often not applicable. In this work, we show how the degree of freedom offered by the elbow joint, i.e., flexion and extension, can be leveraged as an additional one-handed input modality for smartwatches. By moving the watch towards or away from the body, the user is able to provide input to the smartwatch without a second hand. We present the results of a controlled ex-periment focusing on the human capabilities for proximity-based interaction. Based on the results, we propose guide-lines for designing proximity-based smartwatch interfaces and present ProxiWatch: a one-handed and proximity-based input modality for smartwatches alongside a proto-typical implementation.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/riemann2016freetop-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/riemann2016freetop-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/riemann2016freetop-1400.webp"></source> <img src="/assets/img/publication_preview/riemann2016freetop.jpg?d5df218fd47dc4af5fc740439290a7c4" class="preview z-depth-1 rounded" width="auto" height="auto" alt="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="riemann2016freetop" class="col-sm-8"> <div class="title"><a href="/publications/riemann2016freetop">FreeTop: Finding Free Spots for Projective Augmentation</a></div> <div class="author"> Jan Riemann, Mohammadreza Khalilbeigi, Martin Schmitz, Sebastian Doeweling, <em>Florian Müller</em>, and Max Mühlhäuser</div> <div class="periodical"> <em>In Proceedings of the 2016 CHI Conference Extended Abstracts on Human Factors in Computing Systems</em>, May 2016 </div> <div class="periodical"> </div> <div class="links"> <a href="/assets/pdf/riemann2016freetop.pdf" class="btn btn-sm z-depth-0" role="button"><i class="fa-solid fa-file-pdf"></i> PDF</a> <a href="https://dx.doi.org/10.1145/2851581.2892321" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="ai ai-acmdl"> ACM</i></a> </div> <div class="abstract hidden"> <p>Augmenting the physical world using projection technologies or head-worn displays becomes increasingly popular in research and commercial applications. However, a common problem is interference between the physical surface’s texture and the projection. In this paper, we present FreeTop, a combined approach to finding areas suitable for projection, which considers multiple aspects influencing projection quality, like visual texture and physical surface structure. FreeTop can be used in stationary and mobile settings for locating free areas in arbitrary physical settings suitable for projective augmentation and touch interaction.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/schmitz2016liquido-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/schmitz2016liquido-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/schmitz2016liquido-1400.webp"></source> <img src="/assets/img/publication_preview/schmitz2016liquido.jpg?45f25cbaf29da93433c204d3c953f799" class="preview z-depth-1 rounded" width="auto" height="auto" alt="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="schmitz2016liquido" class="col-sm-8"> <div class="title"><a href="/publications/schmitz2016liquido">Liquido: Embedding Liquids into 3D Printed Objects to Sense Tilting and Motion</a></div> <div class="author"> Martin Schmitz, Andreas Leister, Niloofar Dezfuli, Jan Riemann, <em>Florian Müller</em>, and Max Mühlhäuser</div> <div class="periodical"> <em>In Proceedings of the 2016 CHI Conference Extended Abstracts on Human Factors in Computing Systems</em>, May 2016 </div> <div class="periodical"> </div> <div class="links"> <a href="/assets/pdf/schmitz2016liquido.pdf" class="btn btn-sm z-depth-0" role="button"><i class="fa-solid fa-file-pdf"></i> PDF</a> <a href="https://dx.doi.org/10.1145/2851581.2892275" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="ai ai-acmdl"> ACM</i></a> </div> <div class="abstract hidden"> <p>Tilting and motion are widely used as interaction modalities in smart objects such as wearables and smart phones (e.g., to detect posture or shaking). They are often sensed with accelerometers. In this paper, we propose to embed liquids into 3D printed objects while printing to sense various tilting and motion interactions via capacitive sensing. This method reduces the assembly effort after printing and is a low-cost and easy-to-apply way of extending the input capabilities of 3D printed objects. We contribute two liquid sensing patterns and a practical printing process using a standard dual-extrusion 3D printer and commercially available materials. We validate the method by a series of evaluations and provide a set of interactive example applications. \textcopyright 2016 Authors.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/schnelle2016scwt-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/schnelle2016scwt-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/schnelle2016scwt-1400.webp"></source> <img src="/assets/img/publication_preview/schnelle2016scwt.jpg?d9cc237c32f2b143af8d5fd7a6e637ee" class="preview z-depth-1 rounded" width="auto" height="auto" alt="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="schnelle2016scwt" class="col-sm-8"> <div class="title"><a href="/publications/schnelle2016scwt">SCWT: A Joint Workshop on Smart Connected and Wearable Things</a></div> <div class="author"> Dirk Schnelle-Walka, Lior Limonad, Tobias Grosse-Puppendahl, Joel Lanir, <em>Florian Müller</em>, Massimo Mecella, Kris Luyten, Tsvi Kuflik, Oliver Brdiczka, and Max Mühlhäuser</div> <div class="periodical"> <em>In Companion Publication of the 21st International Conference on Intelligent User Interfaces</em>, Mar 2016 </div> <div class="periodical"> </div> <div class="links"> <a href="/assets/pdf/schnelle2016scwt.pdf" class="btn btn-sm z-depth-0" role="button"><i class="fa-solid fa-file-pdf"></i> PDF</a> <a href="https://dx.doi.org/10.1145/2876456.2882849" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="ai ai-acmdl"> ACM</i></a> </div> <div class="abstract hidden"> <p>The increasing number of smart objects in our everyday life shapes how we interact beyond the desktop. In this workshop we discuss how advanced interactions with smart objects in the context of the Internet-of-Thingsshould be designed from various perspectives, such as HCI and AI as well as industry and academia.</p> </div> </div> </div> </li> </ol> <h2 class="bibliography">2015</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/joStudyOnbodyUser2015-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/joStudyOnbodyUser2015-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/joStudyOnbodyUser2015-1400.webp"></source> <img src="/assets/img/publication_preview/joStudyOnbodyUser2015.jpg?11b9293f7bc1df9fd0b9ed4000a80e61" class="preview z-depth-1 rounded" width="auto" height="auto" alt="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="joStudyOnbodyUser2015" class="col-sm-8"> <div class="title"><a href="/publications/joStudyOnbodyUser2015">A Study of On-body User Interface: PiAM(Palm interAction Module)</a></div> <div class="author"> Seng-Kyoun Jo,  이현우, Kim Jinsul, <em>Florian Müller</em>, Mohammed Khalilbeigi, and Max Mühlhäuser</div> <div class="periodical"> <em>Journal of Knowledge Information Technology and Systems</em>, Dec 2015 </div> <div class="periodical"> </div> <div class="links"> <a href="/assets/pdf/joStudyOnbodyUser2015.pdf" class="btn btn-sm z-depth-0" role="button"><i class="fa-solid fa-file-pdf"></i> PDF</a> </div> <div class="abstract hidden"> <p>User interface providing easy and efficient control environments with user is emerging as a core keyword in ICT market and smart phone industry as the influence and importance of UI has increased recently. To succeed in ICT market, however, UI has to satisfy industrial requirements including simple and easy control, intuitional access and high recognition. Thus study for providing user-friendly interface has been actively researched. In this paper, we investigate user interface based on human’s</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/Muller2015-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/Muller2015-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/Muller2015-1400.webp"></source> <img src="/assets/img/publication_preview/Muller2015.jpg?6b3ab2119a32993b25f67d6a21851de9" class="preview z-depth-1 rounded" width="auto" height="auto" alt="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Muller2015" class="col-sm-8"> <div class="title"><a href="/publications/Muller2015">A Study on Proximity-based Hand Input for One-handed Mobile Interaction</a></div> <div class="author"> <em>Florian Müller</em>, Mohammadreza Khalilbeigi, Niloofar Dezfuli, Alireza Sahami Shirazi, <a href="https://www.sebastian-guenther.com/" rel="external nofollow noopener" target="_blank">Sebastian Günther</a>, and Max Mühlhäuser</div> <div class="periodical"> <em>In Proceedings of the 3rd ACM Symposium on Spatial User Interaction</em>, Aug 2015 </div> <div class="periodical"> </div> <div class="links"> <a href="/assets/pdf/Muller2015.pdf" class="btn btn-sm z-depth-0" role="button"><i class="fa-solid fa-file-pdf"></i> PDF</a> <a href="https://dx.doi.org/10.1145/2788940.2788955" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="ai ai-acmdl"> ACM</i></a> </div> <div class="abstract hidden"> <p>On-body user interfaces utilize the human’s skin for both sensing input and displaying graphical output. In this paper, we present how the degree of freedom offered by the elbow joint, i.e., exion and extension, can be leveraged to extend the input space of projective user interfaces. The user can move his hand towards or away from himself to browse through a multi-layer information space. We conducted a controlled experiment to investigate how accurately and ef-ficiently users can interact in the space. The results revealed that the accuracy and effciency of proximity-based interactions mainly depend on the traveling distance to the target layer while neither the hand side nor the direction of interaction have a signifcant inuence. Based on our findings, we propose guidelines for designing on-body user interfaces.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/Muller2015b-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/Muller2015b-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/Muller2015b-1400.webp"></source> <img src="/assets/img/publication_preview/Muller2015b.jpg?ef4c495742114bc07c43a8bab0a2f1d5" class="preview z-depth-1 rounded" width="auto" height="auto" alt="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Muller2015b" class="col-sm-8"> <div class="title"><a href="/publications/Muller2015b">Palm-Based Interaction with Head-mounted Displays</a></div> <div class="author"> <em>Florian Müller</em>, Niloofar Dezfuli, Max Mühlhäuser, Martin Schmitz, and Mohammadreza Khalilbeigi</div> <div class="periodical"> <em>In Proceedings of the 17th International Conference on Human-Computer Interaction with Mobile Devices and Services Adjunct - MobileHCI ’15</em>, Aug 2015 </div> <div class="periodical"> </div> <div class="links"> <a href="/assets/pdf/Muller2015b.pdf" class="btn btn-sm z-depth-0" role="button"><i class="fa-solid fa-file-pdf"></i> PDF</a> <a href="https://dx.doi.org/10.1145/2786567.2794314" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="ai ai-acmdl"> ACM</i></a> </div> </div> </div> </li> </ol> <h2 class="bibliography">2012</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/Dezfuli2012-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/Dezfuli2012-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/Dezfuli2012-1400.webp"></source> <img src="/assets/img/publication_preview/Dezfuli2012.jpg?dda1c3e232fd24681bea2a479b79f2b6" class="preview z-depth-1 rounded" width="auto" height="auto" alt="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Dezfuli2012" class="col-sm-8"> <div class="title"><a href="/publications/Dezfuli2012">PalmRC: Imaginary Palm-Based Remote Control for Eyes-Free Television Interaction</a></div> <div class="author"> Niloofar Dezfuli, Mohammadreza Khalilbeigi, Jochen Huber, <em>Florian Müller</em>, and Max Mühlhäuser</div> <div class="periodical"> <em>In EuroiTV’12 - Proceedings of the 10th European Conference on Interactive TV and Video</em>, Jul 2012 </div> <div class="periodical"> </div> <div class="links"> <a href="/assets/pdf/Dezfuli2012.pdf" class="btn btn-sm z-depth-0" role="button"><i class="fa-solid fa-file-pdf"></i> PDF</a> <a href="https://dx.doi.org/10.1145/2325616.2325623" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="ai ai-acmdl"> ACM</i></a> </div> <div class="abstract hidden"> <p>User input on television (TV) typically requires a mediator device, such as a handheld remote control. While being a well-established interaction paradigm, a handheld device has serious drawbacks: it can be easily misplaced due to its mobility and in case of a touch screen interface, it also requires additional visual attention. Emerging interaction paradigms like 3D mid-air gestures using novel depth sensors, such as Microsoft’s Kinect, aim at overcoming these limitations, but are known to be e.g. tiring. In this paper, we propose to leverage the palm as an interactive surface for TV remote control. Our contribution is three-fold: (1) we explore the conceptual design space in an exploratory study. (2) Based upon these results, we investigate the effectiveness and accuracy of such an interface in a controlled experiment. And (3), we contribute PalmRC: an eyes-free, palm-surface-based TV remote control, which in turn is evaluated in an early user feedback session. Our results show that the palm has the potential to be leveraged for device-less and eyes-free TV remote interaction without any third-party mediator device. \textcopyright 2012 ACM.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/Dezfuli2012a-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/Dezfuli2012a-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/Dezfuli2012a-1400.webp"></source> <img src="/assets/img/publication_preview/Dezfuli2012a.jpg?357b88a1ecbdd63a0019d6ecc812ae95" class="preview z-depth-1 rounded" width="auto" height="auto" alt="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Dezfuli2012a" class="col-sm-8"> <div class="title"><a href="/publications/Dezfuli2012a">Leveraging the Palm Surface as an Eyes-Free Tv Remote Control</a></div> <div class="author"> Niloofar Dezfuli, Mohammadreza Khalilbeigi, Jochen Huber, <em>Florian Müller</em>, and Max Mühlhäuser</div> <div class="periodical"> <em>In CHI ’12 Extended Abstracts on Human Factors in Computing Systems</em>, May 2012 </div> <div class="periodical"> </div> <div class="links"> <a href="/assets/pdf/Dezfuli2012a.pdf" class="btn btn-sm z-depth-0" role="button"><i class="fa-solid fa-file-pdf"></i> PDF</a> <a href="https://dx.doi.org/10.1145/2212776.2223823" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="ai ai-acmdl"> ACM</i></a> </div> <div class="abstract hidden"> <p>User input on television typically requires a mediator device such as a handheld remote control. While being a well-established interaction paradigm, a handheld device has serious drawbacks: it can be easily misplaced due to its mobility and in case of a touch screen interface, it also requires additional visual attention. Emerging interaction paradigms like 3D mid-air gestures using novel depth sensors such as Microsoft’s Kinect aim at overcoming these limitations, but are known for instance to be tiring. In this paper, we propose to leverage the palm as an interactive surface for TV remote control. Our contribution is two-fold: (1) we have explored the conceptual design space in an exploratory study. (2) Based upon these results, we investigated the accuracy and effectiveness of such an interface in a controlled experiment. Our results show that the palm has the potential to be leveraged for device-less and eyes-free TV interactions without any third-party mediator device.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/Dezfuli2012Couch-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/Dezfuli2012Couch-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/Dezfuli2012Couch-1400.webp"></source> <img src="/assets/img/publication_preview/Dezfuli2012Couch.jpg?2cc73910202bd00303fbd8d524e45f40" class="preview z-depth-1 rounded" width="auto" height="auto" alt="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Dezfuli2012Couch" class="col-sm-8"> <div class="title"><a href="/publications/Dezfuli2012Couch">CouchTV: Leveraging the Spatial Information of Viewers for Social Interactive Television Systems - Poster Presentation</a></div> <div class="author"> Niloofar Dezfuli, Manolis Pavlakis, <em>Florian Müller</em>, Mohammadreza Khalilbeigi, and Max Mühlhäuser</div> <div class="periodical"> <em>In 10th European Interactive TV Conference 2012</em>, Jul 2012 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> </ol> </div> </article> </div> </div> <footer class="sticky-bottom mt-5"> <div class="container"> © Copyright Florian Müller | <a href="https://flomuedev.github.io/imprint">Impressum</a> </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script type="text/javascript">$(function(){$('[data-toggle="tooltip"]').tooltip()});</script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?7b30caa5023af4af8408a472dc4e1ebb"></script> <script defer src="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js?d633890033921b33e0ceb13d22340a9c"></script> <script defer src="/assets/js/common.js?acdb9690d7641b2f8d40529018c71a01"></script> <script defer src="/assets/js/copy_code.js?07b8786bab9b4abe90d10e61f7d12ff7" type="text/javascript"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>