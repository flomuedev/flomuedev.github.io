<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>publications | Florian Müller</title> <meta name="author" content="Florian Müller"> <meta name="description" content="publications by categories in reversed chronological order. generated by jekyll-scholar."> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?19f3075a2d19613090fe9e16b564e1fe" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://flomuedev.github.io/publications/"> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Florian </span>Müller</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about</a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">blog</a> </li> <li class="nav-item active"> <a class="nav-link" href="/publications/">publications<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects</a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">repositories</a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv</a> </li> <li class="nav-item "> <a class="nav-link" href="/teaching/">teaching</a> </li> <li class="nav-item "> <a class="nav-link" href="/people/">people</a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">submenus</a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item" href="/publications/">publications</a> <div class="dropdown-divider"></div> <a class="dropdown-item" href="/projects/">projects</a> </div> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">publications</h1> <p class="post-description">publications by categories in reversed chronological order. generated by jekyll-scholar.</p> </header> <article> <div class="publications"> <h2 class="bibliography">2021</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/Matviienko2021-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/Matviienko2021-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/Matviienko2021-1400.webp"></source> <img src="/assets/img/publication_preview/Matviienko2021.jpg" class="preview z-depth-1 rounded" width="auto" height="auto" alt="Matviienko2021.jpg" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Matviienko2021" class="col-sm-8"> <div class="title">VRtangibles: Assisting Children in Creating Virtual Scenes using Tangible Objects and Touch Input</div> <div class="author"> Andrii Matviienko, Marcel Langer, <em>Florian Müller</em>, Martin Schmitz, and Max Mühlhäuser</div> <div class="periodical"> <em>In Extended Abstracts of the 2021 CHI Conference on Human Factors in Computing Systems</em>, May 2021 </div> <div class="periodical"> </div> <div class="links"> <a href="10.1145/3411763.3451671" class="btn btn-sm z-depth-0" role="button"><i class="ai ai-doi"></i></a> <a href="/assets/pdf/matviienko2021vrtangibles.pdf" class="btn btn-sm z-depth-0" role="button"><i class="ai ai-doi"></i></a> <a href="http://arxiv.org/abs/2303.15800" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="ai ai-arxiv"></i></a> <a class="bibtex btn btn-sm z-depth-0" role="button"><i class="fa-solid fa-quote-left"></i></a> <a href="https://www.youtube.com/watch?v=wW-d9yokhYI" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-youtube"></i></a> <a href="https://www.youtube.com/watch?v=wW-d9yokhYI" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="fa-solid fa-person-chalkboard"></i></a> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">Matviienko2021</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{New York, NY, USA}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Matviienko, Andrii and Langer, Marcel and M{\"{u}}ller, Florian and Schmitz, Martin and M{\"{u}}hlh{\"{a}}user, Max}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Extended Abstracts of the 2021 CHI Conference on Human Factors in Computing Systems}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1145/3411763.3451671}</span><span class="p">,</span>
  <span class="na">isbn</span> <span class="p">=</span> <span class="s">{9781450380959}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">may</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{1--7}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{ACM}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{{VRtangibles: Assisting Children in Creating Virtual Scenes using Tangible Objects and Touch Input}}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://dl.acm.org/doi/10.1145/3411763.3451671}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span><span class="p">,</span>
  <span class="na">talk</span> <span class="p">=</span> <span class="s">{https://www.youtube.com/watch?v=wW-d9yokhYI}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/Guenther2021-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/Guenther2021-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/Guenther2021-1400.webp"></source> <img src="/assets/img/publication_preview/Guenther2021.jpg" class="preview z-depth-1 rounded" width="auto" height="auto" alt="Guenther2021.jpg" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Guenther2021" class="col-sm-8"> <div class="title">ActuBoard: An Open Rapid Prototyping Platform to integrate Hardware Actuators in Remote Applications</div> <div class="author"> <a href="https://www.sebastian-guenther.com/" rel="external nofollow noopener" target="_blank">Sebastian Günther</a>, <em>Florian Müller</em>, Felix Hübner, Max Mühlhäuser, and Andrii Matviienko</div> <div class="periodical"> <em>In Companion of the 2021 ACM SIGCHI Symposium on Engineering Interactive Computing Systems</em>, Jun 2021 </div> <div class="periodical"> </div> <div class="links"> <a href="10.1145/3459926.3464757" class="btn btn-sm z-depth-0" role="button"><i class="ai ai-doi"></i></a> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge" style="background-color:#00369f"><a href="https://chi2024.acm.org/" rel="external nofollow noopener" target="_blank">CHI</a></abbr></div> <div id="Elsayed2021" class="col-sm-8"> <div class="title">CameraReady: Assessing the Influence of Display Types and Visualizations on Posture Guidance</div> <div class="author"> Hesham Elsayed, Philipp Hoffmann, <a href="https://www.sebastian-guenther.com/" rel="external nofollow noopener" target="_blank">Sebastian Günther</a>, Martin Schmitz, Martin Weigel, Max Mühlhäuser, and <em>Florian Müller</em> </div> <div class="periodical"> <em>In Designing Interactive Systems Conference 2021</em>, Jun 2021 </div> <div class="periodical"> </div> <div class="links"> <a href="10.1145/3461778.3462026" class="btn btn-sm z-depth-0" role="button"><i class="ai ai-doi"></i></a> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> </div> <div class="abstract hidden"> <p>Computer-supported posture guidance is used in sports, dance training, expression of art with movements, and learning gestures for interaction. At present, the influence of display types and visualizations have not been investigated in the literature. These factors are important as they directly impact perception and cognitive load, and hence influence the performance of participants. In this paper, we conducted a controlled experiment with 20 participants to compare the use of five display types with different screen sizes: smartphones, tablets, desktop monitors, TVs, and large displays. On each device, we compared three common visualizations for posture guidance: skeletons, silhouettes, and 3d body models. To conduct our assessment, we developed a mobile and cross-platform system that only requires a single camera. Our results show that compared to a smartphone display, larger displays show a lower error (12%). Regarding the choice of visualization, participants rated 3D body models as significantly more usable in comparison to a skeleton visualization.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"></div> <div id="Schmitz2021" class="col-sm-8"> <div class="title">Oh , Snap ! A Fabrication Pipeline to Magnetically Connect Conventional and 3D-Printed Electronics</div> <div class="author"> Martin Schmitz, Jan Riemann, <em>Florian Müller</em>, Steffen Kreis, and Max Mühlhäuser</div> <div class="periodical"> <em>In Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems - CHI ’21</em>, Jun 2021 </div> <div class="periodical"> </div> <div class="links"> <a href="https://www.youtube.com/watch?v=ado4a_chzqo" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-youtube"></i></a> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"></div> <div id="Schmitz2021a" class="col-sm-8"> <div class="title">Itsy-Bits : Fabrication and Recognition of 3D-Printed Tangibles with Small Footprints on Capacitive Touchscreens</div> <div class="author"> Martin Schmitz, <em>Florian Müller</em>, Max Mühlhäuser, Jan Riemann, and Huy Viet Le</div> <div class="periodical"> <em>In Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems - CHI ’21</em>, Jun 2021 </div> <div class="periodical"> </div> <div class="links"> <a href="https://www.youtube.com/watch?v=55vHxnOKl6k" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-youtube"></i></a> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"></div> <div id="Marky2021" class="col-sm-8"> <div class="title">Let ’ s Frets ! Assisting Guitar Students During Practice via Capacitive Sensing</div> <div class="author"> Karola Marky, Andreas Weiß, Andrii Matviienko, Florian Brandherm, Sebastian Wolf, Martin Schmitz, Florian Krell, <em>Florian Müller</em>, Max Mühlhäuser, and Thomas Kosch</div> <div class="periodical"> <em>In Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems - CHI ’21</em>, Jun 2021 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"></div> <div id="Muller2020c" class="col-sm-8"> <div class="title">Around-Body Interaction: Über die Nutzung der Bewegungen von Gliedmaßen zur Interaktion in einer digital erweiterten physischen Welt</div> <div class="author"> <em>Florian Müller</em> </div> <div class="periodical"> <em>In Ausgezeichnete Informatikdissertationen 2019</em>, Jun 2021 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> </ol> <h2 class="bibliography">2020</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 abbr"></div> <div id="Meurisch2020a" class="col-sm-8"> <div class="title">Exploring User Expectations of Proactive AI Systems</div> <div class="author"> Christian Meurisch, Cristina A. Mihale-Wilson, Adrian Hawlitschek, Florian Giger, <em>Florian Müller</em>, Oliver Hinz, and Max Mühlhäuser</div> <div class="periodical"> <em>Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.</em>, Dec 2020 </div> <div class="periodical"> </div> <div class="links"> <a href="10.1145/3432193" class="btn btn-sm z-depth-0" role="button"><i class="ai ai-doi"></i></a> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> </div> <div class="abstract hidden"> <p>Recent advances in artificial intelligence (AI) enabled digital assistants to evolve towards proactive user support. However, expectations as to when and to what extent assistants should take the initiative are still unclear; discrepancies to the actual system behavior might negatively affect user acceptance. In this paper, we present an in-the-wild study for exploring user expectations of such user-supporting AI systems in terms of different proactivity levels and use cases. We collected 3,168 in-situ responses from 272 participants through a mixed method of automated user tracking and context-triggered surveying. Using a data-driven approach, we gain insights into initial expectations and how they depend on different human factors and contexts. Our insights can help to design AI systems with varying degree of proactivity and preset to meet individual expectations.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"></div> <div id="Elsayed2020a" class="col-sm-8"> <div class="title">VRSketchPen: Unconstrained Haptic Assistance for Sketching in Virtual 3D Environments</div> <div class="author"> Hesham Elsayed, Mayra Donaji Barrera Machuca, Christian Schaarschmidt, Karola Marky, <em>Florian Müller</em>, Jan Riemann, Andrii Matviienko, Martin Schmitz, Martin Weigel, and Max Mühlhäuser</div> <div class="periodical"> <em>Proceedings of the ACM Symposium on Virtual Reality Software and Technology, VRST</em>, Dec 2020 </div> <div class="periodical"> </div> <div class="links"> <a href="10.1145/3385956.3418953" class="btn btn-sm z-depth-0" role="button"><i class="ai ai-doi"></i></a> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> </div> <div class="abstract hidden"> <p>Accurate sketching in virtual 3D environments is challenging due to aspects like limited depth perception or the absence of physical support. To address this issue, we propose VRSketchPen - a pen that uses two haptic modalities to support virtual sketching without constraining user actions: (1) pneumatic force feedback to simulate the contact pressure of the pen against virtual surfaces and (2) vibrotactile feedback to mimic textures while moving the pen over virtual surfaces. To evaluate VRSketchPen, we conducted a lab experiment with 20 participants to compare (1) pneumatic, (2) vibrotactile and (3) a combination of both with (4) snapping and no assistance for flat and curved surfaces in a 3D virtual environment. Our findings show that usage of pneumatic, vibrotactile and their combination significantly improves 2D shape accuracy and leads to diminished depth errors for flat and curved surfaces. Qualitative results indicate that users find the addition of unconstraining haptic feedback to significantly improve convenience, confidence and user experience.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"></div> <div id="Elsayed2020" class="col-sm-8"> <div class="title">VibroMap: Understanding the Spacing of Vibrotactile Actuators across the Body</div> <div class="author"> Hesham Elsayed, Martin Weigel, <em>Florian Müller</em>, Martin Schmitz, Karola Marky, <a href="https://www.sebastian-guenther.com/" rel="external nofollow noopener" target="_blank">Sebastian Günther</a>, Jan Riemann, and Max Mühlhäuser</div> <div class="periodical"> <em>Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.</em>, Dec 2020 </div> <div class="periodical"> </div> <div class="links"> <a href="10.1145/3432189" class="btn btn-sm z-depth-0" role="button"><i class="ai ai-doi"></i></a> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> </div> <div class="abstract hidden"> <p>In spite of the great potential of on-body vibrotactile displays for a variety of applications, research lacks an understanding of the spacing between vibrotactile actuators. Through two experiments, we systematically investigate vibrotactile perception on the wrist, forearm, upper arm, back, torso, thigh, and leg, each in transverse and longitudinal body orientation. In the first experiment, we address the maximum distance between vibration motors that still preserves the ability to generate phantom sensations. In the second experiment, we investigate the perceptual accuracy of localizing vibrations in order to establish the minimum distance between vibration motors. Based on the results, we derive VibroMap, a spatial map of the functional range of inter-motor distances across the body. VibroMap supports hardware and interaction designers with design guidelines for constructing body-worn vibrotactile displays.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"></div> <div id="Willich2020" class="col-sm-8"> <div class="title">Podoportation: Foot-Based Locomotion in Virtual Reality</div> <div class="author"> Julius Von Willich, Martin Schmitz, <em>Florian Müller</em>, Daniel Schmitt, and Max Mühlhäuser</div> <div class="periodical"> <em>In Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems - CHI ’20</em>, Dec 2020 </div> <div class="periodical"> </div> <div class="links"> <a href="10.1145/3313831.3376626" class="btn btn-sm z-depth-0" role="button"><i class="ai ai-doi"></i></a> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://www.youtube.com/watch?v=UIoRAh8TGIk" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-youtube"></i></a> </div> <div class="abstract hidden"> <p>Virtual Reality (VR) allows for infinitely large environments. However, the physical traversable space is always limited by real-world boundaries. This discrepancy between physical and virtual dimensions renders traditional locomotion methods used in real world unfeasible. To alleviate these limitations, research proposed various artificial locomotion concepts such as teleportation, treadmills, and redirected walking. However, these concepts occupy the user’s hands, require complex hardware or large physical spaces. In this paper, we contribute nine VR locomotion concepts for foot-based locomotion, relying on the 3D position of the user’s feet and the pressure applied to the sole as input modalities. We evaluate our concepts and compare them to state-of-the-art point &amp; teleport technique in a controlled experiment with 20 participants. The results confirm the viability of our approaches for foot-based and engaging locomotion. Further, based on the findings, we contribute a wireless hardware prototype implementation.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"></div> <div id="Mueller_Diss_published" class="col-sm-8"> <div class="title">Around-Body Interaction: Leveraging Limb-movements for Interacting in a Digitally Augmented Physical World</div> <div class="author"> <em>Florian Müller</em> </div> <div class="periodical"> Dec 2020 </div> <div class="periodical"> </div> <div class="links"> <a href="10.25534/tuprints-00011388" class="btn btn-sm z-depth-0" role="button"><i class="ai ai-doi"></i></a> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"></div> <div id="Muller2020" class="col-sm-8"> <div class="title">Walk The Line: Leveraging Lateral Shifts of the Walking Path as an Input Modality for Head-Mounted Displays</div> <div class="author"> <em>Florian Müller</em>, Daniel Schmitt, <a href="https://www.sebastian-guenther.com/" rel="external nofollow noopener" target="_blank">Sebastian Günther</a>, Martin Schmitz, Markus Funk, and Max Mühlhäuser</div> <div class="periodical"> <em>In Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems - CHI ’20</em>, Dec 2020 </div> <div class="periodical"> </div> <div class="links"> <a href="10.1145/3313831.3376852" class="btn btn-sm z-depth-0" role="button"><i class="ai ai-doi"></i></a> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://www.youtube.com/watch?v=ylAlzFqWx7g" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-youtube"></i></a> </div> <div class="abstract hidden"> <p>Recent technological advances have made head-mounted displays (HMDs) smaller and untethered, fostering the vision of ubiquitous interaction in a digitally augmented physical world. Consequently, a major part of the interaction with such devices will happen on the go, calling for interaction techniques that allow users to interact while walking. In this paper, we explore lateral shifts of the walking path as a hands-free input modality. The available input options are visualized as lanes on the ground parallel to the user’s walking path. Users can select options by shifting the walking path sideways to the respective lane. We contribute the results of a controlled experiment with 18 participants, confirming the viability of our approach for fast, accurate, and joyful interactions. Further, based on the findings of the controlled experiment, we present three example applications.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"></div> <div id="Gunther2020a" class="col-sm-8"> <div class="title">PneumoVolley: Pressure-based Haptic Feedback on the Head through Pneumatic Actuation</div> <div class="author"> <a href="https://www.sebastian-guenther.com/" rel="external nofollow noopener" target="_blank">Sebastian Günther</a>, Dominik Schön, <em>Florian Müller</em>, Max Mühlhäuser, and Martin Schmitz</div> <div class="periodical"> <em>In Extended Abstracts of the 2020 CHI Conference on Human Factors in Computing Systems - CHI ’20</em>, Dec 2020 </div> <div class="periodical"> </div> <div class="links"> <a href="10.1145/3334480.3382916" class="btn btn-sm z-depth-0" role="button"><i class="ai ai-doi"></i></a> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://www.youtube.com/watch?v=ZKnV8HrUx9M" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-youtube"></i></a> </div> <div class="abstract hidden"> <p>Haptic Feedback brings immersion and presence in Virtual Reality (VR) to the next level. While research proposes the usage of various tactile sensations, such as vibration or ultrasound approaches, the potential applicability of pressure feedback on the head is still under-explored. In this paper, we contribute concepts and design considerations for pressure-based feedback on the head through pneumatic actuation. As a proof-of-concept implementing our pressure-based haptics, we further present PneumoVolley: a VR experience similar to the classic Volleyball game but played with the head. In an exploratory user study with 9 participants, we evaluated our concepts and identified a significantly increased involvement compared to a no-haptics baseline along with high realism and enjoyment ratings using pressure-based feedback on the head in VR. LBW033, Page 1 CHI 2020 Late-Breaking Work</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"></div> <div id="Gunther2020" class="col-sm-8"> <div class="title">Therminator : Understanding the Interdependency of Visual and On-Body Thermal Feedback in Virtual Reality</div> <div class="author"> <a href="https://www.sebastian-guenther.com/" rel="external nofollow noopener" target="_blank">Sebastian Günther</a>, Omar Elmoghazy, <em>Florian Müller</em>, Max Mühlhäuser, Dominik Schön, and Martin Schmitz</div> <div class="periodical"> <em>In Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems - CHI ’20</em>, Dec 2020 </div> <div class="periodical"> </div> <div class="links"> <a href="10.1145/3313831.3376195" class="btn btn-sm z-depth-0" role="button"><i class="ai ai-doi"></i></a> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://www.youtube.com/watch?v=q5lkmqAua78" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-youtube"></i></a> </div> <div class="abstract hidden"> <p>Figure 1. Therminator concepts and example VR applications showing (a) a user during our experiment with a snow visual stimulus, (b) a cold game environment with a user throwing snowballs, (c) a warm tropical islands, and (d) a firefighting simulation with a user extinguishing flames. ABSTRACT Recent advances have made Virtual Reality (VR) more realistic than ever before. This improved realism is attributed to to-day’s ability to increasingly appeal to human sensations, such as visual, auditory or tactile. While research also examines temperature sensation as an important aspect, the interdepen-dency of visual and thermal perception in VR is still underex-plored. In this paper, we propose Therminator, a thermal display concept that provides warm and cold on-body feedback in VR through heat conduction of flowing liquids with different temperatures. Further, we systematically evaluate the interdependency of different visual and thermal stimuli on the temperature perception of arm and abdomen with 25 participants. As part of the results, we found varying temperature perception depending on the stimuli, as well as increasing involvement of users during conditions with matching stimuli.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"></div> <div id="Muller2020a" class="col-sm-8"> <div class="title">Around-body Interaction: Interacting While on the Go</div> <div class="author"> <em>Florian Müller</em>, <a href="https://www.sebastian-guenther.com/" rel="external nofollow noopener" target="_blank">Sebastian Günther</a>, and Max Mühlhäuser</div> <div class="periodical"> <em></em> Dec 2020 </div> <div class="periodical"> </div> <div class="links"> <a href="10.1109/MPRV.2020.2977850" class="btn btn-sm z-depth-0" role="button"><i class="ai ai-doi"></i></a> </div> </div> </div> </li> </ol> <h2 class="bibliography">2019</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 abbr"></div> <div id="Muller2019" class="col-sm-8"> <div class="title">Mind the Tap: Assessing Foot-Taps for Interacting with Head-Mounted Displays</div> <div class="author"> <em>Florian Müller</em>, Joshua McManus, <a href="https://www.sebastian-guenther.com/" rel="external nofollow noopener" target="_blank">Sebastian Günther</a>, Martin Schmitz, Max Mühlhäuser, and Markus Funk</div> <div class="periodical"> <em>In Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems - CHI ’19</em>, Dec 2019 </div> <div class="periodical"> </div> <div class="links"> <a href="10.1145/3290605.3300707" class="btn btn-sm z-depth-0" role="button"><i class="ai ai-doi"></i></a> <a href="https://www.youtube.com/watch?v=D5hTVIEb7iA" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-youtube"></i></a> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"></div> <div id="VonWillich2019" class="col-sm-8"> <div class="title">You Invaded my Tracking Space! Using Augmented Virtuality for Spotting Passersby in Room-Scale Virtual Reality</div> <div class="author"> Julius Willich, Markus Funk, <em>Florian Müller</em>, Karola Marky, Jan Riemann, and Max Mühlhäuser</div> <div class="periodical"> <em>Proceedings of the 2019 on Designing Interactive Systems Conference - DIS ’19</em>, Dec 2019 </div> <div class="periodical"> </div> <div class="links"> <a href="10.1145/3322276.3322334" class="btn btn-sm z-depth-0" role="button"><i class="ai ai-doi"></i></a> <a href="https://www.youtube.com/watch?v=SGOFeRX0tmk" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-youtube"></i></a> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"></div> <div id="Distante2019" class="col-sm-8"> <div class="title">Trends on engineering interactive systems: An overview of works presented in workshops at EICS 2019</div> <div class="author"> Damiano Distante, Marco Winckler, Regina Bernhaupt, Judy Bowen, José Creissac Campos, <em>Florian Müller</em>, Philippe Palanque, Jan Van Den Bergh, Benjamin Weyers, and Alexandra Voit</div> <div class="periodical"> <em>In Proceedings of the ACM SIGCHI Symposium on Engineering Interactive Computing Systems, EICS 2019</em>, Dec 2019 </div> <div class="periodical"> </div> <div class="links"> <a href="10.1145/3319499.3335655" class="btn btn-sm z-depth-0" role="button"><i class="ai ai-doi"></i></a> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> </div> <div class="abstract hidden"> <p>Workshops are a great opportunity for identifying innovative topics of research that might require discussion and maturation. This paper summarizes the outcomes of the workshops track of the 11th Engineering Interactive Computing Systems conference (EICS 2019), held in Valencia (Spain) on 18-21 June 2019. The track featured three workshops, one half-day, one full-day and one two-days workshop, each focused on specific topics of the ongoing research in engineering usable and effective interactive computing systems. In particular, the list of discussed topics include novel forms of interaction and emerging themes in HCI related to new application domains, more efficient and enjoyable interaction possibilities associated to smart objects and smart environments, challenges faced in designing, developing and using interactive systems involving multiple stakeholders.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"></div> <div id="Funk2019" class="col-sm-8"> <div class="title">Assessing the Accuracy of Point &amp; Teleport Locomotion with Orientation Indication for Virtual Reality using Curved Trajectories</div> <div class="author"> Markus Funk, <em>Florian Müller</em>, Marco Fendrich, Megan Shene, Moritz Kolvenbach, Niclas Dobbertin, <a href="https://www.sebastian-guenther.com/" rel="external nofollow noopener" target="_blank">Sebastian Günther</a>, and Max Mühlhäuser</div> <div class="periodical"> <em>In Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems - CHI ’19</em>, Dec 2019 </div> <div class="periodical"> </div> <div class="links"> <a href="10.1145/3290605.3300377" class="btn btn-sm z-depth-0" role="button"><i class="ai ai-doi"></i></a> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://www.youtube.com/watch?v=uXctClcQu_g" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-youtube"></i></a> </div> <div class="abstract hidden"> <p>Figure 1: A user is teleporting herself in a Virtual Environment using the Curved Teleport. It allows her to teleport around an obstacle and graphically choose the orientation, which she wants to face after teleportation only by using the curved trajectory visualization with orientation indication, and without having to turn her body in the physical world. ABSTRACT Room-scale Virtual Reality (VR) systems have arrived in users’ homes where tracked environments are set up in limited physical spaces. As most Virtual Environments (VEs) are larger than the tracked physical space, locomotion techniques are used to navigate in VEs. Currently, in recent VR games, point &amp; teleport is the most popular locomotion technique. However, it only allows users to select the position of the teleportation and not the orientation that the user is facing after the teleport. This results in users having to manually correct their orientation after teleporting and possibly getting entangled by the cable of the headset. In this paper, we introduce and evaluate three diferent point &amp; teleport techniques that enable users to specify the target orientation while teleporting. The results show that, although the three teleportation techniques with orientation indication increase the average teleportation time, they lead to a decreased need for correcting the orientation after teleportation.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"></div> <div id="Marky2019" class="col-sm-8"> <div class="title">Teachyverse: Collaborative e-learning in virtual reality lecture halls</div> <div class="author"> K. Marky, F. Müller, M. Funk, A. Geiß, <a href="https://www.sebastian-guenther.com/" rel="external nofollow noopener" target="_blank">S. Günther</a>, M. Schmitz, J. Riemann, and M. Mühlhäuser</div> <div class="periodical"> <em>In ACM International Conference Proceeding Series</em>, Dec 2019 </div> <div class="periodical"> </div> <div class="links"> <a href="10.1145/3340764.3344917" class="btn btn-sm z-depth-0" role="button"><i class="ai ai-doi"></i></a> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> </div> <div class="abstract hidden"> <p>\textcopyright 2019 Copyright held by the owner/author(s). Publication rights licensed to ACM. Over the last decades, E-learning has gained a lot of popularity and enabled students to learn in front of their computers using Internet-based learning systems rather than physically attending lectures. Those E-learning systems are different from traditional learning and do not fully immerse the student in the learning environment. Thus, we propose Teachyverse, an immersive VR lecture hall that combines e-learning, traditional learning, and remote collaboration. Teachyverse immerses the student in a virtual lecture hall. A proof-of-concept study shows that students perceive lectures in Teachyverse as fun and would like to use Teachyverse as a further E-Learning option.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"></div> <div id="VonWillich2019a" class="col-sm-8"> <div class="title">VRChairRacer: Using an Office Chair Backrest as a Locomotion Technique for VR Racing Games</div> <div class="author"> Julius Willich, Dominik Schön, <a href="https://www.sebastian-guenther.com/" rel="external nofollow noopener" target="_blank">Sebastian Günther</a>, <em>Florian Müller</em>, Max Mühlhäuser, and Markus Funk</div> <div class="periodical"> <em>In Extended Abstracts of the 2019 CHI Conference on Human Factors in Computing Systems - CHI ’19</em>, Dec 2019 </div> <div class="periodical"> </div> <div class="links"> <a href="10.1145/3290607.3313254" class="btn btn-sm z-depth-0" role="button"><i class="ai ai-doi"></i></a> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://www.youtube.com/watch?v=v906aGntoKY" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-youtube"></i></a> </div> <div class="abstract hidden"> <p>Locomotion in Virtual Reality (VR) is an important topic as there is a mismatch between the size of a Virtual Environment and the physically available tracking space. Although many locomotion techniques have been proposed, research on VR locomotion has not concluded yet. In this demonstration, we contribute to the area of VR locomotion by introducing VRChairRacer. VRChairRacer introduces a novel mapping the velocity of a racing cart on the backrest of an office chair. Further, it maps a users’ rotation onto the steering of a virtual racing cart. VRChairRacer demonstrates this locomotion technique to the community through an immersive multiplayer racing demo.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"></div> <div id="Gunther2019" class="col-sm-8"> <div class="title">Slappyfications: Towards Ubiquitous Physical and Embodied Notifications</div> <div class="author"> <a href="https://www.sebastian-guenther.com/" rel="external nofollow noopener" target="_blank">Sebastian Günther</a>, <em>Florian Müller</em>, Markus Funk, and Max Mühlhäuser</div> <div class="periodical"> <em>In Extended Abstracts of the 2019 CHI Conference on Human Factors in Computing Systems - CHI ’19</em>, Dec 2019 </div> <div class="periodical"> </div> <div class="links"> <a href="10.1145/3290607.3311780" class="btn btn-sm z-depth-0" role="button"><i class="ai ai-doi"></i></a> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://www.youtube.com/watch?v=qDmrSgyV20s" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-youtube"></i></a> </div> <div class="abstract hidden"> <p>With emerging trends of notifying persons through ubiquitous technologies [2], such as ambient light, vibrotactile, or auditory cues, none of these technologies are truly ubiquitous and have proven to be easily missed or ignored. In this work, we propose Slappyfications, a novel way of sending unmissable embodied and ubiquitous notifications through a palm-based interface [1]. Our prototype enables the users to send three types of Slappyfications: poke, slap, and the STEAM-HAMMER. Through a Wizard-of-Oz study, we show the applicability of our system in real-world scenarios. The results reveal a promising trend, as none of the participants missed a single Slappyfication.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"></div> <div id="SchmitzMartinStitzFlorianMuller2019" class="col-sm-8"> <div class="title">./trilaterate: A Fabrication Pipeline to Design and 3D Print Hover-, Touch-, and Force-Sensitive Objects</div> <div class="author"> Martin Schmitz, Martin Stitz, <em>Florian Müller</em>, Markus Funk, and Max Mühlhäuser</div> <div class="periodical"> <em>In Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems - CHI ’19</em>, Dec 2019 </div> <div class="periodical"> </div> <div class="links"> <a href="10.1145/3290605.3300684" class="btn btn-sm z-depth-0" role="button"><i class="ai ai-doi"></i></a> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://www.youtube.com/watch?v=4BT6Nw2kWPs" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-youtube"></i></a> </div> <div class="abstract hidden"> <p>Hover, touch, and force are promising input modalities that get increasingly integrated into screens and everyday objects. However, these interactions are often limited to fat surfaces and the integration of suitable sensors is time-consuming and costly. To alleviate these limitations, we contribute Tri-laterate: A fabrication pipeline to 3D print custom objects that detect the 3D position of a fnger hovering, touching, or forcing them by combining multiple capacitance measurements via capacitive trilateration. Trilaterate places and routes actively-shielded sensors inside the object and operates on consumer-level 3D printers. We present technical evaluations and example applications that validate and demonstrate the wide applicability of Trilaterate. CCS CONCEPTS • Human-centered computing → Interaction devices; • Hardware → Tactile and hand-based interfaces;</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"></div> <div id="Gunther2019a" class="col-sm-8"> <div class="title">PneumAct: Pneumatic Kinesthetic Actuation of Body Joints in Virtual Reality Environments</div> <div class="author"> <a href="https://www.sebastian-guenther.com/" rel="external nofollow noopener" target="_blank">Sebastian Günther</a>, Mohit Makhija, <em>Florian Müller</em>, Dominik Schön, Max Mühlhäuser, and Markus Funk</div> <div class="periodical"> <em>Proceedings of the 2019 on Designing Interactive Systems Conference - DIS ’19</em>, Dec 2019 </div> <div class="periodical"> </div> <div class="links"> <a href="10.1145/3322276.3322302" class="btn btn-sm z-depth-0" role="button"><i class="ai ai-doi"></i></a> <a href="https://www.youtube.com/watch?v=4lRWxzs4Rgs" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-youtube"></i></a> </div> </div> </div> </li> </ol> <h2 class="bibliography">2018</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 abbr"></div> <div id="mueller2018camea" class="col-sm-8"> <div class="title">CaMea: Camera-Supported Workpiece Measurement for CNC Milling Machines</div> <div class="author"> <em>Florian Müller</em>, Maximilian Barnikol, Markus Funk, Martin Schmitz, and Max Mühlhäuser</div> <div class="periodical"> <em>In Proceedings of the 11th PErvasive Technologies Related to Assistive Environments Conference on - PETRA ’18</em>, Dec 2018 </div> <div class="periodical"> </div> <div class="links"> <a href="10.1145/3197768.3201569" class="btn btn-sm z-depth-0" role="button"><i class="ai ai-doi"></i></a> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"></div> <div id="murauer2018analysis" class="col-sm-8"> <div class="title">An Analysis of Language Impact on Augmented Reality Order Picking Training</div> <div class="author"> Nela Murauer, <em>Florian Müller</em>, <a href="https://www.sebastian-guenther.com/" rel="external nofollow noopener" target="_blank">Sebastian Günther</a>, Dominik Schön, Nerina Pflanz, and Markus Funk</div> <div class="periodical"> <em>In Proceedings of the 11th PErvasive Technologies Related to Assistive Environments Conference on - PETRA ’18</em>, Dec 2018 </div> <div class="periodical"> </div> <div class="links"> <a href="10.1145/3197768.3201570" class="btn btn-sm z-depth-0" role="button"><i class="ai ai-doi"></i></a> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"></div> <div id="SmartObjects2018" class="col-sm-8"> <div class="title">Proceedings of the 6th Workshop on Interacting with Smart Objects (SmartObjects)</div> <div class="author"> <em>Florian Müller</em>, Dirk Schnelle-Walka, <a href="https://www.sebastian-guenther.com/" rel="external nofollow noopener" target="_blank">Sebastian Günther</a>, and Markus (eds.) Funk</div> <div class="periodical"> Dec 2018 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"></div> <div id="muller2018smartobjects" class="col-sm-8"> <div class="title">SmartObjects: Sixth Workshop on Interacting with Smart Objects</div> <div class="author"> <em>Florian Müller</em>, Dirk Schnelle-Walka, Tobias Grosse-Puppendahl, <a href="https://www.sebastian-guenther.com/" rel="external nofollow noopener" target="_blank">Sebastian Günther</a>, Markus Funk, Kris Luyten, Oliver Brdiczka, Niloofar Dezfuli, and Max Mühlhäuser</div> <div class="periodical"> <em>In Extended Abstracts of the 2018 CHI Conference on Human Factors in Computing Systems - CHI ’18</em>, Dec 2018 </div> <div class="periodical"> </div> <div class="links"> <a href="10.1145/3170427.3170606" class="btn btn-sm z-depth-0" role="button"><i class="ai ai-doi"></i></a> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> </div> <div class="abstract hidden"> <p>Smart objects are everyday objects that have computing capabilities and give rise to new ways of interaction with our environment. The increasing number of smart objects in our life shapes how we interact beyond the desktop. In this workshop we explore various aspects of the design, development and deployment of smart objects including how one can interact with smart objects.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"></div> <div id="Meurisch2018" class="col-sm-8"> <div class="title">UPA’18: 3rd International Workshop on Ubiquitous Personal Assistance</div> <div class="author"> Christian Meurisch, Max Mühlhäuser, Philipp M. Scholl, Usman Naeem, Veljko Pejović, <em>Florian Müller</em>, Elena Di Lascio, Pei-Yi Patricia Kuo, Sebastian Kauschke, and Muhammad Awais Azam</div> <div class="periodical"> <em>In Proceedings of the 2018 ACM International Joint Conference and 2018 International Symposium on Pervasive and Ubiquitous Computing and Wearable Computers - UbiComp ’18</em>, Dec 2018 </div> <div class="periodical"> </div> <div class="links"> <a href="10.1145/3267305.3274133" class="btn btn-sm z-depth-0" role="button"><i class="ai ai-doi"></i></a> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"></div> <div id="gunther2018tactileglove" class="col-sm-8"> <div class="title">TactileGlove: Assistive Spatial Guidance in 3D Space through Vibrotactile Navigation</div> <div class="author"> <a href="https://www.sebastian-guenther.com/" rel="external nofollow noopener" target="_blank">Sebastian Günther</a>, <em>Florian Müller</em>, Markus Funk, Jan Kirchner, Niloofar Dezfuli, and Max Mühlhäuser</div> <div class="periodical"> <em>In Proceedings of the 11th PErvasive Technologies Related to Assistive Environments Conference on - PETRA ’18</em>, Dec 2018 </div> <div class="periodical"> </div> <div class="links"> <a href="10.1145/3197768.3197785" class="btn btn-sm z-depth-0" role="button"><i class="ai ai-doi"></i></a> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"></div> <div id="muller2018personalized" class="col-sm-8"> <div class="title">Personalized User-Carried Single Button Interfaces as Shortcuts for Interacting with Smart Devices</div> <div class="author"> <em>Florian Müller</em>, Martin Schmitz, Markus Funk, <a href="https://www.sebastian-guenther.com/" rel="external nofollow noopener" target="_blank">Sebastian Günther</a>, Niloofar Dezfuli, and Max Mühlhäuser</div> <div class="periodical"> <em>In Extended Abstracts of the 2018 CHI Conference on Human Factors in Computing Systems - CHI ’18</em>, Dec 2018 </div> <div class="periodical"> </div> <div class="links"> <a href="10.1145/3170427.3188661" class="btn btn-sm z-depth-0" role="button"><i class="ai ai-doi"></i></a> <a href="https://www.youtube.com/watch?v=3wNXOfMofXw" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-youtube"></i></a> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"></div> <div id="gunther2018checkmate" class="col-sm-8"> <div class="title">CheckMate: Exploring a Tangible Augmented Reality Interface for Remote Interaction</div> <div class="author"> <a href="https://www.sebastian-guenther.com/" rel="external nofollow noopener" target="_blank">Sebastian Günther</a>, <em>Florian Müller</em>, Martin Schmitz, Jan Riemann, Niloofar Dezfuli, Markus Funk, Dominik Schön, and Max Mühlhäuser</div> <div class="periodical"> <em>In Extended Abstracts of the 2018 CHI Conference on Human Factors in Computing Systems - CHI ’18</em>, Dec 2018 </div> <div class="periodical"> </div> <div class="links"> <a href="10.1145/3170427.3188647" class="btn btn-sm z-depth-0" role="button"><i class="ai ai-doi"></i></a> <a href="https://www.youtube.com/watch?v=ZjG8n9P5sD8" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-youtube"></i></a> </div> </div> </div> </li> </ol> <h2 class="bibliography">2017</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 abbr"></div> <div id="schnelle2017smartobjects" class="col-sm-8"> <div class="title">SmartObjects: Fifth Workshop on Interacting with Smart Objects</div> <div class="author"> Dirk Schnelle-Walka, <em>Florian Müller</em>, Tobias Grosse-Puppendahl, Kris Luyten, Max Mühlhäuser, and Oliver Brdiczka</div> <div class="periodical"> <em>In Proceedings of the 22nd International Conference on Intelligent User Interfaces Companion - IUI ’17 Companion</em>, Dec 2017 </div> <div class="periodical"> </div> <div class="links"> <a href="10.1145/3030024.3040249" class="btn btn-sm z-depth-0" role="button"><i class="ai ai-doi"></i></a> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> </div> <div class="abstract hidden"> <p>Smart objects are everyday objects that have computing capabilities and give rise to new ways of interaction with our environment. The increasing number of smart objects in our life shapes how we interact beyond the desktop. In this workshop we explore various aspects of the design, development and deployment of smart objects including how one can interact with smart objects.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"></div> <div id="gunther2017byo" class="col-sm-8"> <div class="title">BYO*: Utilizing 3D Printed Tangible Tools for Interaction on Interactive Surfaces</div> <div class="author"> <a href="https://www.sebastian-guenther.com/" rel="external nofollow noopener" target="_blank">Sebastian Günther</a>, Martin Schmitz, <em>Florian Müller</em>, Jan Riemann, and Max Mühlhäuser</div> <div class="periodical"> <em>In Proceedings of the 2017 ACM Workshop on Interacting with Smart Objects - SmartObject ’17</em>, Dec 2017 </div> <div class="periodical"> </div> <div class="links"> <a href="10.1145/3038450.3038456" class="btn btn-sm z-depth-0" role="button"><i class="ai ai-doi"></i></a> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> </div> <div class="abstract hidden"> <p>Sharing and manipulating information are essential for collaborative work in meeting scenarios. Nowadays, people tend to bring their own devices as a result of increasing mobility possibilities. However, transferring data from one device to another can be cumbersome and tedious if restrictions like different platforms, form factors or environmental limitations apply. In this paper, we present two concepts to enrich interaction on and between devices through 3D printed customized tangibles: 1) Bring your own information, and 2) bring your own tools. For this, we enable interactivity for low-cost and passive tangible 3D printed objects by adding conductive material and make use of touch-enabled surfaces. Our system allows users to easily share digital contents across various devices and to manipulate them with individually designed tools without additional hardware required. Copyright \textcopyright 2017 ACM.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"></div> <div id="riemann2017evaluation" class="col-sm-8"> <div class="title">An Evaluation of Hybrid Stacking on Interactive Tabletops</div> <div class="author"> Jan Riemann, <em>Florian Müller</em>, <a href="https://www.sebastian-guenther.com/" rel="external nofollow noopener" target="_blank">Sebastian Günther</a>, and Max Mühlhäuser</div> <div class="periodical"> <em>In Proceedings of the 2017 ACM Workshop on Interacting with Smart Objects - SmartObject ’17</em>, Dec 2017 </div> <div class="periodical"> </div> <div class="links"> <a href="10.1145/3038450.3038451" class="btn btn-sm z-depth-0" role="button"><i class="ai ai-doi"></i></a> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> </div> <div class="abstract hidden"> <p>\textcopyright 2017 Copyright held by the owner/author(s). Stacking is a common practice of organizing documents in the physical world. With the recent advent of interactive tabletops, physical documents can now coexist with digital documents on the same surface. As a result, systems were developed and studied which allow piling of both types of documents with the physical documents being placed on top of the digital ones. In this paper, we study the concept of true hybrid stacking, allowing users to stack both types of documents in an arbitrary order using a hybrid tabletop system called StackTop. We discuss the results and derive implications for future hybrid tabletop systems with stacking support.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"></div> <div id="muller2017cloudbits" class="col-sm-8"> <div class="title">Cloudbits: supporting conversations through augmented zero-query search visualization</div> <div class="author"> <em>Florian Müller</em>, <a href="https://www.sebastian-guenther.com/" rel="external nofollow noopener" target="_blank">Sebastian Günther</a>, Azita Hosseini Nejad, Niloofar Dezfuli, Mohammadreza Khalilbeigi, and Max Mühlhäuser</div> <div class="periodical"> <em>In Proceedings of the 5th Symposium on Spatial User Interaction - SUI ’17</em>, Dec 2017 </div> <div class="periodical"> </div> <div class="links"> <a href="10.1145/3131277.3132173" class="btn btn-sm z-depth-0" role="button"><i class="ai ai-doi"></i></a> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> </div> <div class="abstract hidden"> <p>\textcopyright 2017 Copyright held by the owner/author(s). The retrieval of additional information from public (e.g., map data) or private (e.g., e-mail) information sources using personal smart devices is a common habit in today’s co-located conversations. This behavior of users imposes challenges in two main areas: 1) cognitive focus switching and 2) information sharing.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"></div> <div id="Schnelle-Walka2017" class="col-sm-8"> <div class="title">Proceedings of the 2017 ACM Workshop on Interacting with Smart Objects</div> <div class="author"> </div> <div class="periodical"> Dec 2017 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> </ol> <h2 class="bibliography">2016</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 abbr"></div> <div id="schmitz2016liquido" class="col-sm-8"> <div class="title">Liquido: Embedding Liquids into 3D Printed Objects to Sense Tilting and Motion</div> <div class="author"> Martin Schmitz, Andreas Leister, Niloofar Dezfuli, Jan Riemann, <em>Florian Müller</em>, and Max Mühlhäuser</div> <div class="periodical"> <em>In Proceedings of the 2016 CHI Conference Extended Abstracts on Human Factors in Computing Systems - CHI EA ’16</em>, Dec 2016 </div> <div class="periodical"> </div> <div class="links"> <a href="10.1145/2851581.2892275" class="btn btn-sm z-depth-0" role="button"><i class="ai ai-doi"></i></a> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> </div> <div class="abstract hidden"> <p>Tilting and motion are widely used as interaction modalities in smart objects such as wearables and smart phones (e.g., to detect posture or shaking). They are often sensed with accelerometers. In this paper, we propose to embed liquids into 3D printed objects while printing to sense various tilting and motion interactions via capacitive sensing. This method reduces the assembly effort after printing and is a low-cost and easy-to-apply way of extending the input capabilities of 3D printed objects. We contribute two liquid sensing patterns and a practical printing process using a standard dual-extrusion 3D printer and commercially available materials. We validate the method by a series of evaluations and provide a set of interactive example applications. \textcopyright 2016 Authors.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"></div> <div id="schnelle2016scwt" class="col-sm-8"> <div class="title">SCWT: A Joint Workshop on Smart Connected and Wearable Things</div> <div class="author"> Dirk Schnelle-Walka, Max Mühlhäuser, Lior Limonad, Tobias Grosse-Puppendahl, Joel Lanir, <em>Florian Müller</em>, Massimo Mecella, Kris Luyten, Tsvi Kuflik, and Oliver Brdiczka</div> <div class="periodical"> <em>In Companion Publication of the 21st International Conference on Intelligent User Interfaces - IUI ’16 Companion</em>, Dec 2016 </div> <div class="periodical"> </div> <div class="links"> <a href="10.1145/2876456.2882849" class="btn btn-sm z-depth-0" role="button"><i class="ai ai-doi"></i></a> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> </div> <div class="abstract hidden"> <p>The increasing number of smart objects in our everyday life shapes how we interact beyond the desktop. In this workshop we discuss how advanced interactions with smart objects in the context of the Internet-of-Thingsshould be designed from various perspectives, such as HCI and AI as well as industry and academia.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"></div> <div id="muller2016-proxiwatch" class="col-sm-8"> <div class="title">ProxiWatch: Enhancing smartwatch interaction through proximity-based hand input</div> <div class="author"> <em>Florian Müller</em>, <a href="https://www.sebastian-guenther.com/" rel="external nofollow noopener" target="_blank">Sebastian Günther</a>, Niloofar Dezfuli, Mohammadreza Khalilbeigi, and Max Mühlhäuser</div> <div class="periodical"> <em>In Proceedings of the 2016 CHI Conference Extended Abstracts on Human Factors in Computing Systems - CHI EA ’16</em>, Dec 2016 </div> <div class="periodical"> </div> <div class="links"> <a href="10.1145/2851581.2892450" class="btn btn-sm z-depth-0" role="button"><i class="ai ai-doi"></i></a> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> </div> <div class="abstract hidden"> <p>Smartwatches allow ubiquitous and mobile interaction with digital contents. Because of the small screen sizes, tradi-tional interaction techniques are often not applicable. In this work, we show how the degree of freedom offered by the elbow joint, i.e., flexion and extension, can be leveraged as an additional one-handed input modality for smartwatches. By moving the watch towards or away from the body, the user is able to provide input to the smartwatch without a second hand. We present the results of a controlled ex-periment focusing on the human capabilities for proximity-based interaction. Based on the results, we propose guide-lines for designing proximity-based smartwatch interfaces and present ProxiWatch: a one-handed and proximity-based input modality for smartwatches alongside a proto-typical implementation.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"></div> <div id="riemann2016-freetop" class="col-sm-8"> <div class="title">FreeTop: Finding Free Spots for Projective Augmentation</div> <div class="author"> Jan Riemann, Mohammadreza Khalilbeigi, Martin Schmitz, Sebastian Doeweling, <em>Florian Müller</em>, and Max Mühlhäuser</div> <div class="periodical"> <em>In Proceedings of the 2016 CHI Conference Extended Abstracts on Human Factors in Computing Systems - CHI EA ’16</em>, Dec 2016 </div> <div class="periodical"> </div> <div class="links"> <a href="10.1145/2851581.2892321" class="btn btn-sm z-depth-0" role="button"><i class="ai ai-doi"></i></a> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> </div> <div class="abstract hidden"> <p>Augmenting the physical world using projection technologies or head-worn displays becomes increasingly popular in research and commercial applications. However, a common problem is interference between the physical surface’s texture and the projection. In this paper, we present FreeTop, a combined approach to finding areas suitable for projection, which considers multiple aspects influencing projection quality, like visual texture and physical surface structure. FreeTop can be used in stationary and mobile settings for locating free areas in arbitrary physical settings suitable for projective augmentation and touch interaction.</p> </div> </div> </div> </li> </ol> <h2 class="bibliography">2015</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 abbr"></div> <div id="Muller2015b" class="col-sm-8"> <div class="title">Palm-based Interaction with Head-mounted Displays</div> <div class="author"> <em>Florian Müller</em>, Niloofar Dezfuli, Max Mühlhäuser, Martin Schmitz, and Mohammadreza Khalilbeigi</div> <div class="periodical"> <em>In Proceedings of the 17th International Conference on Human-Computer Interaction with Mobile Devices and Services Adjunct - MobileHCI ’15</em>, Dec 2015 </div> <div class="periodical"> </div> <div class="links"> <a href="10.1145/2786567.2794314" class="btn btn-sm z-depth-0" role="button"><i class="ai ai-doi"></i></a> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"></div> <div id="muller2015a" class="col-sm-8"> <div class="title">A Study on Proximity-based Hand Input for One-handed Mobile Interaction</div> <div class="author"> <em>Florian Müller</em>, Mohammadreza Khalilbeigi, Niloofar Dezfuli, Alireza Sahami Shirazi, <a href="https://www.sebastian-guenther.com/" rel="external nofollow noopener" target="_blank">Sebastian Günther</a>, and Max Mühlhäuser</div> <div class="periodical"> <em>In Proceedings of the 3rd ACM Symposium on Spatial User Interaction - SUI ’15</em>, Dec 2015 </div> <div class="periodical"> </div> <div class="links"> <a href="10.1145/2788940.2788955" class="btn btn-sm z-depth-0" role="button"><i class="ai ai-doi"></i></a> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> </div> <div class="abstract hidden"> <p>On-body user interfaces utilize the human’s skin for both sensing input and displaying graphical output. In this paper, we present how the degree of freedom offered by the elbow joint, i.e., flexion and extension, can be leveraged to extend the input space of projective user interfaces. The user can move his hand towards or away from himself to browse through a multi-layer information space. We conducted a controlled experiment to investigate how accurately and efficiently users can interact in the space. The results revealed that the accuracy and efficiency of proximity-based interactions mainly depend on the traveling distance to the target layer while neither the hand side nor the direction of interaction have a significant influence. Based on our findings, we propose guidelines for designing on-body user interfaces.</p> </div> </div> </div> </li> </ol> <h2 class="bibliography">2012</h2> <ol class="bibliography"><li> <div class="row"> <div class="col-sm-2 abbr"></div> <div id="Dezfuli2012" class="col-sm-8"> <div class="title">PalmRC: imaginary palm-based remote control for eyes-free television interaction</div> <div class="author"> Niloofar Dezfuli, Mohammadreza Khalilbeigi, Jochen Huber, <em>Florian Müller</em>, and Max Mühlhäuser</div> <div class="periodical"> <em>In Proceedings of the 10th European conference on Interactive tv and video - EuroiTV ’12</em>, Jul 2012 </div> <div class="periodical"> </div> <div class="links"> <a href="10.1145/2325616.2325623" class="btn btn-sm z-depth-0" role="button"><i class="ai ai-doi"></i></a> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> </div> <div class="abstract hidden"> <p>User input on television (TV) typically requires a mediator device, such as a handheld remote control. While being a well-established interaction paradigm, a handheld device has serious drawbacks: it can be easily misplaced due to its mobility and in case of a touch screen interface, it also requires additional visual attention. Emerging interaction paradigms like 3D mid-air gestures using novel depth sensors, such as Microsoft’s Kinect, aim at overcoming these limitations, but are known to be e.g. tiring. In this paper, we propose to leverage the palm as an interactive surface for TV remote control. Our contribution is three-fold: (1) we explore the conceptual design space in an exploratory study. (2) Based upon these results, we investigate the effectiveness and accuracy of such an interface in a controlled experiment. And (3), we contribute PalmRC: an eyes-free, palm-surface-based TV remote control, which in turn is evaluated in an early user feedback session. Our results show that the palm has the potential to be leveraged for device-less and eyes-free TV remote interaction without any third-party mediator device.</p> </div> </div> </div> </li></ol> </div> </article> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2023 Florian Müller. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. Last updated: December 19, 2023. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?7b30caa5023af4af8408a472dc4e1ebb"></script> <script defer src="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js?d633890033921b33e0ceb13d22340a9c"></script> <script defer src="/assets/js/common.js?acdb9690d7641b2f8d40529018c71a01"></script> <script defer src="/assets/js/copy_code.js?07b8786bab9b4abe90d10e61f7d12ff7" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>